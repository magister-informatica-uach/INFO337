{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Make fonts readable at 1024x768 -->\n",
    "<style>\n",
    ".rendered_html { font-size:1.em; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and matplotlib configuration\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "print('Running PyMC3 v{}'.format(pm.__version__))\n",
    "import theano.tensor as T\n",
    "from matplotlib import animation\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "from style import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universidad Austral de Chile\n",
    "\n",
    "# INFO337 - Herramientas estadísticas para la investigación\n",
    "\n",
    "# Markov Chain Monte Carlo\n",
    "\n",
    "### A course of the masters in informatics program\n",
    "\n",
    "### https://github.com/magister-informatica-uach/INFO337\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"index\"></a>\n",
    "\n",
    "# Contents\n",
    "***\n",
    "\n",
    "1. [The Bayesian setting](#section1)\n",
    "1. [Markov Chain Monte Carlo](#section2)\n",
    "1. [Tutorial: Linear regression](#section3)\n",
    "1. [Tutorial: GMM](#section4)\n",
    "1. [Tutorial: Multilabel logistic regression](#section5)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. Davidson-Pilon, \"[Bayesian methods for hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\", *Addison Wesley*, 2016, **Chapter 2 and 3**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"section1\"></a>\n",
    "\n",
    "# The Bayesian setting\n",
    "***\n",
    "\n",
    "Reminder from previous classes:\n",
    "- **Probability:** Represents how believable an event is\n",
    "     - How confident we are that the event occurs\n",
    "- Our belief of a certain event $A$ is the **prior probability** $p(A)$\n",
    "- We collect evidence $X$ to **update** our belief on $A$ forming a **posterior** $p(A|X)$\n",
    "- How? Bayes Theorem\n",
    "$$\n",
    "p(A|X) = \\frac{p(X|A)p(A)}{p(X)} \\propto p(X|A)p(A)\n",
    "$$\n",
    "- In general the larger the amount of evidence $N$ the less influence the prior has\n",
    "\n",
    "***\n",
    "\n",
    "Model/parameter estimation:\n",
    "- Maximum likelihood (MLE): point estimate\n",
    "- Maximum a posterior (MAP): point estimate plus prior\n",
    "- Bayes: full posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One more time: Bayes Theorem + law of total probability:\n",
    "$$\n",
    "p(A|X) = \\frac{p(X|A)p(A)}{p(X)} = \\frac{p(X|A)p(A)}{\\int p(X, A) dA} = \\frac{p(X|A)p(A)}{\\int p(X|A)p(A) dA}\n",
    "$$\n",
    "\n",
    "- We propose the **prior** and a **likelihood**: our assumptions on the data and parameter distributions\n",
    "- The **evidence** is ... usually intractable \n",
    "    - We are integrating on all the possible values of the parameters, remember GMM?\n",
    "- Are we done? Luckily no\n",
    "    - Use priors/likelihoods so that posterior is analytical (conjugates)\n",
    "    - Approximate inference (Variational Bayes)\n",
    "    - **Today:** Monte-Carlo Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [The Bayesian landscape](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb#The-Bayesian-landscape)\n",
    "\n",
    "- In the following example we draw two dimensional Poisson distributed data\n",
    "- We consider exponential priors for the $\\lambda$\n",
    "- This is essentially a two dimensional surface in parameter space\n",
    "- The prior sets the initial shape \n",
    "- The observations warp the surface\n",
    "- To find the best parameters we need to explore this possibly high-dim space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 4))\n",
    "def update(rseed, N):\n",
    "    np.random.seed(rseed); ax[0].cla(); ax[1].cla();\n",
    "    lambda_1_true = 1; lambda_2_true = 3\n",
    "    data = np.concatenate([scipy.stats.poisson.rvs(lambda_1_true, size=(N, 1)),\n",
    "                           scipy.stats.poisson.rvs(lambda_2_true, size=(N, 1))\n",
    "                          ], axis=1)\n",
    "\n",
    "    x = y = np.linspace(.01, 5, 100)\n",
    "    likelihood_x = np.array([scipy.stats.poisson.pmf(data[:, 0], _x)\n",
    "                            for _x in x]).prod(axis=1)\n",
    "    likelihood_y = np.array([scipy.stats.poisson.pmf(data[:, 1], _y)\n",
    "                            for _y in y]).prod(axis=1)\n",
    "    L = np.dot(likelihood_x[:, None], likelihood_y[None, :])\n",
    "\n",
    "\n",
    "    exp_x = scipy.stats.expon.pdf(x, loc=0, scale=3)\n",
    "    exp_y = scipy.stats.expon.pdf(x, loc=0, scale=10)\n",
    "    M = np.dot(exp_x[:, None], exp_y[None, :])\n",
    "\n",
    "    im = ax[0].imshow(M, interpolation='none', origin='lower',\n",
    "                    cmap=plt.cm.jet, extent=(0, 5, 0, 5))\n",
    "    ax[0].scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\n",
    "    ax[0].set_xlim(0, 5); ax[0].set_ylim(0, 5)\n",
    "    ax[0].set_title(\"Prior Landscape\")\n",
    "\n",
    "    im = ax[1].imshow(M * L, interpolation='none', origin='lower',\n",
    "                    cmap=plt.cm.jet, extent=(0, 5, 0, 5))\n",
    "\n",
    "    ax[1].scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\n",
    "    ax[1].set_title(\"Landscape warped \\nby %d data observation.\" % N)\n",
    "    ax[1].set_xlim(0, 5); ax[1].set_ylim(0, 5);\n",
    "interact(update, N=SelectionSlider_nice(options=[1, 10, 100]),\n",
    "         rseed=IntSlider_nice(min=0, max=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "<a id=\"section2\"></a>\n",
    "\n",
    "# Markov Chain Monte Carlo (MCMC)\n",
    "***\n",
    "\n",
    "- Monte carlo methods obtain numerical results via repeated random sampling\n",
    "- Example: Monte-carlo integration\n",
    "    - Non-deterministic approach to compute definite integral\n",
    "    - Draw uniform random samples (N-d square)\n",
    "    - Check which of those are inside your function and which are not\n",
    "    - The search is naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    return (x-0)**2 + (y-0)**2 -1. <= 0.\n",
    "x = np.linspace(0, 1, num=1000); X, Y = np.meshgrid(x, x)\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "def update(N, rseed):\n",
    "    np.random.seed(rseed);\n",
    "    ax.cla(); ax.contourf(X, Y, func(X, Y), cmap=plt.cm.Reds); ax.set_aspect('equal')\n",
    "    xr = np.random.rand(N, 2)\n",
    "    if N < 1000000:\n",
    "        ax.scatter(xr[:, 0], xr[:, 1], s=1, alpha=0.5)\n",
    "    N_inside = len(np.where(func(xr[:, 0], xr[:, 1]))[0])\n",
    "    print(\"%0.4f\" %(4.*N_inside/N))\n",
    "\n",
    "interact(update, N=SelectionSlider_nice(options=[1, 10, 100, 1000, 10000, 100000, 1000000]), \n",
    "         rseed=IntSlider_nice(min=0, max=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now consider a system, i.e. a set of states that evolve in time (steps)\n",
    "- A system can be modeled with a **Markov Chain**: stochastic process for sequences\n",
    "- Given that it fits the **Markov property**\n",
    "    - The conditional probability of future states depends only on the present state\n",
    "    - $p(X_n|X_{n-1},\\ldots, X_0) = p(X_n|X_{n-1})$\n",
    "    - We only need knowledge from one state to move to another\n",
    "\n",
    "## MCMC \n",
    "\n",
    "- MCMC is a family of algorithms to generate random samples from a probability distribution\n",
    "- We sample (Monte-carlo) but every sample depends on the previous one (Markov chain)\n",
    "    - This way we search the space in a less naive way\n",
    "    - Random walk with a generated **step**\n",
    "    - We will review some algorithms that select the step\n",
    "- A collection of samples is called a **trace**\n",
    "    - From the trace  we can approximate the posterior we are looking for\n",
    "    - ... if the chain **converged**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Metropolis Hastings\n",
    "- Random walker that moves on all dimensions simultaneously\n",
    "- A candidate step is drawn from an arbitrary but symmetric distribution $x^{new} \\sim g(x^{new}|x_t)$\n",
    "- We accept the step if $f(x^{new})/f(x^t)$ is equal or larger than a certain threshold\n",
    "- $f(\\cdot)$ needs only to be proportional to the target distribution (evidence is canceled in the ratio)\n",
    "- Repeat many times until convergence\n",
    "\n",
    "\n",
    "## Hamiltonian Monte-Carlo\n",
    "- Family of step proposing methods that use momentum (derivatives)\n",
    "- Only for continuous variables\n",
    "- Cost more than MH (single iteration) but they require less iterations\n",
    "- http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "# Probabilistic programming\n",
    "***\n",
    "\n",
    "- PP: Doing statistics (Bayesian inference) using the tools of computer science \n",
    "- PP languages: unify general purpose programming with probabilistic modeling\n",
    "- Python friendly PP frameworks/libraries:\n",
    "    - [PyMC3](https://docs.pymc.io/notebooks/getting_started.html): Black-box VI, MH, Gibbs, NUTS sampler. Uses theano\n",
    "    - [PyStan](https://pystan.readthedocs.io/en/latest/): Python interface for [Stan platform](http://mc-stan.org/)\n",
    "    - [Edward](http://edwardlib.org/): Black-box VI, neural networks. Uses tensorflow\n",
    "    - [emcee](http://dfm.io/emcee/current/): Pure python implementation of the Affine invariant MCMC ensemble sampler\n",
    "    - http://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1809.10756\"><img src=\"img/PP.png\"></a>\n",
    "\n",
    "PP run in two directions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"section3\"></a>\n",
    "***\n",
    "# A PyMC3 tutorial\n",
    "***\n",
    "\n",
    "In this tutorial we will review how to do\n",
    "1. Model definition\n",
    "1. Fitting\n",
    "1. Convergence checks\n",
    "1. Posterior analysis\n",
    "with PyMC3\n",
    "\n",
    "We start with an example of Bayesian linear regression (class 2)\n",
    "- Gaussian noise with variance $\\sigma^2$\n",
    "- One independent variables: $X$\n",
    "- Three parameters $\\beta$: intercept plus one coefficient per covariate\n",
    "\n",
    "$$\n",
    "Y = \\beta X + \\epsilon \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "Credit: https://docs.pymc.io/notebooks/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "beta_true, sigma_true, N = [1, 2.5], 1., 30\n",
    "X = np.random.randn(N)\n",
    "Y = beta_true[0] + beta_true[1]*X +  np.random.randn(N)*sigma_true\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.scatter(X, Y)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model specification\n",
    "- First we instantiate a model from `pm.Model()`\n",
    "- This creates a context for our model\n",
    "- Within this context we will set priors, likelihood, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as my_model:\n",
    "    # Priors\n",
    "    beta = pm.Normal(name='beta', mu=0, sd=10, shape=2)\n",
    "    sigma = pm.HalfNormal('sigma', sd=1, testval=np.std(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We have set two stochastic variables\n",
    "- These are the priors for $\\beta$ and $\\sigma$\n",
    "- Check available distributions [here](https://docs.pymc.io/api/distributions.html)\n",
    "- For $\\beta$ we used a normal prior. The constructor for normal is\n",
    "\n",
    "`pm.Normal(name='beta', mu=0, sd=10, shape=3)`\n",
    "where\n",
    "1. name (string): Unique identifier, in this case 'beta'\n",
    "1. mu and sd (floats): mean and standard deviation of the normal distribution in this case 0 and 10, respectively\n",
    "1. shape: specifies the dimensionality, in this case we create 3 univariate normals\n",
    "\n",
    "\n",
    "- For $\\sigma$ we use a half-normal prior\n",
    "    - $\\sigma$ is non-negative so we have to use a non-negative prior, e.g. gamma\n",
    "    - Other option is to use a bounded distribution\n",
    "    - One can create arbitrary bounded distributions with\n",
    "    \n",
    "    `x = pm.Bound(pm.Normal, lower=0.0)('x', mu=1.0, sd=3.0)`\n",
    "\n",
    "\n",
    "\n",
    "- We can give initial values for the variables using `testval`\n",
    "\n",
    "\n",
    "To continue working in this context we use \n",
    "\n",
    "`with my_model:` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with my_model:\n",
    "    # Likelihood\n",
    "    Y_obs = pm.Normal('Y_obs', mu=beta[0] + beta[1]*X, sd=sigma, observed=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To specify the likelihood we select a distribution and give the \"special\" argument `observed`\n",
    "- This corresponds to the data\n",
    "- It can be a numpy ndarray or pandas data frame\n",
    "- By giving `beta` and `sigma` as parameter for `Y_obs` we automatically create a parent-child relation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(my_model.free_RVs)\n",
    "print(my_model.deterministics)\n",
    "print(my_model.observed_RVs)\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model fitting\n",
    "\n",
    "In PyMC3 we can do MAP to get point estimates, VI to do approximate inference and MCMC for posterior sampling\n",
    "\n",
    "- MAP and VI follow an optimization approach. \n",
    "- They are generally faster than MCMC but return less information \n",
    "    - point estimate with no uncertainty\n",
    "    - approximate factorized distribution (only continuous variables)\n",
    "- MAP and VI can be used to find reasonable initial states for MCMC\n",
    "- For very complex model and large number of observations we may not be able to do MCMC at all \n",
    "\n",
    "**MAP estimate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with my_model:\n",
    "    map_estimate = pm.find_MAP(progressbar=True)\n",
    "map_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MCMC sampling\n",
    "\n",
    "- To do MCMC first you have to specify a step method (MH, Gibbs, NUTS, etc)\n",
    "- PyMC3 have very good default options\n",
    "    - No-U-Turn sampler is the default option for continuous parameters\n",
    "    - MH is the default for discrete parameters\n",
    "- Use diagnostics to check the convergence of the chains\n",
    "- Check posteriors with traceplots\n",
    "\n",
    "`sample(draws=500, step=None, init='auto', n_init=200000, start=None, trace=None, chain_idx=0, chains=None, cores=None, tune=500, nuts_kwargs=None, step_kwargs=None, progressbar=True, model=None, random_seed=None, live_plot=False, discard_tuned_samples=True, live_plot_kwargs=None, compute_convergence_checks=True, use_mmap=False, **kwargs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with my_model:\n",
    "    # draw 500 posterior samples\n",
    "    trace = pm.sample(draws=2000, start=map_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, figsize=(9, 6), combined=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, figsize=(9, 6), kde_plot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_trace = pm.trace_to_dataframe(trace)\n",
    "pd.plotting.scatter_matrix(df_trace, diagonal='kde', figsize=(8, 6))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.summary(trace).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_plot = np.ones(shape=(1000, 2))\n",
    "X_plot[:, 1] = np.linspace(np.amin(X), np.amax(X), num=1000)\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.scatter(X, Y, c='k', label='data');\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); \n",
    "\n",
    "beta_mean, beta_std = np.mean(trace['beta'], axis=0), np.std(trace['beta'], axis=0)\n",
    "ax.plot(X_plot[:, 1], np.dot(X_plot, beta_mean), lw=3, label='Inferred model') \n",
    "ax.fill_between(X_plot[:, 1], np.dot(X_plot, beta_mean-2*beta_std), \n",
    "                np.dot(X_plot, beta_mean+2*beta_std), alpha=0.5)\n",
    "ax.plot(X_plot[:, 1], np.dot(X_plot, beta_true), lw=3, label='True model')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Gelman-Rubin diagnostic tests for lack of convergence \n",
    "- Compares the variance between multiple chains to the variance within each chain.\n",
    "- Values greater than one indicate that one or more chains have not yet converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.gelman_rubin(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"section4\"></a>\n",
    "***\n",
    "# Second example: Mixture of Gaussians\n",
    "***\n",
    "- In this example we will try to infer the parameters of a mixture of two 1d Gaussians\n",
    "- Let's cerate some data\n",
    "\n",
    "Ref: http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu_true = [-1, 2]\n",
    "std_true = [2, 0.75]\n",
    "p_true, N = 0.4, 200\n",
    "p = np.array([p_true, 1-p_true])\n",
    "np.random.seed(0)\n",
    "data = np.concatenate((scipy.stats.norm(loc=mu_true[0], scale=std_true[0]).rvs(size=int(p[0]*N)),\n",
    "                       scipy.stats.norm(loc=mu_true[1], scale=std_true[1]).rvs(size=int(p[1]*N))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "ax.hist(data, bins=20, alpha=0.8, density=True);\n",
    "x_plot = np.linspace(np.amin(data), np.amax(data), num=1000)\n",
    "for k in range(2):\n",
    "    ax.plot(x_plot, p[k]*np.exp(-0.5*(x_plot - mu_true[k])**2/std_true[k]**2)/(np.sqrt(2.*np.pi)*std_true[k]), 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model specification**\n",
    "\n",
    "We create priors for the center, dispersion and weights of the Gaussians\n",
    "\n",
    "Ref: https://docs.pymc.io/notebooks/gaussian_mixture_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior on concentration\n",
    "    p = pm.Dirichlet('p', a=np.array([0.1, 0.1]), shape=2)\n",
    "    p_min_potential = pm.Potential('p_min_potential', T.switch(T.min(p) < .1, -np.inf, 0))\n",
    "    z = pm.Categorical(\"z\", p, shape=data.shape[0])\n",
    "    # Prior on standard deviations\n",
    "    sds = pm.Uniform(\"sds\", lower=0, upper=100, shape=2)\n",
    "    # Prior on means\n",
    "    centers = pm.Normal(\"centers\", \n",
    "                        mu=np.array([-1, 1]), \n",
    "                        sd=np.array([10, 10]), \n",
    "                        shape=2)\n",
    "    order_means_potential = pm.Potential('order_means_potential',\n",
    "                                         T.switch(centers[1]-centers[0] < 0, -np.inf, 0))\n",
    "    \n",
    "    center_i = pm.Deterministic('center_i', centers[z])\n",
    "    sd_i = pm.Deterministic('sd_i', sds[z])\n",
    "    \n",
    "    # Likelihood\n",
    "    observations = pm.Normal(\"obs\", mu=center_i, sd=sd_i, observed=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Deterministic variables allow us to track custom variables in the traces\n",
    "\n",
    "`pm.Deterministic('name', var)`\n",
    "\n",
    "Potentials allow us to add an arbitrary factor the the likelihood\n",
    "\n",
    "`pm.Potential('name', var)`\n",
    "\n",
    "In this case we add potentials to\n",
    "- Penalize solutions with  empty clusters\n",
    "- Forcing that the \"first\" gaussian is always to the left (remember z flipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model fitting**\n",
    "\n",
    "Let's try MAP and then MCMC as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    start = pm.find_MAP()\n",
    "print(start['centers'])\n",
    "print(start['sds'])\n",
    "print(start['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    step1 = pm.Metropolis(vars=[p, sds, centers])\n",
    "    # step1 = pm.NUTS(vars=[p, sds, centers])\n",
    "    step2 = pm.ElemwiseCategorical(vars=[z], values=[0, 1])\n",
    "    # step2 = pm.CategoricalGibbsMetropolis(vars=[z])\n",
    "    trace = pm.sample(draws=100, step=[step1, step2], tune=0, chains=2, cores=2, start=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - `step`: In this case we have specified the step functions for each variable\n",
    " - Burn-in period `tune`: N first steps of the chain are discarded from the trace to build posteriors from the converged zone\n",
    " - `chains`: We can also specify the amount of chains and cores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, figsize=(9, 6), combined=True, varnames=['p', 'centers', 'sds']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(pm.gelman_rubin(trace)['centers'])\n",
    "print(pm.gelman_rubin(trace)['sds'])\n",
    "print(pm.gelman_rubin(trace)['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plots.autocorrplot(trace=trace, figsize=(9, 8), varnames=['centers', 'sds', 'p']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A chain that is exploring the space well will exhibit very high autocorrelation. \n",
    "- Low autocorrelation is a sufficient condition for converged MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.summary(trace, varnames=['centers', 'sds', 'p']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, figsize=(9, 7), kde_plot=True, varnames=['centers', 'sds', 'p']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Posterior predictive checks (PPC)\n",
    "\n",
    "Another way to validate a model is to generate data from the posterior draws and compare with the original distribution\n",
    "\n",
    "This is done with `pm.sample_posterior_predictive` or `pm.sample_ppc` depending on your PyMC version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ppc = pm.sample_ppc(samples=1000, trace=trace, model=model)\n",
    "print(ppc.keys())\n",
    "x_plot = np.linspace(np.amin(data), np.amax(data), num=1000)\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "ax.hist(data, bins=20, alpha=0.8, density=True);\n",
    "mean_score = 0.0\n",
    "for i in range(100):\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(ppc['obs'][i, :].reshape(-1, 1))\n",
    "    score = np.exp(kde.score_samples(x_plot.reshape(-1, 1)))\n",
    "    mean_score += score\n",
    "    ax.plot(x_plot, score, 'k', alpha=0.05)\n",
    "ax.plot(x_plot, mean_score/100, 'k', lw=4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Useful tips for MCMC](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb#Useful-tips-for-MCMC)\n",
    "\n",
    "- Intelligent starting values\n",
    "- Choose your priors well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"section5\"></a>\n",
    "***\n",
    "# Third example: Multilabel logistic (softmax) regression\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!wget -nc http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "df = pd.read_table(\"data/iris.data\", delimiter=\",\",  na_values=\"?\",\n",
    "                   names= [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"class\"])\n",
    "# numeric labels\n",
    "labels = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "def compare(x):\n",
    "    for k in range(3):\n",
    "        if x == labels[k]:\n",
    "            return k\n",
    "df[\"class\"] = df[\"class\"].apply(compare)\n",
    "# To numpy\n",
    "label = df.iloc[:, -1].values\n",
    "data =df.iloc[:, 0:4].values\n",
    "# Create train and test index\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3)\n",
    "train_idx, test_idx = next(sss.split(data, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from theano import shared\n",
    "X = shared(data[train_idx, :])\n",
    "Y = shared(label[train_idx])\n",
    "\n",
    "with pm.Model() as my_model:\n",
    "    # Priors\n",
    "    alpha = pm.Normal(name='alpha', mu=0, sd=10, shape=(3, ))\n",
    "    beta = pm.Normal(name='beta', mu=0, sd=10, shape=(4, 3))\n",
    "    #sigma = pm.HalfNormal('sigma', sd=1)\n",
    "    p = T.nnet.softmax(X.dot(beta) + alpha)\n",
    "    label_obs = pm.Categorical('labels_obs', p=p, observed=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case we will try ADVI to get a starting point\n",
    "\n",
    "**Automatic differentiation Variational inference (ADVI)**\n",
    "\n",
    "[PyMC Variational API](https://docs.pymc.io/notebooks/variational_api_quickstart.html)\n",
    "\n",
    "1. Select 'advi' or 'fullrank_advi' as method to fit the model\n",
    "1. Choose number of iterations\n",
    "1. Get a trace from the fitted model\n",
    "1. Study the convergence of the objective function and traced parameters\n",
    "1. Inspect traces and posteriors with diagnostic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with my_model:    \n",
    "    # inference = pm.ADVI()\n",
    "    inference = pm.FullRankADVI()\n",
    "    # inference = pm.SVGD(n_particles=500, jitter=1)\n",
    "    approx = pm.fit(method=inference, n=30000, \n",
    "                    obj_optimizer=pm.adam(learning_rate=0.001))\n",
    "    train_probs = approx.sample_node(p)\n",
    "    test_probs = approx.sample_node(p, more_replacements={X: data[test_idx, :]})\n",
    "    advi_trace = approx.sample(draws=5000)     \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(approx.hist, alpha=.3); ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, train_acc = [], []\n",
    "for i in range(10):\n",
    "    test_acc.append(np.mean(test_probs.argmax(-1).eval() ==  label[test_idx]))\n",
    "    train_acc.append(np.mean(train_probs.argmax(-1).eval() ==  label[train_idx]))\n",
    "train_acc = np.array(train_acc)\n",
    "test_acc = np.array(test_acc)\n",
    "print(\"Train: %f %f\" % (np.mean(train_acc), np.std(train_acc)))\n",
    "print(\"Test: %f %f\" % (np.mean(test_acc), np.std(test_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_trace = pm.trace_to_dataframe(advi_trace)\n",
    "pd.plotting.scatter_matrix(df_trace, diagonal='kde', figsize=(9, 8));"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
