{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and matplotlib configuration\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "from ipywidgets import interact, Button, Output, Box\n",
    "from IPython.display import display\n",
    "from style import *\n",
    "\n",
    "# Others\n",
    "from scipy.special import erf\n",
    "gaussian_pdf = lambda x, mu=0, s=1: np.exp(-0.5*(x-mu)**2/s**2)/(s*np.sqrt(2*np.pi))\n",
    "gaussian_cdf = lambda x, mu=0, s=1: 0.5 + 0.5*erf((x-mu)/(s*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universidad Austral de Chile\n",
    "\n",
    "# INFO337 - Herramientas estadísticas para la investigación\n",
    "\n",
    "# Statistical inference: Modeling\n",
    "\n",
    "### A course of the masters in informatics program\n",
    "\n",
    "### https://github.com/magister-informatica-uach/INFO337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "1. Hastie, Tibshirani and Friedman, \"The elements of statistical learning\" 2nd Ed., *Springer*, **Chapter 8**\n",
    "1. Murphy, \"Machine Learning: A Probabilistic Perspective\", *MIT Press*, 2012, **Chapter 5**\n",
    "1. Ivezic, Connolly, VanderPlas and Gray, \"Statistic, Data Mining, and Machine Learning in Astronomy\", *Princeton University Press*, 2014, **Chapters 4 and 5**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is statistical inference?\n",
    "\n",
    "\n",
    "**Inference:** \n",
    "\n",
    "<center>*Draw conclusions from facts through a scientific premise*</center>\n",
    "\n",
    "**Statistical inference**:\n",
    "- Facts: Observed data\n",
    "- Premise: Probabilistic model\n",
    "- Conclusion: An unobserved quantity of interest\n",
    "- Objective: Quantify the uncertainty of the conclusion given the data and the premise\n",
    "\n",
    "\n",
    "Examples of statistical inference tasks:\n",
    "- **Parameter estimation:** What is the best estimate of a model parameter based on the observed data?\n",
    "- **Confidence estimation:** How trustworthy is our point estimate?\n",
    "- **Hypothesis testing:** Is the data consistent with a given hypothesis or model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametric and nonparametric models\n",
    "\n",
    "To conduct inference we start by defining a statistical model. Models can be broadly classified as:\n",
    "\n",
    "- **Parametric models:** \n",
    "    - It corresponds to an analytical function  (distribution) with free parameters\n",
    "    - Has an *a-priori* fixed number of parameters\n",
    "    - In general: Stronger assumptions, easier to interpret, faster to use\n",
    "    \n",
    "    \n",
    "- **Non-parametric models:** \n",
    "    - Distribution-free model but they do have parameters and assumptions (e.g. dependence)\n",
    "    - The number of parameters depends on the amount of training data\n",
    "    - In general: More flexible, harder to train\n",
    "    \n",
    "**Statistical modeling how to's**\n",
    "- How to collect the data?\n",
    "- How to construct a probabilistic model?\n",
    "- How to incorporate expert (*a priori*) knowledge?\n",
    "- How to interpret results? How to make predictions from data?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Frequentist and Bayesian inference\n",
    "\n",
    "There are two paradigms or perspectives for statistical inference: Frequentist (F) or classical and Bayesian (B). \n",
    "\n",
    "There are conceptual differences between these paradigms, for example\n",
    "\n",
    "**Definition of probability:**\n",
    "- F: Relative frequency of an event. An objective property of the real world\n",
    "- B: Degree of subjective belief. Probability statements can be made not only on data but also on parameters and models themselves\n",
    "\n",
    "**Interpretation of parameters:**\n",
    "- F: They are unknown and fixed constants\n",
    "- B: They have distributions that quantify the uncertainty of our knowledge about them. We can compute expected values of the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Frequentist approach on parametric modeling \n",
    "\n",
    "**Parametric inference:** We assume that observations follow a distribution, *i.e.* observations are a realization of a random process (sampling) \n",
    "\n",
    "The conceptual (iterative) steps of parametric inference are:\n",
    "1. **Model fitting:** Find parameters by fitting data to the current model\n",
    "1. **Model proposition:** Propose a new model that accommodates important features of the data better than the previous one\n",
    "\n",
    "In the frequentist approach Step 1 is typically solved using **Maximum Likelihood Estimation (MLE)**, Method of Moments (MoM) or the M-estimator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The likelihood function is\n",
    "\n",
    "- a quantitative description of our experiment (measuring process)\n",
    "- the starting point for **parametric modeling** in both F and B paradigms\n",
    "- a metric that tells us how good our model is with respect to the **observed data**\n",
    "\n",
    "\n",
    "### Formally speaking\n",
    "\n",
    "- We have an experiment that we model as a set of R.Vs $X_1, X_2, \\ldots, X_N$\n",
    "- We have observations/realizations from our R.Vs $\\{x_i\\} = x_1, x_2, \\ldots, x_N$\n",
    "- We assume that the R.Vs follow a particular probability distribution $x_i \\sim f(x_i, \\theta)$\n",
    "- The distribution has (unknown) parameters $\\theta$\n",
    "- The likelihood is a function of the parameters which is defined from the joint distribution\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= P(X_1=x_1, X_2=x_2, \\ldots, X_N=x_n) \\nonumber \\\\\n",
    "&= f(x_1, x_2, \\ldots, x_N | \\theta) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Assuming that our observations are **independent and identically distributed** (iid)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= f(x_1| \\theta) \\cdot f(x_2| \\theta) \\cdot \\ldots \\cdot f(x_N| \\theta) \\nonumber \\\\\n",
    "&= \\prod_{i=1}^N f(x_i| \\theta) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<center>\"Given $\\{x_i\\}$, How likely is it that it was generated by $L(\\theta)=\\prod_{i=1}^N f(x_i| \\theta)$?\"</center>\n",
    "<center>\"Given $\\{x_i\\}$, How likely is it that the unknown parameter was $\\theta$?\"</center>\n",
    " \n",
    "\n",
    "***\n",
    "\n",
    "### Note: Likelihood is not probability\n",
    "\n",
    "- The likelihood of a single value is given by the true pdf \n",
    "- The likelihood of a set is not normalized to 1, *i.e.* in general the likelihood is not a valid pdf\n",
    "- The likelihood by itself cannot be interpreted as a probability of $\\theta$\n",
    "- Given a fixed data set the likelihood is defined as a function of $\\theta$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example: Likelihood of N Gaussian dist. samples \n",
    "\n",
    "- Let's say we have N random numbers and assume they are Gaussian *iid*\n",
    "- We can compute their likelihood using the formula above for a given set of parameters:\n",
    "\n",
    "$$\n",
    "L(\\theta=\\{\\mu, \\sigma^2\\}) = f(\\{x\\} | \\mu, \\sigma^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left ( -\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "mu_hat = np.linspace(-2.2, 2.2, num=200); \n",
    "s_hat = np.linspace(0.2, 2.2, num=200)\n",
    "X, Y = np.meshgrid(mu_hat, s_hat)\n",
    "fig, ax = plt.subplots(figsize=(7, 4), tight_layout=True);\n",
    "#cax = make_axes_locatable(ax).append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "\n",
    "def update(N, mu, sigma, seed):\n",
    "    ax.cla(); logL = np.zeros(shape=X.shape);\n",
    "    ax.set_xlabel(r\"$\\mu$\"); ax.set_ylabel(r\"$\\sigma$\")\n",
    "    np.random.seed(seed); xhat = mu + sigma*np.random.randn(N)\n",
    "    for i, mu_hat_ in enumerate(mu_hat):\n",
    "        for j, s_hat_ in enumerate(s_hat):\n",
    "            logL[j, i] = -0.5*len(xhat)*np.log(2.*np.pi*s_hat_**2) - 0.5*np.sum((xhat-mu_hat_)**2)/s_hat_**2\n",
    "    levels = [k*np.amax(logL) for k in np.logspace(0, 0.5, num=20)]\n",
    "    ax.scatter(mu, sigma, s=100, marker='d', c='white', zorder=100)\n",
    "    CS = ax.contourf(X, Y, (logL), levels=levels[::-1], cmap=plt.cm.Blues); \n",
    "    #fig.colorbar(CS, cax=cax)\n",
    "    \n",
    "interact(update, \n",
    "         N=SelectionSlider_nice(options=[10, 100, 1000], value=100),\n",
    "         mu=FloatSlider_nice(description=r\"$\\mu$\", min=-2, max=2, value=0.), \n",
    "         sigma=FloatSlider_nice(description=r\"$\\sigma$\", min=0.5, max=2., value=1.),\n",
    "         seed=IntSlider_nice(min=0, max=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- The value of the likelihood itself does not hold much meaning\n",
    "- But it can be used to make comparisons between different parameter vectors/models\n",
    "- **The larger the likelihood the better the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "\n",
    "In parametric modeling we are interested in finding $\\theta$ that best fit our observations. \n",
    "\n",
    "One method to do this is **MLE**:\n",
    "\n",
    "1. Select a distribution/model for the observations and formulate the likelihood $L(\\theta)$\n",
    "1. Search for the $\\theta$ that maximize $L(\\theta)$ given the data\n",
    "$$\n",
    "\\hat \\theta = \\text{arg} \\max_\\theta L(\\theta),\n",
    "$$\n",
    "where the point estimate $\\hat \\theta$  is called the **maximum likelihood estimator** of $\\theta$\n",
    "1. Determine the confidence region of $\\hat \\theta$ either analytically or numerically (bootstrap, cross-validation)\n",
    "1. Make conclusions about your model (hypothesis test)\n",
    "\n",
    "\n",
    "**Important**: A wrong assumption in step 1 can ruin your inference. How to select a model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Example: MLE  for the mean of a Gaussian distribution\n",
    "\n",
    "Let us:\n",
    "- consider a set of N measurements $\\{x_i\\}_{i=1,\\ldots, N}$ which corresponds to my weight :)\n",
    "- assume that the instrument used to measure weight has an error that follows a Gaussian distribution with variance $\\sigma^2$\n",
    "\n",
    "**System interpretation:** The measurements can be viewed as noisy realizations of the true weight $\\mu$\n",
    "\n",
    "$$\n",
    "x_i = \\mu + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "hence \n",
    "\n",
    "$$\n",
    "f(x_i) = \\mathcal{N}(x_i |\\mu,\\sigma^2) \\quad \\forall i\n",
    "$$\n",
    "\n",
    "The likelihood of the the true weight $\\mu$ given the measurements and the variance $\\sigma^2$ is \n",
    "$$\n",
    "L(\\mu) = f(\\{x_i\\}| \\mu, \\sigma^2) = \\prod_{i=1}^N f(x_i| \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\prod_{i=1}^N  \\exp  \\left( -\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "**Objective:** Find the value of $\\mu$ that maximize the likelihood given $\\{x_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "**Trick of the trade: The log likelihood** \n",
    "- In many cases (exponential family) it is more practical to find the maximum of the logarithm of the likelihood\n",
    "- Logarithm is a monotonic function and its maximum is the same as its argument.\n",
    "***\n",
    "\n",
    "In this case the log likelihood is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log L (\\mu) &= \\log \\prod_{i=1}^N f(x_i|\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\log f(x_i|\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&= - \\frac{1}{2} \\sum_{i=1}^N \\log 2\\pi\\sigma^2 - \\frac{1}{2} \\sum_{i=1}^N  \\frac{(x_i-\\mu)^2}{\\sigma^2}  \\nonumber  \\\\\n",
    "&=  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We maximize by making the derivative of the log likelihood equal to zero\n",
    "\n",
    "$$\n",
    "\\frac{d  \\log L (\\mu)}{d\\mu} =  \\frac{1}{\\sigma^{2}}  \\sum_{i=1}^N (x_i-\\mu) =0\n",
    "$$\n",
    "\n",
    "Finally the MLE of $\\mu$ is \n",
    "$$\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i, \\quad \\sigma >0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example: MLE for the variance of a Gaussian distribution\n",
    "\n",
    "The MLE estimator of the variance can be obtained using the same procedure:\n",
    "\n",
    "$$\n",
    "\\log L (\\mu, \\sigma^2) =  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d  \\log L (\\mu, \\sigma^2)}{d\\sigma^2} =  - \\frac{N}{2} \\frac{1}{\\sigma^2} + \\frac{1}{2\\sigma^{4}}\\sum_{i=1}^N (x_i-\\mu)^2 =0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i- \\hat\\mu)^2\n",
    "$$\n",
    "\n",
    "- If the true mean is not known then this is a biased estimator of the true variance\n",
    "- MLE can produce biased estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4), tight_layout=True); \n",
    "ax2 = ax.twinx()\n",
    "np.random.seed(0)\n",
    "x = 80 + np.random.randn(10000)\n",
    "#x = 80 + 2*np.random.rand(1000)  # What happens if the data is not normal\n",
    "k_list = [int(x) for x in np.logspace(0, 4, num=50)]\n",
    "hat_mu = np.array([np.sum(x[:k])/k for k in k_list])\n",
    "hat_var = np.array([np.sum((x[:k]-hat_mu[i])**2)/(k) for i, k in enumerate(k_list)])\n",
    "ax.plot(k_list, hat_mu); ax2.plot(k_list, hat_var, linestyle='--'); ax.set_xscale('log')\n",
    "ax.set_xlabel('Number of samples'); \n",
    "ax.set_ylabel('$\\hat \\mu$ (solid line)'); ax2.set_ylabel('$\\hat \\sigma^2$ (dashed line)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Biased and unbiased estimator\n",
    "\n",
    "For a parameter $\\theta$ and an estimator $\\hat \\theta$, if\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat \\theta] = \\theta,\n",
    "$$\n",
    "\n",
    "then $\\hat \\theta$ is an unbiased estimator of $\\theta$\n",
    "\n",
    "\n",
    "#### Example: Is the MLE of $\\mu$ unbiased?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat \\mu] &= \\mathbb{E} \\left[ \\frac{1}{N} \\sum_{i=1}^N x_i \\right]  \\nonumber \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E}[x_i] = \\frac{1}{N} \\sum_{i=1}^N \\mu = \\mu  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "YES\n",
    "\n",
    "\n",
    "#### Example: Is the MLE of $\\sigma^2$ unbiased?\n",
    "\n",
    "First lets expand the expression of the MLE of the variance\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2 &= \\frac{1}{N} \\sum_{i=1}^N \\left(x_i- \\frac{1}{N}\\sum_{j=1}^N x_j \\right)^2 \\nonumber \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N x_i  x_j \\nonumber \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j\\neq i} x_i x_j - \\frac{1}{N^2} \\sum_{i=1}^N x_i^2 \\nonumber  \\\\\n",
    "&= \\frac{N-1}{N^2} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j \\neq i} x_i x_j  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then applying the expected value operator we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat \\sigma^2] &= \\frac{N-1}{N^2} \\sum_{i=1}^N \\mathbb{E} [x_i^2] - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j \\neq i} \\mathbb{E} [x_i] \\mathbb{E} [x_j] \\nonumber  \\\\\n",
    "&= \\frac{N-1}{N} (\\sigma^2 + \\mu^2) - \\frac{N-1}{N} \\mu^2 \\nonumber \\\\\n",
    "&= \\frac{N-1}{N} \\sigma^2 \\neq \\sigma^2  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "NO\n",
    "\n",
    "\n",
    "If we multiply it by a constant we obtain the well known unbiased estimator of the variance\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_{u}^2 = \\frac{N}{N-1} \\hat \\sigma^2 = \\frac{1}{N-1} \\sum_{i=1}^N (x_i- \\hat\\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MLE of a Gaussian mixture\n",
    "\n",
    "Let's imagine that our *iid* data come from a mixture of Gaussians with K components\n",
    "\n",
    "$$\n",
    "f(x_i|\\pi,\\mu,\\sigma^2) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\sigma_k^2),\n",
    "$$\n",
    "where $\\sum_{k=1}^K \\pi_k = 1$ and $\\pi_k \\in [0, 1] ~~ \\forall k$\n",
    "\n",
    "We can write the log likelihood as\n",
    "\n",
    "$$\n",
    "\\log L(\\pi,\\mu,\\sigma^2) = \\sum_{i=1}^N \\log \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\sigma_k^2)\n",
    "$$\n",
    "\n",
    "- Oh my! We cannot obtain analytical expressions for the parameters as before\n",
    "- We have to resort to iterative methods/optimizers, Can you name any?\n",
    "- **Expectation Maximization** (we will see this in a future class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimality properties and uncertainty of MLEs \n",
    "\n",
    "Assuming that the data truly comes from the specified model the MLE is\n",
    "\n",
    "- **Consistent:** The estimate converge to the true parameter as data points increase\n",
    "\n",
    "$$\n",
    "\\lim_{N\\to \\infty} \\hat \\theta = \\theta\n",
    "$$\n",
    "\n",
    "- **Asymptotically normal:** The distribution of the estimate approaches a normal centered at the true parameter. \n",
    "\n",
    "$$\n",
    "\\lim_{N\\to \\infty} p(\\hat \\theta) = \\mathcal{N}(\\hat \\theta | \\theta, \\sigma_\\theta^2)\n",
    "$$\n",
    "\n",
    "> This is a consequence of the central limit theorem!\n",
    "\n",
    "For *i.i.d.* $\\{X_i\\}, i=1,\\ldots,N$ with $\\mathbb{E}[X] < \\infty$ and $\\text{Var}[X] < \\infty$ then\n",
    "\n",
    "$$\n",
    "\\lim_{N\\to\\infty} \\sqrt{N} (\\bar X - \\mathbb{E}[X]) = \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "> **Consequence:** Because MLE have asymptotically normal distributions then log likelihood ratio have asymptotically a *chi-square* distributions (more on this later)\n",
    "\n",
    "- **Minimum variance:** The estimate achieve the theoretical minimal variance given by the Cramer-Rao bound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramer-Rao lower bound\n",
    "\n",
    "It is the inverse of the expected Fisher information, *i.e* the second derivative of $- \\log L$ with respect to $\\theta$\n",
    "\n",
    "$$\n",
    "\\sigma_{nm}^2 =  \\left (- \\frac{d^2 \\log L (\\theta)}{d\\theta_n \\theta_m} \\bigg\\rvert_{\\theta = \\hat\\theta}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\n",
    "- $\\sigma_{nm}^2$ is the minimum variance achieved by an unbiased estimator.\n",
    "- $\\sigma_{nn}^2$ give the marginal error bars \n",
    "- If $\\sigma_{nm} \\neq 0 ~~ n\\neq m$, then errors are correlated, *i.e* some combinations of parameters might be better determined than others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Cramer-Rao bound for the MLE of $\\mu$\n",
    "\n",
    "Considering a Gaussian likelihood\n",
    "\n",
    "$$\n",
    "\\log L (\\mu, \\sigma^2) =  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 \n",
    "$$\n",
    "\n",
    "What is the uncertainty of the MLE \n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "\n",
    "In this case the Cramer-rao bound\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma_{\\hat\\mu}^2  &= \\left (- \\frac{d^2 \\log L(\\mu, \\sigma^2)}{d\\mu^2} \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1}  \\nonumber \\\\\n",
    "&=  \\left (- \\frac{1}{\\sigma^2} \\frac{d}{d\\mu}  \\sum_{i=1}^N (x-\\mu) \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1}  \\nonumber \\\\\n",
    "&=  \\left ( \\frac{N}{\\sigma^2}  \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1} = \\frac{\\sigma^2}{N}  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- This is known as the **standard error of the mean**\n",
    "- Also, $p(\\hat \\mu) \\to \\mathcal{N}(\\hat \\mu| \\mu, \\sigma^2/N)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3.5), tight_layout=True)\n",
    "ax.set_title('Mean of Gaussian distributed data')\n",
    "N, mu_real, s_real = int(1e+4), 2.23142, 1.124123\n",
    "np.random.seed(0)\n",
    "x = mu_real + s_real*np.random.randn(N)\n",
    "mu_estimator = np.array([np.mean(x[:i]) for i in range(1, N)])\n",
    "\n",
    "ax.plot([1, N], [mu_real, mu_real], 'k--', label='Real')\n",
    "ax.plot(range(1,N), mu_estimator, label='MLE');\n",
    "ax.fill_between(np.arange(1, N), \n",
    "                mu_estimator-s_real/np.sqrt(np.arange(1, N)), \n",
    "                mu_estimator+s_real/np.sqrt(np.arange(1, N)), alpha=0.5);\n",
    "ax.set_xscale('log'); \n",
    "ax.set_label('Number of samples'); \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparing models using the likelihood\n",
    "\n",
    "- We can compare estimated parameters using the likelihood\n",
    "- We can formulate a hypothesis test for the MLE's using the asymptotic distribution\n",
    "\n",
    "Some of these tests are based on the $\\chi^2$ distribution with $k$ degrees of freedom\n",
    "\n",
    "The mean of the $\\chi^2$ is $k$ and the variance is $2k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "def update(dof):\n",
    "    ax.cla()\n",
    "    ax.set_title('Mean: dof, Variance: 2dof')\n",
    "    x = np.linspace(chi2.ppf(0.01, dof), chi2.ppf(0.99, dof), 100)\n",
    "    px = chi2.pdf(x, dof)\n",
    "    ax.plot(x, px, linewidth=2)\n",
    "    ax.set_xlim([np.amin(x)*0.9, np.amax(x)*1.1])\n",
    "    ax.set_ylim([np.amin(px)*0.9, np.amax(px)*1.1])  \n",
    "    display(\"95% quantile at:\", chi2.ppf(0.95, dof))\n",
    "    \n",
    "    \n",
    "interact(update, dof=SelectionSlider_nice(options=[1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Hypothesis testing: Wald-test\n",
    "\n",
    "Suppose we wish to test\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_0: \\theta = \\theta_0\n",
    "$$\n",
    "$$\n",
    "\\mathcal{H}_A: \\theta \\neq \\theta_0\n",
    "$$\n",
    "\n",
    "Under the null we can write \n",
    "\n",
    "$$\n",
    "\\text{Wald} = \\frac{(\\hat \\theta - \\theta_0)^2}{\\left (- \\frac{d^2 \\log L (\\theta)}{d\\theta^2} \\bigg\\rvert_{\\theta = \\hat\\theta}\\right)^{-1}} = (\\hat \\theta - \\theta_0)^2 \\sigma_{\\hat \\theta}^2 \\to \\chi^2_1\n",
    "$$\n",
    "\n",
    "If $\\text{Wald}$ is greather than the $(1-\\alpha)100\\%$ quantile of $\\chi^2_1$ we reject the null\n",
    "\n",
    "### Hypothesis testing: Log-likelihood ratio test or Wilks test\n",
    "\n",
    "Suppose we wish to test\n",
    "\n",
    "$$\n",
    "\\mathcal{H}_0: \\theta = \\theta_0\n",
    "$$\n",
    "$$\n",
    "\\mathcal{H}_A: \\theta =\\theta_1\n",
    "$$\n",
    "\n",
    "We can write a ratio between likelihoods\n",
    "\n",
    "$$\n",
    "\\lambda(\\mathcal{D}) = \\frac{L(\\theta_0|\\mathcal{D})}{L(\\theta_1|\\mathcal{D})} \n",
    "$$\n",
    "\n",
    "Asymptotically, under the null, we have  \n",
    "$$\n",
    "-2 \\log \\lambda(\\mathcal{D}) \\to \\chi^2_1\n",
    "$$\n",
    "\n",
    "If $-2 \\log \\lambda(\\mathcal{D})$ is greather than the $(1-\\alpha)100\\%$ quantile of $\\chi^2_1$ we reject the null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model comparison using other criteria\n",
    "\n",
    "- How to compare models with different number of parameters? \n",
    "- In general the more number of parameters the better the fit (overfitting)\n",
    "- How to score models taking into account their complexity?\n",
    "\n",
    "Two options:\n",
    "1. Cross-validation and bias/variance trade-off (based on finite data)\n",
    "1. Akaike information criterion (AIC) (based on asymptotic approximation)\n",
    "\n",
    "For a model with $k$ parameters and N data points the AIC is \n",
    "\n",
    "$$\n",
    "\\text{AIC} = -2 \\log L(\\hat \\theta) + 2k + \\frac{2k(k+1)}{N-k-1},\n",
    "$$\n",
    "\n",
    "which one seeks to minimize\n",
    "\n",
    "***\n",
    "\n",
    "**Parsimony principle** (aka Occam's Razor): Choose the simplest scientific explanation (few parameters) that fits the evidence (high likelihood)\n",
    "\n",
    "Also related: KISS principle\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Homework 1: MLE for a Bernoulli distribution\n",
    "\n",
    "A magician friend of yours has bought a \"magic coin\". \n",
    "\n",
    "He asks you to obtain for him the probability of obtaining a head with such coin.\n",
    "\n",
    "The coin has two outputs (head/tail) so we can assume that it follows a Bernoulli distribution\n",
    "\n",
    "$$\n",
    "f(x|p) = p^x (1-p)^{1-x}, ~~ x \\in \\{0, 1\\}\n",
    "$$\n",
    "\n",
    "Your friend tosses the coin N times and records the outputs $\\{x_i\\}$\n",
    "\n",
    "- 1) Find an analytic expression for $\\log L(p)$ \n",
    "- 2) Find and analytic expression for $\\hat p$ from the first derivative of $\\log L(p)$. Find $\\mathbb{E}[\\hat p]$, is it a biased or unbiased estimator?\n",
    "- 3) Find the Fisher information for $\\hat p$ and its variance $\\sigma_{pp}^2$\n",
    "- 4) Now consider \n",
    "    \n",
    "        coins = scipy.stats.binom.rvs(n=1, p=0.75, random_state=1234, size=100)\n",
    "    \n",
    "    as the $100$ coin tosses and plot $\\log L(p)$ as a function of $p$, highlight the value of $\\hat p$\n",
    "\n",
    "- 5) Consider that the tosses were obtained serially. Plot $\\hat p$ versus $N$. Use shading to show the uncertainty of the estimator as a function of $N$. How large needs $N$ to be so that $|\\hat p - p|/p < 0.05$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
