{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and matplotlib configuration\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "from ipywidgets import interact, Button, Output, Box\n",
    "from IPython.display import display\n",
    "from style import *\n",
    "\n",
    "# Others\n",
    "from scipy.special import erf\n",
    "gaussian_pdf = lambda x, mu=0, s=1: np.exp(-0.5*(x-mu)**2/s**2)/(s*np.sqrt(2*np.pi))\n",
    "gaussian_cdf = lambda x, mu=0, s=1: 0.5 + 0.5*erf((x-mu)/(s*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Nonparametric statistical modeling\n",
    "\n",
    "\n",
    "**Recap:** Statistical models that do not assume an underlying distribution\n",
    "\n",
    "Most famous example: **The histogram**\n",
    "\n",
    "- The histogram is a numerical representation of a distribution \n",
    "- The histogram allow us to visualize our data and explore its statistical features\n",
    "- The histogram is built by dividing the data range in **bins** and counting the observations that fall on a given bin\n",
    "- The parameters of the histogram are the size and location of the bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The importance of setting the number of bins right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4), tight_layout=True)\n",
    "x = np.linspace(-11, 10, num=1000)\n",
    "px = 0.7*gaussian_pdf(x, mu=-4, s=2) + 0.3*gaussian_pdf(x, mu=3, s=2)\n",
    "N = 1000; np.random.seed(0)\n",
    "hatx = np.concatenate((-4 + 2*np.random.randn(int(0.7*N)), \n",
    "                       (3 + 2*np.random.randn(int(0.3*N)))))\n",
    "\n",
    "def update(nbins): \n",
    "    ax.cla()\n",
    "    ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "    hist, bin_edges = np.histogram(hatx, bins=nbins, density=True)\n",
    "    ax.bar(bin_edges[:-1], hist, width=bin_edges[1:] - bin_edges[:-1], \n",
    "           edgecolor='k', align='edge', alpha=0.8)\n",
    "    \n",
    "interact(update, nbins=SelectionSlider_nice(options=[1, 2, 5, 10, 20, 50, 100], value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- A small number of bins omits the features of the distribution\n",
    "- A large number of bins introduce noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Histogram in practice\n",
    "\n",
    "How to select the width/number of bins?\n",
    "\n",
    "- Cross validation\n",
    "    - AMISE: (Asymptotic) Mean integrated square error\n",
    "- Rules of thumb, *e.g.* Scott's rule and Silverman's rule\n",
    "    - Proportional to the scale of data\n",
    "    - Inversely proportional to the number of samples \n",
    "    - Obtained through assumptions\n",
    "\n",
    "\n",
    "### Silverman's rule\n",
    "\n",
    "The width of the bins is \n",
    "\n",
    "$$\n",
    "h = 0.9 \\frac{\\min[\\sigma, 0.7412 (q_{75} - q_{25})]}{N^{1/5}},\n",
    "$$\n",
    "\n",
    "where $N$ is the number of observations, $\\sigma$ is the standard deviation and $q_{75}-q_{25}$ is the interquartile range. \n",
    "\n",
    "\n",
    "**Silverman's assumption**: The unknown density is Gaussian\n",
    "\n",
    "\n",
    "Assuming uniformly spaced bins then the number of bins is\n",
    "\n",
    "$$\n",
    "N_{bins} = \\frac{\\max(x)-\\min(x)}{h}\n",
    "$$\n",
    "\n",
    "\n",
    "### Other considerations\n",
    "\n",
    "- Bins could have different boundaries (offsets)\n",
    "- Bins could have different widths\n",
    "- Multiresolution approach (wavelet style)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel density estimation (KDE)\n",
    "\n",
    "- Other option for non-parametric density estimation is KDE\n",
    "- In KDE each point has its \"own bin\", and bins can overlap\n",
    "- KDE does not require choosing bin boundaries, only bin width\n",
    "\n",
    "The unidimensional KDE for a set $\\{x_i\\}_{i=1,\\ldots, N}$ is\n",
    "\n",
    "$$\n",
    "\\hat f_h(x) = \\frac{1}{Nh} \\sum_{i=1}^N \\kappa \\left ( \\frac{x - x_i}{h} \\right)\n",
    "$$\n",
    "\n",
    "where $h$ is called the **kernel bandwidth** or kernel size and $\\kappa(u)$ is the **kernel function** that need to be positive, zero mean and integrate to unity.\n",
    "\n",
    "For example, one broadly used kernel is \n",
    "\n",
    "$$\n",
    "\\kappa(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left ( - \\frac{u^2}{2} \\right),\n",
    "$$\n",
    "\n",
    "the Gaussian kernel. \n",
    "\n",
    "**Other widely used kernels:** Exponential, Top-hat, Epanechnikov\n",
    "\n",
    "\n",
    "\n",
    "<center><b>KDE in a nutshell</b>: Place a kernel on top of each point and get the average</center>\n",
    "\n",
    "\n",
    "\n",
    "**Avoid confusion:** \n",
    "\n",
    "- Assuming that the data is **Gaussian distributed** and doing KDE with the **Gaussian kernel** are very **different things**! \n",
    "- Using the Gaussian kernel for non-Gaussian data is perfectly fine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "fig, ax = plt.subplots(figsize=(7, 4), tight_layout=True)\n",
    "ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "line_kde = ax.plot(x, np.zeros_like(x))\n",
    "hs = 0.9*np.std(hatx)*N**(-1/5)\n",
    "def update(k, kernel): \n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=hs*k).fit(hatx.reshape(-1, 1))\n",
    "    line_kde[0].set_ydata(np.exp(kde.score_samples(x.reshape(-1, 1))))\n",
    "    \n",
    "interact(update, k=SelectionSlider_nice(description=\"$k =h/h_s$\", options=[1/8, 1/4, 1/2, 1, 2, 4], value=1),\n",
    "        kernel=SelectionSlider_nice(options=[\"gaussian\", \"exponential\", \"epanechnikov\", \"tophat\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Other non-parametric methods\n",
    "\n",
    "- Splines, kernel regression\n",
    "- Support Vector Machine and Gaussian Processes\n",
    "- Nearest neighbors\n",
    "- Neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-parametric uncertainty: Bootstrap\n",
    "\n",
    "The uncertainty of a point-estimate can be non-parametrically calculated using **bootstrap resampling**\n",
    "\n",
    "In bootstrap you generate new datasets that follow the properties of the original one \n",
    "\n",
    "<img src=\"img/bootstrap_diagram.png\">\n",
    "\n",
    "The conceptual steps are:\n",
    "\n",
    "1. Create a new set by randomly selecting $N$ observations with replacement\n",
    "1. Compute the value of your estimator on the new dataset\n",
    "1. Go back to one until have $T$ values\n",
    "1. Now you have an empirical distribution for the estimator. Use it to get a confidence interval\n",
    "\n",
    "**Note:** There are many types of bootstrap tests with different properties and assumptions (more on this in a future class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4), tight_layout=True)\n",
    "\n",
    "def update(N, T):\n",
    "    ax.cla(); ax.set_xlabel('x')\n",
    "    np.random.seed(0)\n",
    "    x = np.random.randn(N) # zero mean, unit variance\n",
    "    mle_mu = np.mean(x)    \n",
    "    mle_mu_bs = [np.mean(np.random.choice(x, size=len(x), replace=True)) for k in range(T)]\n",
    "    hist_val, hist_lim, _ = ax.hist(mle_mu_bs, density=True, alpha=0.6)\n",
    "    t = np.linspace(hist_lim[0], hist_lim[-1], num=200)\n",
    "    ax.plot(t, gaussian_pdf(t, mu=mle_mu, s=1/np.sqrt(len(x))), 'k-', linewidth=4)  \n",
    "    ax.scatter(np.mean(x), 0, c='k', s=100, zorder=100)\n",
    "    display(\"Empirical confidence interval at 0.95 = [%0.4f, %0.4f]\" %(np.sort(mle_mu_bs)[int(0.05*T)], \n",
    "                                                                       np.sort(mle_mu_bs)[int(0.95*T)]))    \n",
    "interact(update, N=SelectionSlider_nice(options=[10, 100, 1000, 10000], value=100),\n",
    "         T=SelectionSlider_nice(options=[10, 100, 1000, 10000], value=100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
