{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Ellipse\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Mixture Models\n",
    "\n",
    "\n",
    "References for today's lecture:\n",
    "\n",
    "1. Bishop, \"Pattern recognition and machine learning\", **Chapter 9**\n",
    "1. Murphy, \"Machine Learning: A Probabilistic Perspective\", *MIT Press*, 2012, **Chapter 11**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Unsupervised methods:**\n",
    "\n",
    "- Contrary to supervised (regression, classification) in this scenario there are no labels/targets\n",
    "- The objective of unsupervised methods is **to find structure in the data** \n",
    "- We do this to better understand the data: find hidden causes, find shared features\n",
    "\n",
    "\n",
    "### Clustering\n",
    "\n",
    "Group the data into clusters (sets) such that\n",
    "- \"Similar\" data samples go to the same cluster\n",
    "- Elements of one cluster are \"different\" from the elements of another cluster\n",
    "\n",
    "Clustering requires a notion of similarity\n",
    "\n",
    "### Latent variable models (LVM)\n",
    "\n",
    "- Model the data using a **cause and effect** structure \n",
    "    - Observed variables (effect): Data\n",
    "    - Latent variables (cause): Hidden variables that explains the observed variables \n",
    "- The objective is to infer the latent given the observed\n",
    "- We assume properties for the latent variable, for example\n",
    "    - Dimension of the latent is smaller than the observed &rarr; Dimensionality reduction\n",
    "    - Latent variable is continuous and Gaussian distributed &rarr; **PCA**\n",
    "    - Latent variables are not correlated (observed data might be)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mixture model\n",
    "\n",
    "In this class we focus on LVM for clustering \n",
    "\n",
    "> We assume that there are $K$ clusters (sets)\n",
    "\n",
    "We represent each data point \n",
    "\n",
    "$$\n",
    "x_i \\in \\mathbb{R}^D \\text{ with } i=1,\\ldots, N\n",
    "$$\n",
    "\n",
    "as having a **categorical latent variable**\n",
    "\n",
    "$$\n",
    "z_i \\in {1, 2, \\ldots, K}\n",
    "$$\n",
    "\n",
    "This is the same as saying that $z_i$ has a categorical prior\n",
    "\n",
    "$$\n",
    "p(z_i) = \\text{Cat}(\\vec \\pi),\n",
    "$$\n",
    "\n",
    "where $\\pi_k \\in [0, 1]$ are the mixing coefficients and $\\sum_{k=1}^K \\pi_k=1$ \n",
    "\n",
    "> **Summary:** the latent variable associates each data point with one of the $K$ clusters\n",
    "\n",
    "Another use for mixture models: **Density estimation**\n",
    "\n",
    "### Likelihood of the mixture model\n",
    "    \n",
    "We can write the likelihood as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_i | \\theta) &= \\sum_{k=1}^K p(x_i|z_i = k, \\theta) p(z_i=k| \\theta) \\nonumber \\\\\n",
    "&= \\sum_{k=1}^K p_k(x_i| \\theta_k) \\pi_k ,  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To continue we need to specify the family $p_k(x_i|\\theta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian mixture model (GMM)\n",
    "\n",
    "In the GMM we further assume that each component of the mixture is a Gaussian/Normal distribution.\n",
    "\n",
    "The we can write the likelihood as \n",
    "\n",
    "$$\n",
    "p(x_i | \\theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k),\n",
    "$$\n",
    "\n",
    "where each cluster has three parameters\n",
    "- The **centroid/prototype** of the cluster is the mean of the clusters: $\\mu_k$\n",
    "- The size and orientation of the cluster, given by the covariance: $\\Sigma_k$\n",
    "- The contribution of the cluster to the mixture (importance): $\\pi_k$\n",
    "\n",
    "> Fitting the GMM: **Finding the best value of these parameters** for all clusters given the data\n",
    "\n",
    "\n",
    "**Generative model:** Given a value for the parameters we can sample from the GMM\n",
    "\n",
    "\n",
    "Let's explore the influence of these parameters on the \"shape\" of the mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def create_mixture(pis, mus, Sigmas, N=5000, rseed=None):\n",
    "    \"\"\"\n",
    "    pis (list of floats): mixing coefficients : cluster importance\n",
    "    mus (list of arrays): mean: cluster centers\n",
    "    Sigmas (list of arrays): covariance: cluster shape\n",
    "    N (int): number of points to create\n",
    "    \"\"\"\n",
    "    if rseed is not None:\n",
    "        np.random.seed(rseed)\n",
    "    points = list()\n",
    "    labels = list()\n",
    "    for k in range(len(pis)):\n",
    "        Nc = int(pis[k]*N)\n",
    "        if len(mus[k]) > 1:\n",
    "            x = np.random.multivariate_normal(mus[k], Sigmas[k], Nc)\n",
    "        else:\n",
    "            x = mus[k] + np.sqrt(Sigmas[k])*np.random.randn(Nc)\n",
    "        points.append(x)\n",
    "        labels.append([k]*Nc)\n",
    "    return np.concatenate(points), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = create_mixture(pis=[0.4, 0.5, 0.1], \n",
    "                              mus=[[-1], [2], [2.5]], \n",
    "                              Sigmas=[0.1, 0.5, 0.1])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(6, 3), tight_layout=True)\n",
    "ax.hist(data, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = create_mixture(pis=[0.9, 0.1], \n",
    "                              mus=[[-1, -1], [1, 1]], \n",
    "                              Sigmas=[0.5*np.eye(2), [[1, -0.9],[-0.9, 1]]])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3), tight_layout=True)\n",
    "\n",
    "ax[0].scatter(data[:, 0], data[:, 1], c=labels, s=10, cmap=plt.cm.coolwarm,\n",
    "              linewidths=0.1, edgecolors='k', vmin=0, vmax=1);\n",
    "ax[1].hist2d(x=data[:, 0], y=data[:, 1], bins=30, cmap=plt.cm.Spectral_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Posterior probability a.k.a. picking a cluster\n",
    "\n",
    "Let's assume that we know the parameters of the mixture\n",
    "\n",
    "Typically we want to infer the most probable cluster for a given data point\n",
    "\n",
    "> This is given by the posterior probability of $z$ given $x$\n",
    "\n",
    "\n",
    "Using the Bayes rule and our previous assumptions (likelihood) we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{ik} = p(z_i = k|x_i, \\theta) &= \\frac{ p(x_i|z_i = k, \\theta) p(z_i=k| \\theta)}{p(x_i)} \\nonumber \\\\\n",
    "&= \\frac{ p(x_i|z_i = k, \\theta) p(z_i=k| \\theta)}{ \\sum_{k=1}^K p(x_i|z_i = k, \\theta) p(z_i=k| \\theta) } \\nonumber \\\\\n",
    "&= \\frac{ \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)  \\pi_k}{ \\sum_{k=1}^K \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\pi_k }, \\nonumber\\end{align}\n",
    "$$\n",
    "\n",
    "this posterior is also called the **responsibility** $r_{ik}$ of cluster $k$ for point $i$\n",
    "\n",
    "Note that it is a **soft-cluster assignment**\n",
    "\n",
    "In some cases we only need a **hard-cluster assignment**, *i.e.* the single most probable cluster, which can be obtained as\n",
    "\n",
    "$$\n",
    "c_{i} = \\text{arg}\\max_k r_{ik} = \\text{arg}\\max_k \\log \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) + \\log \\pi_k,\n",
    "$$\n",
    "\n",
    "because of the maximum operator we can omit the evidence and use the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def jointGMM(data, pi, mu, Sigma):\n",
    "    # Assume 2 dimensional data\n",
    "    data_centered = data - mu\n",
    "    norm = pi/(2*np.pi*np.sqrt(np.linalg.det(Sigma)))\n",
    "    return norm*np.exp(-0.5*np.sum(np.multiply(data_centered, np.linalg.solve(Sigma, data_centered.T).T), axis=1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3), tight_layout=True)\n",
    "\n",
    "pis = [0.6, 0.4]; mus=[[-1, -1], [1, 1]]; \n",
    "Sigmas=[0.5*np.eye(2), [[1, -0.3],[-0.3, 1]]]\n",
    "data, labels = create_mixture(pis, mus, Sigmas, rseed=0)\n",
    "\n",
    "posterior = np.zeros(shape=(len(data), len(pis)))\n",
    "for k in range(len(pis)):\n",
    "    posterior[:, k] = jointGMM(data, pis[k], mus[k], Sigmas[k])\n",
    "posterior = posterior/np.sum(posterior, axis=1)[:, np.newaxis]\n",
    "\n",
    "ax[0].scatter(data[:, 0], data[:, 1], c=posterior[:, 0], s=10, \n",
    "              cmap=plt.cm.coolwarm,\n",
    "              linewidths=0.1, edgecolors='k', vmin=0, vmax=1); ax[0].set_title(\"Soft label\")\n",
    "ax[1].scatter(data[:, 0], data[:, 1], c=np.argmax(posterior, axis=1), s=10, \n",
    "              cmap=plt.cm.coolwarm,\n",
    "              linewidths=0.1, edgecolors='k', vmin=0, vmax=1); ax[1].set_title(\"Hard label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How do we fit the GMM?\n",
    "\n",
    "We want to find the optimal point estimates of the parameters of a statistical model \n",
    "\n",
    "Can use **MLE**? The log likelihood in this case is \n",
    "\n",
    "$$\n",
    "\\log L(\\pi,\\mu,\\Sigma) = \\sum_{i=1}^N \\log \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "**Problem** With the exception of $K=1$, we won't be able to find analytic expressions for the parameters\n",
    "\n",
    "\n",
    "> In general when we don't have complete data, i.e. there are missing or hidden variables, the posterior does not factorize\n",
    "\n",
    "In this case we have the latent/hidden variables $z_i$, we need to marginalize it (sum over k) to compute the likelihood\n",
    "\n",
    "Other problems of the MLE/MAP\n",
    "\n",
    "- Non-convex, it has local optima (Murphy 11.3.2)\n",
    "- Very hard to compute (NP-complete hard!)\n",
    "- **Unidentifiability:** There exist $K!$ configurations that yield the exact same solution (label switch)\n",
    "- **Singularities:** A component may collapse to a single point ($\\Sigma \\to 0$ and $\\log L \\to \\infty$)\n",
    "\n",
    "What can we do? \n",
    "\n",
    "- Gradient descent with constraints\n",
    "- Approximate MLE estimation: **Expectation Maximization (EM)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectation Maximization (EM)\n",
    "\n",
    "The MLE solution can be computed if we **\"fully-observe\"** the data \n",
    "\n",
    "> We know $z_i$ for each $x_i$, *i.e.* we know which cluster owns $x_i$\n",
    "\n",
    "The **fully-observed** log likelihood (FOLL) is\n",
    "\n",
    "$$\n",
    "\\log L_{\\text{FO}}(\\theta) = \\sum_{i=1}^N \\log p(z_i) p(x_i|z_i, \\theta), \n",
    "$$\n",
    "\n",
    "but in practice we do not observe $z$ so we marginalize it by computing the **expected value** of the FOLL given the posterior\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(\\theta, \\theta^{\\text{old}}) &= \\mathbb{E}_{p(z|x, \\theta^{\\text{old}})} \\left[\\log L_{\\text{FO}}(\\theta) \\right]   \\nonumber \\\\\n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N p(k|x_i, \\theta^{\\text{old}})  \\log p(k) p(x_i|\\theta_k) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is called the **auxiliary function** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we update our parameters by **maximizing** \n",
    "\n",
    "$$\n",
    "\\theta^{\\text{new}} = \\text{arg}\\max_\\theta Q(\\theta, \\theta^{\\text{old}}),\n",
    "$$\n",
    "\n",
    "and finally we do $\\theta^{\\text{old}} \\leftarrow \\theta^{\\text{new}}$ and repeat the procedure until convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iterative procedure of\n",
    "\n",
    "1. **E-step:** Estimating the expected value of the likelihood given our current parameters\n",
    "1. **M-step:** Maximizing the estimator and updating our parameters\n",
    "\n",
    "\n",
    "is called **Expectation Maximization** (EM)\n",
    "\n",
    "\n",
    "EM is a general algorithm:\n",
    "\n",
    "- it can be used for other problems with non-analytical solution/non-totally observed data\n",
    "- it has many variants: Incremental, Variational, Monte-Carlo (Murphy 11.4.8 and 11.4.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EM updates for the GMM\n",
    "\n",
    "Let's start by defining the auxiliary function for the GMM\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(\\theta, \\theta^{\\text{old}}) \n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N p(k|x_i, \\theta^{\\text{old}})  \\log p(k) p(x_i|\\theta_k)  \\nonumber \\\\\n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N r_{ik}^{\\text{old}}  \\left(\\log \\pi_k + \\log \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right),\\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $r_{ik}^{\\text{old}}$ is the responsibility given the parameters of the previous iteration\n",
    "\n",
    "Now we maximize $Q$ to get an estimator of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the means\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d Q}{d\\mu_{k'}} &= \\frac{d}{d\\mu_{k'}} \\sum_{k=1}^K \\sum_{i=1}^N r_{ik}^{\\text{old}}  \\left(\\log \\pi_k + \\log \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N r_{ik'}^{\\text{old}}  \\frac{d}{d\\mu_{k'}}  \\log \\mathcal{N}(x_i|\\mu_{k'}, \\Sigma_{k'})\\nonumber \\\\\n",
    "&= \\sum_{i=1}^N r_{ik'}^{\\text{old}}  \\frac{d}{d\\mu_{k'}} \\left( -\\frac{1}{2} (x_i - \\mu_{k'})^T \\Sigma_{k'}^{-1} (x_i - \\mu_{k'}) -\\frac{D}{2} \\log(2\\pi) -\\frac{1}{2} \\log(|\\Sigma_{k'}|) \\right)\\nonumber \\\\\n",
    "&= -\\frac{1}{2} \\sum_{i=1}^N r_{ik'}^{\\text{old}}  \\frac{d}{d\\mu_{k'}} (x_i - \\mu_{k'})^T \\Sigma_{k'}^{-1} (x_i - \\mu_{k'})  \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N r_{ik'}^{\\text{old}} \\Sigma_{k'}^{-1} (x_i - \\mu_{k'})  = 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and from here we can show that the new value of the cluster mean is given by\n",
    "\n",
    "$$\n",
    "\\mu_{k}^{\\text{new}}  = \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Following the same procedure for the covariance we arrive to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma_k^{\\text{new}} &= \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} (x_i - \\mu_k) (x_i - \\mu_k)^T \\nonumber \\\\\n",
    "&= -\\mu_k \\mu_k^T  + \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i x_i^T   \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, we incorporate the constraint that $\\sum_k \\pi_k = 1$ to arrive to \n",
    "\n",
    "$$\n",
    "\\pi_k^{\\text{new}} = \\frac{\\sum_{i=1}^N r_{ik}^{\\text{old}}}{N}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a GMM using EM\n",
    "\n",
    "The following algorithms summarizes the application of EM to the GMM problem\n",
    "\n",
    "- **(1)** Set initial conditions\n",
    "- **(2)** Compute cluster responsibilities for each data point using\n",
    "\n",
    "$$\n",
    "r_{ik}^{\\text{old}} = \\frac{ \\mathcal{N}(x_i|\\mu_k^{\\text{old}}, \\Sigma_k^{\\text{old}})  \\pi_k^{\\text{old}}}{ \\sum_{k=1}^K \\mathcal{N}(x_i|\\mu_k^{\\text{old}}, \\Sigma_k^{\\text{old}}) \\pi_k^{\\text{old}} }\n",
    "$$\n",
    "\n",
    "- **(3)** Compute the new values for the parameters using\n",
    "\n",
    "$$\n",
    "\\mu_{k}^{\\text{new}}  = \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_k^{\\text{new}} = -\\mu_k \\mu_k^T  + \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i x_i^T , \n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\pi_k^{\\text{new}} = \\frac{\\sum_{i=1}^N r_{ik}^{\\text{old}}}{N}\n",
    "$$\n",
    "\n",
    "- **(4)** Update the parameters $\\theta^{\\text{old}} \\leftarrow \\theta^{\\text{new}}$\n",
    "\n",
    "- **(5)** If convergence criterion is met stop otherwise go back to **2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example using \"Old faithful\" data\n",
    "\n",
    "Iteration by iteration  (Bishop Fig. 9.8)\n",
    "<img src=\"img/gmm_bishop.png\" width=\"700\">\n",
    "\n",
    "Animation (wikipedia)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Practical details of EM\n",
    "\n",
    "**Initialization and restarts** \n",
    "\n",
    "The EM procedure is guaranteed to converge to a *stationary point* (local minimum/maximum or saddle point)\n",
    "\n",
    "- One can obtain a better solution using the following heuristic:\n",
    "    1. Set a **number of retries**\n",
    "    1. Run GMM with a random initialization\n",
    "    1. If the likelihood of this solution is larger than the previous replace best\n",
    "    1. Repeat until **number of retries** is achieved\n",
    "- Another option is to initialize with the solution from a simpler algorithms (*e.g.* k-means)\n",
    "    - This can avoid catastrophic results\n",
    "    - But it can also limit exploration \n",
    " \n",
    "\n",
    "**Number of clusters**\n",
    "\n",
    "The number of clusters $K$ is a parameter set *a priori* by the user\n",
    "\n",
    "> How do we know what value of $K$ is good?\n",
    "\n",
    "Option 1: The Elbow method, \n",
    "- Plot log likelihood (in the validation set) as a function of $K$\n",
    "- log likelihood may keep decreasing with $K$ (overfit)\n",
    "- but at some point the decrease is negligible: find the **elbow** of the curve\n",
    "\n",
    "<img src=\"img/elbow.png\" width=\"500\">\n",
    "\n",
    "Option 2: Use Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), or other criterion for model selection that penalizes complexity\n",
    "\n",
    "Option 3: [The Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n",
    "\n",
    "\n",
    "**Convergence**\n",
    "\n",
    "EM is an iterative procedure\n",
    "\n",
    "> How many times do we repeat it?\n",
    "\n",
    "- Check log likelihood vs iteration\n",
    "- For example: stop if log likelihood increment is less than 0.1\\% wrt to previous\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Special cases of GMM: The shape of the covariance\n",
    "\n",
    "The covariance $\\Sigma \\in \\mathbb{R}^{D\\times D}$, where $D$ is the dimension of the data, defines the **shape** and **size** of the clusters\n",
    "\n",
    "Instead of a full covariance (maximum flexibility) you could impose constraints such as\n",
    "1. Spherical covariance: $\\Sigma = \\sigma^2 \\text{I}$, $\\sigma \\in \\mathbb{R}_{+}$\n",
    "1. Diagonal covariance: $\\Sigma = \\vec \\sigma^2 \\text{I}$, $\\vec \\sigma \\in \\mathbb{R}_{+}^D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3), sharex=True, sharey=True, tight_layout=True)\n",
    "\n",
    "eps = np.random.randn(2, 5000)\n",
    "\n",
    "def update(s1, s2, rho):\n",
    "    Sigma = [[s1**2, rho*s1*s2], [rho*s1*s2, s2**2]]\n",
    "    data = np.dot(np.linalg.cholesky(Sigma), eps).T\n",
    "    for ax_ in ax:\n",
    "        ax_.cla(); \n",
    "    ax[0].scatter(data[:, 0], data[:, 1], s=10, \n",
    "                  linewidths=0.1, edgecolors='k', vmin=0, vmax=1);\n",
    "    ax[1].hist2d(x=data[:, 0], y=data[:, 1], range=[[-6, 6], [-6, 6]], \n",
    "                 bins=50, cmap=plt.cm.Blues);\n",
    "    \n",
    "interact(update, \n",
    "         s1=FloatSlider(min=0.5, max=2.), \n",
    "         s2=FloatSlider(min=0.5, max=2.),\n",
    "         rho=FloatSlider(min=-0.9, max=0.9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture with [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `sklearn.mixture` scikit learn module\n",
    "\n",
    "```python\n",
    "GaussianMixture(n_components=1, # Number of clusters\n",
    "                covariance_type='full', # Shape of the covariance\n",
    "                tol=0.001, # termination tolerance of EM\n",
    "                reg_covar=1e-06, # small number added to the diagonal of the covariance\n",
    "                max_iter=100, # Max number of EM iterations\n",
    "                n_init=1, # Number of random initializations\n",
    "                init_params='kmeans', # Initialization for the responsabilities (kmeans or random)\n",
    "                ...\n",
    "               )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = create_mixture(pis=[0.33, 0.33, 0.34], \n",
    "                              mus=[[-3, 0], [0, 0], [3 ,0]], \n",
    "                              Sigmas=[[[1, 0.9],[0.9, 1]], \n",
    "                                      [[1, -0.9],[-0.9, 1]], \n",
    "                                      [[1, 0.9],[0.9, 1]]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.scatter(data[:, 0], data[:, 1], s=10, cmap=plt.cm.coolwarm,\n",
    "           linewidths=0.1, edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best number of clusters using the Bayesian information criterion (BIC). A model with lowest BIC is preferred\n",
    "\n",
    "$$\n",
    "BIC = K \\log N - 2 \\log L\n",
    "$$\n",
    "\n",
    "The BIC grows with $k$ the number of parameters and number of samples $n$ and decreases with the maximum value of the log likelihood\n",
    "\n",
    "Try spherical, diagonal and full covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "maxK = 20; \n",
    "cov = 'full' \n",
    "metrics = np.zeros(shape=(maxK-1, ))\n",
    "for i, K in enumerate(range(2, maxK+1)):\n",
    "    gmm = GaussianMixture(n_components=K, covariance_type=cov, n_init=3)\n",
    "    gmm.fit(data)\n",
    "    metrics[i] = gmm.bic(data)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(range(2, maxK+1), metrics, linestyle='--', marker='o');\n",
    "ax.set_ylabel('BIC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect results\n",
    "\n",
    "- Cluster shapes and cluster assignments\n",
    "- uncertainty in the responsibilities\n",
    "- Generative results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=10, covariance_type=cov, n_init=3)\n",
    "gmm.fit(data)\n",
    "hatr = gmm.predict(data); \n",
    "hatdata = gmm.sample(n_samples=10000)\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(6, 8), sharex=True, sharey=True, tight_layout=True)\n",
    "\n",
    "ax[0].scatter(data[:, 0], data[:, 1], c=hatr, s=5, cmap=plt.cm.tab20,\n",
    "              linewidths=0.1, edgecolors='k');\n",
    "ax[0].set_title('Arg max of responsibilities')\n",
    "\n",
    "p = gmm.predict_proba(data)\n",
    "H = -np.sum(p*np.log(p), axis=1)\n",
    "ax[1].scatter(data[:, 0], data[:, 1], c=H, s=10, cmap=plt.cm.Reds,\n",
    "              linewidths=0.1, edgecolors='k');\n",
    "ax[1].set_title('Entropy of of responsibilities')\n",
    "\n",
    "ax[2].scatter(hatdata[0][:, 0], hatdata[0][:, 1], s=5, alpha=0.1)\n",
    "ax[2].set_title('Generated data')\n",
    "\n",
    "ax[2].scatter(gmm.means_[:, 0], gmm.means_[:, 1], s=50, c='k', marker='d')\n",
    "\n",
    "for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "    if cov == 'full':\n",
    "        U, s, Vt = np.linalg.svd(covar)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    elif cov == 'diag':\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covar)    \n",
    "    elif cov == 'spherical':\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covar), 2 * np.sqrt(covar)     \n",
    "    \n",
    "    ax[2].add_patch(Ellipse(pos, 2.5*width, 2.5*height, angle, color='k', alpha=w))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Appendix: Relation to K-means\n",
    "\n",
    "The K-means algorithm is a GMM with the following constraints\n",
    "\n",
    "- Spherical clusters: $\\Sigma = \\sigma^2 \\text{I}$, $\\sigma \\in \\mathbb{R}_{+}$\n",
    "- All clusters have equal size: $\\sigma^2 = \\sigma_1^2 = \\sigma_2^2 = \\ldots = \\sigma_K^2$\n",
    "- Uniform prior for the mixture coefficients: $\\pi_k = \\frac{1}{K}$\n",
    "- Hard labels are used instead of responsibilities\n",
    "\n",
    "The multivariate normal for spherical clusters\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x_i|\\mu_k, \\Sigma_k = \\sigma^2 I) = \\frac{1}{\\sqrt{(2\\pi)^D} \\sigma} \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "The update rule for the responsibility \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat r_{ik}^{\\text{old}} &= \\text{arg} \\max_k \\frac{ \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right) }{ \\sum_{k=1}^K \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right)  } \\nonumber \\\\\n",
    "&=  \\text{arg} \\max_k  \\log \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right)  \\nonumber \\\\\n",
    "&=  \\text{arg} \\min_k  \\frac{1}{2\\sigma^2}  \\|x_i - \\mu_k\\|^2   \\nonumber \\\\\n",
    "&=  \\text{arg} \\min_k  \\|x_i - \\mu_k\\|^2   \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the means\n",
    "$$\n",
    "\\mu_{k}^{\\text{new}}  = \\frac{1}{\\sum_{i=1}^N \\hat r_{ik}^{\\text{old}}} \\sum_{i=1}^N \\hat r_{ik}^{\\text{old}} x_i\n",
    "$$\n",
    "\n",
    "\n",
    "Hence the EM for K-means reduces to\n",
    "\n",
    "1. Update \"hard\" responsibilities\n",
    "1. Update the cluster means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational mixture of Gaussians (Bishop 10.2, Murphy 21)\n",
    "\n",
    "In EM the parameters are point estimates (similar to MLE and MAP)\n",
    "\n",
    "But in a fully Bayesian LVM the parameters are distributions that we estimate (posterior) given some assumptions (priors)\n",
    "\n",
    "$$\n",
    "p(z, \\theta |x) = \\frac{p(x|z,\\theta)p(z, \\theta)}{\\int p(x|z,\\theta)p(z, \\theta) dz d\\theta}\n",
    "$$\n",
    "\n",
    "but computing this integral is intractable (sum over all combinations of z)\n",
    "\n",
    "We have two choices: stochastic approximation (MCMC) or deterministic approximation\n",
    "\n",
    "**Variational Bayes** is an example of the latter\n",
    "\n",
    "VB approximates intractable posteriors with analytical factorized distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/VI.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p(z|x)$ is the intractable posterior\n",
    "- $q_\\nu(z)$ is a family of \"easier\" factorized posterior \n",
    "- We look for $\\nu$ that maximizes a lower bound on the KL divergence between posteriors\n",
    "- This topic is fully explored in INFO320!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Priors for the GMM\n",
    "\n",
    "In GMM we set priors for the weights $\\pi$, means $\\mu$ and covariance $\\Sigma$\n",
    "- For $\\mu$ we use a normal prior\n",
    "- For $\\Sigma$ we use Wishart (covariance) or a mix between LKJ (correlation) and half normal (variance)\n",
    "- For the weights a Dirichlet prior is used\n",
    "  \n",
    "**Recap on Dirichlet distribution**\n",
    "- Two parameters: K (number of categories) and $\\alpha_k$ concentrations\n",
    "- Its support is the simplex: $x_1, x_2, \\ldots, x_K$, $x_k \\in [0,1]$ and $\\sum_k x_k = 1$\n",
    "- By setting the concentration (hyperparameter) we can choose between have equally important clusters or a few active clusters\n",
    "\n",
    "<img src=\"img/dirichlet_plot.png\" width=\"600\">\n",
    "          \n",
    "[Picture source](http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example: Variational GMM** \n",
    "\n",
    "- Using a large number of clusters $K$ but with a small concentration $\\alpha_k = 0.01$\n",
    "- Clusters with negligible concentration are not plotted\n",
    "\n",
    "<img src=\"img/gmmvi_bishop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational GMM with [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html)\n",
    "\n",
    "```python\n",
    "BayesianGaussianMixture(...,\n",
    "                        weight_concentration_prior_type='dirichlet_process',\n",
    "                        weight_concentration_prior=None,\n",
    "                        mean_precision_prior=None,\n",
    "                        mean_prior=None,\n",
    "                        degrees_of_freedom_prior=None,\n",
    "                        covariance_prior=None,\n",
    "                        ...\n",
    "                       )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BayesianGaussianMixture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "data, labels = create_mixture(pis=[0.33, 0.33, 0.34], \n",
    "                              mus=[[-3, 0], [0, 0], [3 ,0]], \n",
    "                              Sigmas=[[[1, 0.9],[0.9, 1]], \n",
    "                                      [[1, -0.9],[-0.9, 1]], \n",
    "                                      [[1, 0.9],[0.9, 1]]])\n",
    "\n",
    "\n",
    "gmm = BayesianGaussianMixture(n_components=10 , \n",
    "                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "                              weight_concentration_prior=10.,\n",
    "                              covariance_type=\"full\", max_iter=1000, n_init=5)\n",
    "gmm.fit(data)\n",
    "hatr = gmm.predict(data)\n",
    "print(np.unique(hatr))\n",
    "hatdata = gmm.sample(n_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, figsize=(6, 8), sharex=True, sharey=True, tight_layout=True)\n",
    "\n",
    "ax[0].scatter(data[:, 0], data[:, 1], c=hatr, s=5, cmap=plt.cm.tab20,\n",
    "              linewidths=0.1, edgecolors='k');\n",
    "ax[0].set_title('Arg max of responsibilities')\n",
    "\n",
    "p = gmm.predict_proba(data)\n",
    "H = -np.sum(p*np.log(p), axis=1)\n",
    "ax[1].scatter(data[:, 0], data[:, 1], c=H, s=10, cmap=plt.cm.Reds,\n",
    "              linewidths=0.1, edgecolors='k');\n",
    "ax[1].set_title('Entropy of of responsibilities')\n",
    "\n",
    "ax[2].scatter(hatdata[0][:, 0], hatdata[0][:, 1], s=5, alpha=0.1)\n",
    "ax[2].set_title('Generated data')\n",
    "\n",
    "ax[2].scatter(gmm.means_[:, 0], gmm.means_[:, 1], s=50, c='k', marker='d')\n",
    "\n",
    "for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "    U, s, Vt = np.linalg.svd(covar)\n",
    "    angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "    width, height = 2 * np.sqrt(s)    \n",
    "    ax[2].add_patch(Ellipse(pos, 2.5*width, 2.5*height, angle, color='k', alpha=w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## More material\n",
    "\n",
    "- EM for mixture of Bernoulli distributions: Bishop 9.3.3\n",
    "- EM for mixture of student-t distributions: Murphy 11.4.5\n",
    "- Online EM: Murphy 11.4.8\n",
    "- GMM with priors (MAP): Murphy 11.4.2.8\n",
    "- [Scikit-learn article: Setting the concentration prior](https://scikit-learn.org/stable/auto_examples/mixture/plot_concentration_prior.html)\n",
    "- [Variational GMM in pymc3](https://docs.pymc.io/notebooks/gaussian-mixture-model-advi.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (astro)",
   "language": "python",
   "name": "astro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
