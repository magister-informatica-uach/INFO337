{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Make fonts readable at 1024x768 -->\n",
    "<style>\n",
    ".rendered_html { font-size:0.7em; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and matplotlib configuration\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "from ipywidgets import interact, Button, Output, Box\n",
    "from IPython.display import display\n",
    "from style import *\n",
    "\n",
    "# Others\n",
    "from scipy.special import erf\n",
    "gaussian_pdf = lambda x, mu=0, s=1: np.exp(-0.5*(x-mu)**2/s**2)/(s*np.sqrt(2*np.pi))\n",
    "gaussian_cdf = lambda x, mu=0, s=1: 0.5 + 0.5*erf((x-mu)/(s*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universidad Austral de Chile\n",
    "\n",
    "# INFO337 - Herramientas estadísticas para la investigación\n",
    "\n",
    "# Statistical inference \n",
    "\n",
    "### A course of the masters in informatics program\n",
    "\n",
    "### https://github.com/magister-informatica-uach/INFO337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"index\"></a>\n",
    "\n",
    "# Contents\n",
    "***\n",
    "\n",
    "1. Statistical inference\n",
    "1. [Maximum Likelihood Estimation](#MLE)\n",
    "1. [Non-parametric modeling](#nonparametric)\n",
    "1. [Bayesian approach on parametric modeling](#bayesian)\n",
    "1. [Maximum a posteriori](#MAP)\n",
    "1. [Appendix](#appendix)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. Hastie, Tibshirani and Friedman, \"The elements of statistical learning\" 2nd Ed., *Springer*, **Chapter 8**\n",
    "1. Murphy, \"Machine Learning: A Probabilistic Perspective\", *MIT Press*, 2012, **Chapter 5**\n",
    "1. Ivezic, Connolly, VanderPlas and Gray, \"Statistic, Data Mining, and Machine Learning in Astronomy\", *Princeton University Press*, 2014, **Chapters 4 and 5**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "# Statistical inference\n",
    "***\n",
    "\n",
    "**Inference:** \n",
    "\n",
    "<center>*Draw conclusions from facts through a scientific premise*</center>\n",
    "\n",
    "**Statistical inference**:\n",
    "- Facts: Observed data\n",
    "- Premise: Probabilistic model\n",
    "- Conclusion: An unobserved quantity of interest\n",
    "- Objective: Quantify the uncertainty of the conclusion given the data and the premise\n",
    "\n",
    "\n",
    "Examples of statistical inference tasks:\n",
    "- **Parameter estimation:** What is the best estimate of a model parameter based on the observed data?\n",
    "- **Confidence estimation:** How trustworthy is our point estimate?\n",
    "- **Hypothesis testing:** Is the data consistent with a given hypothesis or model?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "## Parametric and nonparametric models\n",
    "\n",
    "To conduct inference we start by defining a statistical model. Models can be broadly classified as:\n",
    "\n",
    "- **Parametric models:** \n",
    "    - It corresponds to an analytical function  (distribution) with free parameters\n",
    "    - Has an *a-priori* fixed number of parameters\n",
    "    - In general: Stronger assumptions, easier to interpret, faster to use\n",
    "    \n",
    "    \n",
    "- **Non-parametric models:** \n",
    "    - Distribution-free model but they do have parameters and assumptions (e.g. dependence)\n",
    "    - The number of parameters depends on the amount of training data\n",
    "    - In general: More flexible, harder to train\n",
    "    \n",
    "**Statistical modeling how to's**\n",
    "- How to collect the data?\n",
    "- How to construct a probabilistic model?\n",
    "- How to incorporate expert (*a priori*) knowledge?\n",
    "- How to interpret results? How to make predictions from data?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "## Frequentist and Bayesian inference\n",
    "\n",
    "There are two paradigms or perspectives for statistical inference: Frequentist (F) or classical and Bayesian (B). \n",
    "\n",
    "There are conceptual differences between these paradigms, for example\n",
    "\n",
    "**Definition of probability:**\n",
    "- F: Relative frequency of an event. An objective property of the real world\n",
    "- B: Degree of subjective belief. Probability statements can be made not only on data but also on parameters and models themselves\n",
    "\n",
    "**Interpretation of parameters:**\n",
    "- F: They are unknown and fixed constants\n",
    "- B: They have distributions that quantify the uncertainty of our knowledge about them. We can compute expected values of the parameters\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "## Frequentist approach on parametric modeling \n",
    "\n",
    "**Parametric inference:** We assume that observations follow a distribution, *i.e.* observations are a realization of a random process (sampling) \n",
    "\n",
    "The conceptual (iterative) steps of parametric inference are:\n",
    "1. **Model fitting:** Find parameters by fitting data to the current model\n",
    "1. **Model proposition:** Propose a new model that accommodates important features of the data better than the previous one\n",
    "\n",
    "In the frequentist approach Step 1 is typically solved using **Maximum Likelihood Estimation (MLE)**, Method of Moments (MoM) or the M-estimator. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "# The likelihood function\n",
    "***\n",
    "- The likelihood is a quantitative description of our experiment (measuring process)\n",
    "- The likelihood is the starting point for **parametric modeling** in both F and B paradigms\n",
    "- The likelihood tells us how good our model is with respect to the **observed data**\n",
    "\n",
    "\n",
    "### Formally speaking\n",
    "\n",
    "- We have an experiment that we model as a set of R.Vs $X_1, X_2, \\ldots, X_N$\n",
    "- We have observations/realizations from our R.Vs $\\{x_i\\} = x_1, x_2, \\ldots, x_N$\n",
    "- We assume that the R.Vs follow a particular probability distribution $x_i \\sim f(x_i, \\theta)$\n",
    "- The distribution has (unknown) parameters $\\theta$\n",
    "- The likelihood is a function of the parameters which is defined from the joint distribution\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= P(X_1=x_1, X_2=x_2, \\ldots, X_N=x_n) \\nonumber \\\\\n",
    "&= f(x_1, x_2, \\ldots, x_N | \\theta) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "- Assuming that our observations are **independent and identically distributed** (iid)\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= f(x_1| \\theta) \\cdot f(x_2| \\theta) \\cdot \\ldots \\cdot f(x_N| \\theta) \\nonumber \\\\\n",
    "&= \\prod_{i=1}^N f(x_i| \\theta) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "<center>\"Given $\\{x_i\\}$, How likely is it that it was generated by $L(\\theta)=\\prod_{i=1}^N f(x_i| \\theta)$?\"</center>\n",
    "<center>\"Given $\\{x_i\\}$, How likely is it that the unknown parameter was $\\theta$?\"</center>\n",
    " \n",
    "\n",
    "***\n",
    "\n",
    "### Note: Likelihood is not probability\n",
    "\n",
    "- The likelihood of a single value is given by the true pdf \n",
    "- The likelihood of a set is not normalized to 1, *i.e.* in general the likelihood is not a valid pdf\n",
    "- The likelihood by itself cannot be interpreted as a probability of $\\theta$\n",
    "- Given a fixed data set the likelihood is defined as a function of $\\theta$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "**Example: Likelihood of a single Gaussian dist. sample** \n",
    "\n",
    "If our observation (data point) $x$ was drawn from $\\mathcal{N}(\\mu, \\sigma^2)$, *i.e.* $f(x|\\theta) =  \\mathcal{N}(\\mu, \\sigma^2)$ then the likelihood of $x$ is\n",
    "\n",
    "$$\n",
    "L(\\theta=\\{\\mu, \\sigma^2\\}) = f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left ( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 3))\n",
    "x = np.linspace(-10, 10, num=10000)\n",
    "def update(xi, mu, sigma):\n",
    "    p = gaussian_pdf(x, mu, sigma)\n",
    "    ax.cla();\n",
    "    ax.plot(x, p); ax.fill_between(x, 0, p, alpha=0.5)\n",
    "    likelihood = gaussian_pdf(xi, mu, sigma)\n",
    "    ax.plot([xi, xi], [0, likelihood], 'k--')\n",
    "    ax.scatter(xi, likelihood, color='k', s=100, zorder=100)\n",
    "    ax.set_xlim([-5, 5]); ax.set_ylim([0, np.amax(p)*1.1])\n",
    "    ax.set_title(\"Likelihood $\\mu$ and $\\sigma$ given $x$=%0.2f: %0.2e\" %(xi, likelihood))\n",
    "\n",
    "interact(update, \n",
    "         xi=FloatSlider_nice(description=r\"$x$\", min=-5, max=5, value=0.), \n",
    "         mu=FloatSlider_nice(description=r\"$\\mu$\", min=-3, max=3, value=0.), \n",
    "         sigma=FloatSlider_nice(description=r\"$\\sigma$\", min=0.1, max=2., value=1.));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "**Example: Likelihood of N Gaussian dist. samples ***\n",
    "\n",
    "- Let's say we have N random numbers and assume they are Gaussian *iid*\n",
    "- We can compute their likelihood using the formula above for a given set of parameters:\n",
    "\n",
    "$$\n",
    "L(\\theta=\\{\\mu, \\sigma^2\\}) = f(\\{x\\} | \\mu, \\sigma^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left ( -\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "mu_hat = np.linspace(-2.2, 2.2, num=200); s_hat = np.linspace(0.2, 2.2, num=200)\n",
    "X, Y = np.meshgrid(mu_hat, s_hat)\n",
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4));\n",
    "cax = make_axes_locatable(ax).append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "def update(N, mu, sigma, seed):\n",
    "    ax.cla(); logL = np.zeros(shape=X.shape);\n",
    "    ax.set_xlabel(r\"$\\mu$\"); ax.set_ylabel(r\"$\\sigma$\")\n",
    "    np.random.seed(seed); xhat = mu + sigma*np.random.randn(N)\n",
    "    for i, mu_hat_ in enumerate(mu_hat):\n",
    "        for j, s_hat_ in enumerate(s_hat):\n",
    "            logL[j, i] = -0.5*len(xhat)*np.log(2.*np.pi*s_hat_**2) - 0.5*np.sum((xhat-mu_hat_)**2)/s_hat_**2\n",
    "    levels = [k*np.amax(logL) for k in np.logspace(0, 0.5, num=20)]\n",
    "    ax.scatter(mu, sigma, s=100, c='k', zorder=100)\n",
    "    CS = ax.contour(X, Y, (logL), levels=levels[::-1], cmap=plt.cm.Blues, linewidths=3); \n",
    "    fig.colorbar(CS, cax=cax)\n",
    "    \n",
    "interact(update, \n",
    "         N=SelectionSlider_nice(options=[10, 100, 1000]),\n",
    "         mu=FloatSlider_nice(description=r\"$\\mu$\", min=-2, max=2, value=0.), \n",
    "         sigma=FloatSlider_nice(description=r\"$\\sigma$\", min=0.5, max=2., value=1.),\n",
    "         seed=IntSlider_nice(min=0, max=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The value of the likelihood itself does not hold much meaning\n",
    "- But it can be used to make comparisons between different parameter vectors/models\n",
    "- **The larger the likelihood the better the model**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "<a id=\"MLE\"></a>\n",
    "\n",
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "***\n",
    "In parametric modeling we are interested in finding $\\theta$ that best fit our observations. \n",
    "\n",
    "One method to do this is **MLE**:\n",
    "\n",
    "1. Select a distribution/model for the observations and formulate the likelihood $L(\\theta)$\n",
    "1. Search for the $\\theta$ that maximize $L(\\theta)$ given the data\n",
    "$$\n",
    "\\hat \\theta = \\text{arg} \\max_\\theta L(\\theta),\n",
    "$$\n",
    "where the point estimate $\\hat \\theta$  is called the **maximum likelihood estimator** of $\\theta$\n",
    "1. Determine the confidence region of $\\hat \\theta$ either analytically or numerically (bootstrap, cross-validation)\n",
    "1. Make conclusions about your model (hypothesis test)\n",
    "\n",
    "\n",
    "**Important**: A wrong assumption in step 1 can ruin your inference. How to select a model?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Example: MLE  for the mean of a Gaussian distribution\n",
    "\n",
    "Let us:\n",
    "- consider a set of N measurements $\\{x_i\\}_{i=1,\\ldots, N}$ which corresponds to my weight :)\n",
    "- assume that the instrument used to measure weight has an error that follows a Gaussian distribution with variance $\\sigma^2$\n",
    "\n",
    "**System interpretation:** The measurements can be viewed as noisy realizations of the true weight $\\mu$\n",
    "\n",
    "$$\n",
    "x_i = \\mu + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "hence \n",
    "\n",
    "$$\n",
    "f(x_i) = \\mathcal{N}(x_i |\\mu,\\sigma^2) \\quad \\forall i\n",
    "$$\n",
    "\n",
    "The likelihood of the the true weight $\\mu$ given the measurements and the variance $\\sigma^2$ is \n",
    "$$\n",
    "L(\\mu) = f(\\{x_i\\}| \\mu, \\sigma^2) = \\prod_{i=1}^N f(x_i| \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\prod_{i=1}^N  \\exp  \\left( -\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "**Objective:** Find the value of $\\mu$ that maximize the likelihood given $\\{x_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "**Trick of the trade: The log likelihood** \n",
    "- In many cases (exponential family) it is more practical to find the maximum of the logarithm of the likelihood\n",
    "- Logarithm is a monotonic function and its maximum is the same as its argument.\n",
    "***\n",
    "\n",
    "In this case the log likelihood is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log L (\\mu) &= \\log \\prod_{i=1}^N f(x_i|\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\log f(x_i|\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&= - \\frac{1}{2} \\sum_{i=1}^N \\log 2\\pi\\sigma^2 - \\frac{1}{2} \\sum_{i=1}^N  \\frac{(x_i-\\mu)^2}{\\sigma^2}  \\nonumber  \\\\\n",
    "&=  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "We maximize by making the derivative of the log likelihood equal to zero\n",
    "\n",
    "$$\n",
    "\\frac{d  \\log L (\\mu)}{d\\mu} =  \\frac{1}{\\sigma^{2}}  \\sum_{i=1}^N (x_i-\\mu) =0\n",
    "$$\n",
    "\n",
    "Finally the MLE of $\\mu$ is \n",
    "$$\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i, \\quad \\sigma >0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Example: MLE for the variance of a Gaussian dist.\n",
    "\n",
    "The MLE estimator of the variance can be obtained using the same procedure:\n",
    "\n",
    "$$\n",
    "\\log L (\\mu, \\sigma^2) =  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d  \\log L (\\mu, \\sigma^2)}{d\\sigma^2} =  - \\frac{N}{2} \\frac{1}{\\sigma^2} + \\frac{1}{2\\sigma^{4}}\\sum_{i=1}^N (x_i-\\mu)^2 =0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i- \\hat\\mu)^2\n",
    "$$\n",
    "\n",
    "- If the true mean is not known then this is a biased estimator of the true variance\n",
    "- MLE can produce biased estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(8, 4)); ax2 = ax.twinx()\n",
    "np.random.seed(0)\n",
    "x = 80 + np.random.randn(10000)\n",
    "#x = 80 + 2*np.random.rand(1000)  # What happens if the data is not normal\n",
    "k_list = [int(x) for x in np.logspace(0, 4, num=50)]\n",
    "hat_mu = np.array([np.sum(x[:k])/k for k in k_list])\n",
    "hat_var = np.array([np.sum((x[:k]-hat_mu[i])**2)/(k) for i, k in enumerate(k_list)])\n",
    "ax.plot(k_list, hat_mu); ax2.plot(k_list, hat_var, linestyle='--'); ax.set_xscale('log')\n",
    "ax.set_xlabel('Number of samples'); \n",
    "ax.set_ylabel('$\\hat \\mu$ (solid line)'); ax2.set_ylabel('$\\hat \\sigma^2$ (dashed line)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Extra: Biased and unbiased estimator\n",
    "\n",
    "For a parameter $\\theta$ and an estimator $\\hat \\theta$, if\n",
    "$$\n",
    "\\mathbb{E}[\\hat \\theta] = \\theta,\n",
    "$$\n",
    "then $\\hat \\theta$ is an unbiased estimator of $\\theta$\n",
    "\n",
    "***\n",
    "\n",
    "### Example: MLE of the mean of a Gaussian dist.\n",
    "\n",
    "Is the MLE of the mean of $x\\sim N(\\mu, \\sigma^2)$ unbiased?\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat \\mu] &= \\mathbb{E} \\left[ \\frac{1}{N} \\sum_{i=1}^N x_i \\right]  \\nonumber \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E}[x_i] = \\frac{1}{N} \\sum_{i=1}^N \\mu = \\mu  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "YES\n",
    "\n",
    "***\n",
    "### Example: MLE of the variance of a Gaussian dist.\n",
    "\n",
    "First lets expand the expression of the MLE of the variance\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2 &= \\frac{1}{N} \\sum_{i=1}^N \\left(x_i- \\frac{1}{N}\\sum_{j=1}^N x_j \\right)^2 \\nonumber \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N x_i  x_j \\nonumber \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j\\neq i} x_i x_j - \\frac{1}{N^2} \\sum_{i=1}^N x_i^2 \\nonumber  \\\\\n",
    "&= \\frac{N-1}{N^2} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j \\neq i} x_i x_j  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "Then applying the expected value operator we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat \\sigma^2] &= \\frac{N-1}{N^2} \\sum_{i=1}^N \\mathbb{E} [x_i^2] - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j \\neq i} \\mathbb{E} [x_i] \\mathbb{E} [x_j] \\nonumber  \\\\\n",
    "&= \\frac{N-1}{N} (\\sigma^2 + \\mu^2) - \\frac{N-1}{N} \\mu^2 \\nonumber \\\\\n",
    "&= \\frac{N-1}{N} \\sigma^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "The MLE estimator is biased! \n",
    "\n",
    "\n",
    "**Unbiased estimator:** If we multiply it by a constant we obtain the well known unbiased estimator of the variance\n",
    "$$\n",
    "\\hat \\sigma_{u}^2 = \\frac{N}{N-1} \\hat \\sigma^2 = \\frac{1}{N-1} \\sum_{i=1}^N (x_i- \\hat\\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "### Extra: Connection with MSE cost and least squares\n",
    "\n",
    "The log likelihood when we assume a Gaussian distribution is\n",
    "$$\n",
    "\\log L (\\mu, \\sigma^2) =  - \\frac{N}{2} \\log 2\\pi \\sigma^2 - \\frac{1}{2\\sigma^2}   \\sum_{i=1}^N (x_i-\\mu)^2,\n",
    "$$\n",
    "if we assume that the variance is known and fixed then the MLE solution\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\mu &= \\text{arg}\\max_{\\mu} \\log L(\\mu, \\sigma^2) \\nonumber \\\\\n",
    "&= \\text{arg}\\max_{\\mu} \\text{Const} -   \\text{Const} \\sum_{i=1}^N (x_i-\\mu)^2 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "is equivalent to minimizing the argument of \n",
    "$$\n",
    "\\text{MSE} = \\sum_{i=1}^N (x_i-\\mu)^2 = \\| x - \\mu \\|^2,\n",
    "$$\n",
    "the well-known Mean Square Error criterion (MSE). In summary:\n",
    "\n",
    "\n",
    "MLE for a Gaussian dist. with known variance \n",
    "$\\equiv$\n",
    "**Least Squares:** Minimizing the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "### Exercise: MLE for a Bernoulli distribution\n",
    "\n",
    "A magician friend of yours has bought a \"magic coin\". He asks you to obtain for him the probability of obtaining a head with such coin.\n",
    "\n",
    "The coin has two outputs (head/tail) so we can assume that it follows a Bernoulli distribution\n",
    "\n",
    "$$\n",
    "f(x|p) = p^x (1-p)^{1-x}, ~~ x \\in \\{0, 1\\}\n",
    "$$\n",
    "\n",
    "Your friend tosses the coin N times and records the outputs $\\{x_i\\}$\n",
    "\n",
    "- **Objective 1:** Find and analytic expression for $\\hat p$\n",
    "- **Objective 2:** Use your expression and find $\\hat p$ for the following \"coin toss vector\"    \n",
    "        coin_toss = np.random.binomial(n=1, p=0.75, size=N)\n",
    "    How large needs $N$ to be so that $|\\hat p - p|/p < 0.05$ (try 10 different seeds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case the log likelihood is \n",
    "$$\n",
    "\\begin{align}\n",
    "\\log L(p) &= \\log \\prod_{i=1}^N p^{x_i} (1-p)^{1- x_i} \\nonumber \\\\\n",
    "&= \\sum_i x_i \\log (p)  + (1-x_i) \\log(1-p) \\nonumber \\\\\n",
    "&= \\log (p) \\sum_i x_i +  \\log(1-p) \\left(N - \\sum_i x_i\\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "And the MLE of the mean is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d\\log L(p)}{dp} &=  \\frac{ \\sum_i x_i}{p} -  \\frac{N - \\sum_i x_i}{1-p} =0 \\nonumber \\\\\n",
    "&\\implies \\hat p = \\frac{1}{N} \\sum_{i=1}^N x_i \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "***\n",
    "which is equivalent to the MLE of the mean of a Gaussian dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = np.random.binomial(n=1, p=0.75, size=100)\n",
    "p = np.linspace(1e-2, 1-1e-2, num=100)\n",
    "logL = np.log(p)*np.sum(x) + np.log(1-p)*(len(x)-np.sum(x))\n",
    "ax.plot(p, logL); ax.scatter(p[np.argmax(logL)], np.amax(logL), s=100, c='k', zorder=100)\n",
    "ax.plot([p[np.argmax(logL)], p[np.argmax(logL)]], [np.amin(logL), np.amax(logL)], linestyle='--')\n",
    "ax.set_xlabel('p'); ax.set_ylabel('log L(p)');\n",
    "display(\"Best p: %0.4f\" %(p[np.argmax(logL)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "### Extra: Connection with cross entropy cost\n",
    "\n",
    "The log likelihood assuming a Bernoulli distribution\n",
    "\n",
    "$$\n",
    "\\log L(p) = \\sum_i x_i \\log (p)  + (1-x_i) \\log(1-p) \n",
    "$$\n",
    "\n",
    "is the negative of the binary cross-entropy, *i.e.* maximizing the log likelihood is equivalent to minimize the cross-entropy cost.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Example: MLE of a Gaussian mixture\n",
    "\n",
    "Let's imagine that our *iid* data come from a mixture of Gaussians with K components\n",
    "\n",
    "$$\n",
    "f(x_i|\\pi,\\mu,\\sigma^2) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\sigma_k^2),\n",
    "$$\n",
    "where $\\sum_{k=1}^K \\pi_k = 1$ and $\\pi_k \\in [0, 1] ~~ \\forall k$\n",
    "\n",
    "We can write the log likelihood as\n",
    "\n",
    "$$\n",
    "\\log L(\\pi,\\mu,\\sigma^2) = \\sum_{i=1}^N \\log \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\sigma_k^2)\n",
    "$$\n",
    "\n",
    "- Oh my! We cannot obtain analytical expressions for the parameters as before\n",
    "- We have to resort to iterative methods/optimizers, Can you name any?\n",
    "- **Expectation Maximization** (we will see this in a future class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = np.random.randn(100, 2); y = 5 + 2*np.random.randn(100, 2)\n",
    "ax.scatter(x[:, 0], x[:, 1]); ax.scatter(y[:, 0], y[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Optimality properties and uncertainty of MLEs \n",
    "\n",
    "Assuming that the data truly comes from the specified model the MLE is\n",
    "- **Consistent:** The estimate converge to the true parameter as data points increase\n",
    "$$\n",
    "\\lim_{N\\to \\infty} \\hat \\theta = \\theta\n",
    "$$\n",
    "- **Asymptotically normal:** The distribution of the estimate approaches a normal centered at the true parameter. \n",
    "$$\n",
    "\\lim_{N\\to \\infty} p(\\hat \\theta) = \\mathcal{N}(\\hat \\theta | \\theta, \\sigma_\\theta^2)\n",
    "$$\n",
    "- **Minimum variance:** The estimate achieve the theoretical minimal variance given by the Cramer-Rao bound\n",
    "\n",
    "***\n",
    "\n",
    "### Cramer-Rao lower bound:\n",
    "Inverse of the expected Fisher information, *i.e* the second derivative of $- \\log L$ with respect to $\\theta$\n",
    "$$\n",
    "\\sigma_{nm}^2 =  \\left (- \\frac{d^2 \\log L (\\theta)}{d\\theta_n \\theta_m} \\bigg\\rvert_{\\theta = \\hat\\theta}\\right)^{-1}\n",
    "$$\n",
    "Note that\n",
    "- $\\sigma_{nm}^2$ is the minimum variance achieved by an unbiased estimator.\n",
    "- $\\sigma_{nn}^2$ give the marginal error bars \n",
    "- If $\\sigma_{nm} \\neq 0 ~~ n\\neq m$, then errors are correlated, *i.e* some combinations of parameters might be better determined than others\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Considering a Gaussian likelihood\n",
    "$$\n",
    "\\log L (\\mu, \\sigma^2) =  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 \n",
    "$$\n",
    "\n",
    "What is the uncertainty of the MLE \n",
    "$$\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "\n",
    "In this case the Cramer-rao bound\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma_{\\hat\\mu}^2  &= \\left (- \\frac{d^2 \\log L(\\mu, \\sigma^2)}{d\\mu^2} \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1}  \\nonumber \\\\\n",
    "&=  \\left (- \\frac{1}{\\sigma^2} \\frac{d}{d\\mu}  \\sum_{i=1}^N (x-\\mu) \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1}  \\nonumber \\\\\n",
    "&=  \\left ( \\frac{N}{\\sigma^2}  \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1} = \\frac{\\sigma^2}{N}  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "- This is known as the standard error of the mean\n",
    "- Also, $p(\\hat \\mu) \\to \\mathcal{N}(\\hat \\mu| \\mu, \\sigma^2/N)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "ax.set_title('Mean of Gaussian distributed data')\n",
    "N, mu_real, s_real = int(1e+4), 2.23142, 1.124123\n",
    "np.random.seed(0)\n",
    "x = mu_real + s_real*np.random.randn(N)\n",
    "mu_estimator = np.array([np.mean(x[:i]) for i in range(1, N)])\n",
    "\n",
    "ax.plot([1, N], [mu_real, mu_real], 'k--', label='Real')\n",
    "ax.plot(range(1,N), mu_estimator, label='MLE');\n",
    "ax.fill_between(np.arange(1, N), mu_estimator-s_real/np.sqrt(np.arange(1, N)), \n",
    "                mu_estimator+s_real/np.sqrt(np.arange(1, N)), alpha=0.5);\n",
    "ax.set_xscale('log'); ax.set_label('Number of samples'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Bootstrap\n",
    "\n",
    "The uncertainty of a point-estimate can be non-parametrically calculated using **bootstrap resampling**\n",
    "\n",
    "In bootstrap you generate new datasets that follow the properties of the original one \n",
    "\n",
    "<img src=\"img/bootstrap_diagram.png\">\n",
    "\n",
    "The conceptual steps are:\n",
    "\n",
    "1. Create a new set by randomly selecting $N$ observations with replacement\n",
    "1. Compute the value of your estimator on the new dataset\n",
    "1. Go back to one until have $T$ values\n",
    "1. Now you have an empirical distribution for the estimator. Use it to get a confidence interval\n",
    "\n",
    "**Note:** There are many types of bootstrap tests with different properties and assumptions (more on this in a future class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "def update(N, T):\n",
    "    ax.cla(); ax.set_xlabel('x')\n",
    "    np.random.seed(0)\n",
    "    x = np.random.randn(N) # zero mean, unit variance\n",
    "    mle_mu = np.mean(x)    \n",
    "    mle_mu_bs = [np.mean(np.random.choice(x, size=len(x), replace=True)) for k in range(T)]\n",
    "    hist_val, hist_lim, _ = ax.hist(mle_mu_bs, density=True, alpha=0.6)\n",
    "    t = np.linspace(hist_lim[0], hist_lim[-1], num=200)\n",
    "    ax.plot(t, gaussian_pdf(t, mu=mle_mu, s=1/np.sqrt(len(x))), 'k-', linewidth=4)  \n",
    "    ax.scatter(np.mean(x), 0, c='k', s=100, zorder=100)\n",
    "    display(\"Empirical confidence interval at 0.95 = [%0.4f, %0.4f]\" %(np.sort(mle_mu_bs)[int(0.05*T)], \n",
    "                                                                       np.sort(mle_mu_bs)[int(0.95*T)]))    \n",
    "interact(update, N=SelectionSlider_nice(options=[10, 100, 1000, 10000], value=100),\n",
    "         T=SelectionSlider_nice(options=[10, 100, 1000, 10000], value=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Goodness of fit\n",
    "\n",
    "- We can compare estimated parameters through the likelihood\n",
    "- The MLE estimators give the maximum value of the likelihood\n",
    "- But how good is it? \"Best\" might still be poor...\n",
    "\n",
    "In the Gaussian likelihood with fixed variance \n",
    "$$\n",
    "\\begin{align}\n",
    "\\log L (\\mu) &=  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2}   \\sum_{i=1}^N \\frac{(x_i-\\mu)^2}{\\sigma^2}  \\nonumber \\\\\n",
    "&=  \\text{Const} - \\frac{1}{2}   \\sum_{i=1}^N z_i^2,  \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where $z_i = (x_i-\\mu)/\\sigma$.\n",
    "\n",
    "Meet the $\\chi^2$ distribution with $k$ degrees of freedom (dof), the distribution of a **sum of k independent and standard normal distributed RVs**\n",
    "\n",
    "$\\chi^2$ cheat sheet:\n",
    "- It has support on $\\mathbb{R}^+$\n",
    "- Degrees of freedom $k \\in \\mathbb{N}$\n",
    "- $f(x) = \\frac{1}{2^{k/2} \\Gamma(k/2)}  x^{k/2-1} e^{-x/2}$\n",
    "- Mean: $k$\n",
    "- Variance: $2k$\n",
    "\n",
    "Note that the distribution does not depend on $\\mu$ and $\\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "line = ax.plot([], [], linewidth=4)\n",
    "ax.set_title('Mean: dof, Variance: 2dof')\n",
    "def update(dof):\n",
    "    x = np.linspace(chi2.ppf(0.01, dof), chi2.ppf(0.99, dof), 100)\n",
    "    px = chi2.pdf(x, dof)\n",
    "    line[0].set_xdata(x); ax.set_xlim([np.amin(x)*0.9, np.amax(x)*1.1])\n",
    "    line[0].set_ydata(px); ax.set_ylim([np.amin(px)*0.9, np.amax(px)*1.1])    \n",
    "    \n",
    "interact(update, dof=SelectionSlider_nice(options=[1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this  case\n",
    "\n",
    "$$\n",
    "\\log L (\\mu) \\propto  - \\frac{1}{2} \\chi_N^2\n",
    "$$\n",
    "\n",
    "So if:\n",
    "\n",
    "$$\n",
    "\\chi_N^2  \\ll N  + \\sqrt{2N}\n",
    "$$\n",
    "\n",
    "Then it is very likely that the data was generated by a model $\\mu=\\hat \\mu$  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Model comparison\n",
    "\n",
    "- What if the data was not Gaussian distributed in the first place? \n",
    "- How to compare models with different number of parameters? \n",
    "- In general the more number of parameters the better the fit (overfitting)\n",
    "- How to score models taking into account their complexity?\n",
    "\n",
    "Two options:\n",
    "1. Cross-validation and bias/variance trade-off (based on finite data)\n",
    "1. Akaike information criterion (AIC) (based on asymptotic approximation)\n",
    "\n",
    "For a model with $k$ parameters and N data points the AIC is \n",
    "\n",
    "$$\n",
    "\\text{AIC} = -2 \\log L(\\hat \\theta) + 2k + \\frac{2k(k+1)}{N-k-1},\n",
    "$$\n",
    "\n",
    "which you seek to minimize\n",
    "\n",
    "***\n",
    "\n",
    "**Parsimony principle** (aka Occam's Razor): Choose the simplest scientific explanation that fits the evidence. \n",
    "\n",
    "Also related: KISS principle\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "<a id=\"nonparametric\"></a>\n",
    "\n",
    "# Nonparametric statistical modeling\n",
    "\n",
    "***\n",
    "\n",
    "**Recap:** Statistical models that do not assume an underlying distribution\n",
    "\n",
    "Most famous example: **The histogram**\n",
    "\n",
    "- The histogram is a numerical representation of a distribution \n",
    "- The histogram allow us to visualize our data and explore its statistical features\n",
    "- The histogram is built by dividing the data range in **bins** and counting the observations that fall on a given bin\n",
    "- The parameters of the histogram are the size and location of the bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "The importance of setting the number of bins right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = np.linspace(-11, 10, num=1000)\n",
    "px = 0.7*gaussian_pdf(x, mu=-4, s=2) + 0.3*gaussian_pdf(x, mu=3, s=2)\n",
    "N = 1000; np.random.seed(0)\n",
    "hatx = np.concatenate((-4 + 2*np.random.randn(int(0.7*N)), \n",
    "                       (3 + 2*np.random.randn(int(0.3*N)))))\n",
    "\n",
    "def update(nbins): \n",
    "    ax.cla()\n",
    "    ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "    hist, bin_edges = np.histogram(hatx, bins=nbins, density=True)\n",
    "    ax.bar(bin_edges[:-1], hist, width=bin_edges[1:] - bin_edges[:-1], \n",
    "           edgecolor='k', align='edge', alpha=0.8)\n",
    "    \n",
    "interact(update, nbins=SelectionSlider_nice(options=[1, 2, 5, 10, 20, 50, 100], value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A small number of bins omits the features of the distribution\n",
    "- A large number of bins introduce noise\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "## Histogram in practice\n",
    "\n",
    "How to select the width/number of bins?\n",
    "\n",
    "- Cross validation\n",
    "    - AMISE: (Asymptotic) Mean integrated square error\n",
    "- Rules of thumb, *e.g.* Scott's rule and Silverman's rule\n",
    "    - Proportional to the scale of data\n",
    "    - Inversely proportional to the number of samples \n",
    "    - Obtained through assumptions\n",
    "\n",
    "***\n",
    "\n",
    "### Silverman's rule\n",
    "\n",
    "The width of the bins is \n",
    "\n",
    "$$\n",
    "h = 0.9 \\frac{\\min[\\sigma, 0.7412 (q_{75} - q_{25})]}{N^{1/5}},\n",
    "$$\n",
    "\n",
    "where $N$ is the number of observations, $\\sigma$ is the standard deviation and $q_{75}-q_{25}$ is the interquartile range. \n",
    "\n",
    "\n",
    "**Silverman's assumption**: The unknown density is Gaussian\n",
    "\n",
    "\n",
    "Assuming uniformly spaced bins then the number of bins is\n",
    "\n",
    "$$\n",
    "N_{bins} = \\frac{\\max(x)-\\min(x)}{h}\n",
    "$$\n",
    "\n",
    "***\n",
    "### Other considerations\n",
    "\n",
    "- Bins could have different boundaries (offsets)\n",
    "- Bins could have different widths\n",
    "- Multiresolution approach (wavelet style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Kernel density estimation (KDE)\n",
    "\n",
    "- Other option for non-parametric density estimation is KDE\n",
    "- In KDE each point has its \"own bin\", and bins can overlap\n",
    "- KDE does not require choosing bin boundaries, only bin width\n",
    "\n",
    "The unidimensional KDE for a set $\\{x_i\\}_{i=1,\\ldots, N}$ is\n",
    "\n",
    "$$\n",
    "\\hat f_h(x) = \\frac{1}{Nh} \\sum_{i=1}^N \\kappa \\left ( \\frac{x - x_i}{h} \\right)\n",
    "$$\n",
    "\n",
    "where $h$ is called the **kernel bandwidth** or kernel size and $\\kappa(u)$ is the **kernel function** that need to be positive, zero mean and integrate to unity.\n",
    "\n",
    "For example, one broadly used kernel is \n",
    "\n",
    "$$\n",
    "\\kappa(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left ( - \\frac{u^2}{2} \\right),\n",
    "$$\n",
    "\n",
    "the Gaussian kernel. \n",
    "\n",
    "**Other widely used kernels:** Exponential, Top-hat, Epanechnikov\n",
    "\n",
    "***\n",
    "\n",
    "<center><b>KDE in a nutshell</b>: Place a kernel on top of each point and get the average</center>\n",
    "\n",
    "***\n",
    "\n",
    "**Avoid confusion:** \n",
    "- Assuming that the data is **Gaussian distributed** and doing KDE with the **Gaussian kernel** are very **different things**! \n",
    "- Using the Gaussian kernel for non-Gaussian data is perfectly fine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "line_kde = ax.plot(x, np.zeros_like(x))\n",
    "hs = 0.9*np.std(hatx)*N**(-1/5)\n",
    "def update(k, kernel): \n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=hs*k).fit(hatx.reshape(-1, 1))\n",
    "    line_kde[0].set_ydata(np.exp(kde.score_samples(x.reshape(-1, 1))))\n",
    "    \n",
    "interact(update, k=SelectionSlider_nice(description=\"$k =h/h_s$\", options=[1/8, 1/4, 1/2, 1, 2, 4], value=1),\n",
    "        kernel=SelectionSlider_nice(options=[\"gaussian\", \"exponential\", \"epanechnikov\", \"tophat\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Other non-parametric methods\n",
    "\n",
    "- Splines, kernel regression\n",
    "- Support Vector Machine and Gaussian Processes\n",
    "- Nearest neighbors\n",
    "- Neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"bayesian\"></a>\n",
    "\n",
    "# Bayesian approach to parametric modeling\n",
    "\n",
    "***\n",
    "\n",
    "**Recap:** The Bayesian premise\n",
    "- Inference is made by producing probability density functions (pdf): **posterior**\n",
    "- Model the uncertainty of the data, experiment, parameters, etc. as a **joint pdf**\n",
    "- $\\theta$ is a R.V., *i.e.* it follows a distribution: **prior**\n",
    "\n",
    "The Bayes theorem and the law of total probability tell us\n",
    "\n",
    "$$\n",
    "p(\\theta| \\{x\\}) = \\frac{p(\\{x\\}, \\theta)}{p(\\{x\\})}= \\frac{p(\\{x\\}|\\theta) p(\\theta)}{\\int p(\\{x\\}|\\theta) p(\\theta) d\\theta} \\propto p(\\{x\\}|\\theta) p(\\theta),\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- In Bayesian model fitting we seek the **posterior** (parameters given the data) \n",
    "- The posterior is build from the **likelihood**, **prior** and **evidence** (marginal data likelihood)\n",
    "- The posterior can be small if either the likelihood or the prior are small\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "### Why/When should I use the Bayesian formalism?\n",
    "\n",
    "- In many cases the Bayesian inference will not differ much from MLE\n",
    "- In general the Bayesian inference is harder to compute and requires more sophisticated methods\n",
    "\n",
    "Then? \n",
    "- We can integrate unknown/missing/uninteresting (nuisance) parameters\n",
    "- Principled way of injecting prior knowledge (regularization)\n",
    "- Built-in error bars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### The Bayesian inference procedure\n",
    "\n",
    "1. Formulate data likelihood\n",
    "1. Choose a prior\n",
    "1. Build a joint distribution (relation of all parameters)\n",
    "1. Determine the posterior using Bayes Theorem\n",
    "1. Find MAP and credible regions\n",
    "1. Do hypothesis test\n",
    "1. **Criticize:** Evaluate how appropriate the model is and suggest improvements\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Priors\n",
    "\n",
    "Priors summarize what we know about the parameters before-hand, for example\n",
    "- a parameter is bounded/unbounded (Normal/Cauchy)\n",
    "- a parameter is positive (Half-normal, Half-Cauchy, Lognormal, Inverse Gamma)\n",
    "- a parameter is positive-semidefinite (Inverse Wishart, LKJ)\n",
    "- a parameter follows a simplex (Dirichlet)\n",
    "\n",
    "Priors can be \n",
    "- Informative, *e.g.* my parameter is $\\mathcal{N}(\\theta|\\mu=5.4, \\sigma^2=0.1)$\n",
    "- Weakly informative, *e.g.* my parameter is $\\mathcal{N}(\\theta|\\mu=0, \\sigma^2=100.)$\n",
    "- Uninformative (objective), *e.g.* my parameter is positive\n",
    "\n",
    "Priors should \n",
    "- add positive probabilistic weights on possible values\n",
    "- no weight to impossible values\n",
    "- help regularize the solution\n",
    "\n",
    "Other guidelines to select priors:\n",
    "- **Conjugate priors:** Given a likelihood the posterior has the same distribution as the prior\n",
    "- Maximum entropy principle\n",
    "\n",
    "Stan prior choice recommendations:\n",
    "\n",
    "https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"MAP\"></a>\n",
    "\n",
    "# Maximum *a posteriori* (MAP) estimation\n",
    "\n",
    "***\n",
    "\n",
    "In the Bayesian setting the best \"point estimate\" of the parameters of the model is given by the MAP\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\text{arg} \\max_\\theta p(\\theta|\\{x\\}) =  \\text{arg} \\max_\\theta p(\\{x\\}| \\theta) p(\\theta),\n",
    "$$\n",
    "\n",
    "where we \"omit\" the evidence because it does not depend on $\\theta$\n",
    "\n",
    "Applying the logarithm (monotonic) we can decouple the likelihood from the prior\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\text{arg} \\max_\\theta \\log p(\\{x\\}| \\theta) + \\log p(\\theta),\n",
    "$$\n",
    "\n",
    "\n",
    "- MAP estimation is also referred as penalized MLE\n",
    "- We saw that the likelihood can be interpreted as the error between model and data (*e.g* MSE, cross-entropy)\n",
    "- The prior can be interpreted as a regularizer on the parameters (more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Example: MAP of the mean of a Gaussian dist.\n",
    "\n",
    "We want to find the MAP for the weight of your professor. \n",
    "\n",
    "Assuming that the likelihood is Gaussian with known variance we have\n",
    "\n",
    "$$\n",
    "\\log p(\\{x\\}|\\theta) = \\log L (\\mu)  = - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2, \n",
    "$$\n",
    "\n",
    "and further assuming that the true weight has a Gaussian prior $\\mathcal{N}(\\mu|\\mu_0, \\sigma^2_0)$\n",
    "\n",
    "$$\n",
    "\\log p(\\theta) = -\\frac{1}{2} \\log 2 \\pi \\sigma^2_0 - \\frac{1}{2 \\sigma^2_0}  (\\mu - \\mu_0)^2,\n",
    "$$\n",
    "\n",
    "then we set the derivative to zero\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\mu} \\log p(\\{x\\}|\\theta) + \\log p(\\theta) =   \\frac{1}{\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)  - \\frac{1}{ \\sigma^2_0}  (\\mu - \\mu_0) = 0,\n",
    "$$\n",
    "and we get the MAP estimate\n",
    "\n",
    "$$\n",
    "\\hat \\mu =  \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0} \\right)^{-1} \\left(\\frac{N\\bar x}{\\sigma^2}  + \\frac{\\mu_0}{\\sigma^2_0} \\right),\n",
    "$$\n",
    "where $\\bar x = \\frac{1}{N} \\sum_{i=1}^N x_i$.\n",
    "\n",
    "**IMPORTANT:** Do not confuse $\\sigma^2$ (the noise variance) and $\\sigma^2_0$ (prior variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Particular cases \n",
    "\n",
    "- The MAP estimator for a  standard normal prior $\\mathcal{N}(\\mu| 0, 1)$ is\n",
    "$$\n",
    "\\hat \\mu =  \\left(\\frac{N}{\\sigma^2} + 1 \\right)^{-1} \\left(\\frac{N\\bar x}{\\sigma^2} \\right) = \\frac{1}{1 + \\sigma^2/N} \\bar x,\n",
    "$$\n",
    "note that when \n",
    "$$\n",
    "\\lim_{N \\to \\infty} \\hat \\mu = \\bar x,\n",
    "$$\n",
    "which is the MLE solution. Can you explain why this happens?\n",
    "\n",
    "\n",
    "- Similarly, the MAP estimator for a normal prior $\\mathcal{N}(\\mu| 0, \\sigma^2_0)$ with $\\sigma^2_0 \\to \\infty$\n",
    "$$\n",
    "\\hat \\mu =  \\left(\\frac{N}{\\sigma^2} \\right)^{-1} \\left(\\frac{N\\bar x}{\\sigma^2} \\right) =  \\bar x,\n",
    "$$\n",
    "which is again the MLE solution. The prior is uninformative of $\\mu$ so it has no influence on the result\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### General case\n",
    "\n",
    "Note that\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\mu &=  \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0} \\right)^{-1} \\left(\\frac{N\\bar x}{\\sigma^2}  + \\frac{\\mu_0}{\\sigma^2_0} \\right)  \\nonumber \\\\\n",
    "&=  \\frac{N \\bar x \\sigma^2_0 + \\mu_0 \\sigma^2}{N\\sigma^2_0+ \\sigma^2} = \\frac{\\bar x + \\mu_0 \\frac{\\sigma^2}{\\sigma^2_0 N}}{1 + \\frac{\\sigma^2}{\\sigma^2_0 N}}  \\nonumber \\\\\n",
    "&= w \\bar x + (1-w) \\mu_0, \\qquad w = \\frac{1}{1 + \\frac{\\sigma^2}{\\sigma^2_0 N}}  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "The MAP estimate of the mean is an average between the prior mean $\\mu_0$ and the MLE solution.\n",
    "\n",
    "If either $\\sigma^2_0$ or $N$ are large wrt $\\sigma^2$, then $w=1$ and the MLE is recovered\n",
    "\n",
    "\n",
    "**Reflect on the following:** The prior has more influence when\n",
    "- You have few samples\n",
    "- Your samples are noisy\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*** \n",
    "### Extra: MAP intepretation as a penalized MLE/regularized LS\n",
    "\n",
    "The MAP estimate of the mean of a Gaussian dist with known variance using a zero-mean normal prior is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\mu &= \\text{arg} \\max_\\mu  \\log p(\\{x\\}| \\mu, \\sigma^2) + \\log p(\\mu) \\nonumber \\\\\n",
    "&= \\text{arg} \\max_\\mu   - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 -  \\frac{1}{2\\sigma_0^2} \\mu^2 \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\mu \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 +  \\frac{1}{2\\sigma_0^2} \\mu^2 \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\mu \\|x-\\mu\\|^2  + \\lambda \\|\\mu \\|^2, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\lambda = \\frac{\\sigma^2}{\\sigma_0^2}$. \n",
    "\n",
    "We recognize the last equation as a regularized least squares problem\n",
    "- A Gaussian prior yields a L2 regularizer (ridge regression)\n",
    "- A Laplacian prior yields a L1 regularizer (LASSO)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Full posterior with conjugate prior \n",
    "\n",
    "The MAP is only a point estimate. For the mean of a Gaussian dist we can get the full posterior analytically\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta |\\{x\\}) &\\propto p(\\{x\\} |\\theta ) p(\\theta ) \\nonumber \\\\\n",
    "&\\propto \\exp \\left ( \\frac{1}{2\\sigma^2} \\sum_i (x_i - \\mu)^2 \\right) \\exp \\left ( \\frac{1}{2\\sigma_0^2} (\\mu - \\mu_0)^2 \\right) \\nonumber \\\\\n",
    "&\\propto \\exp \\left ( -\\frac{1}{2 \\hat \\sigma^2} (\\mu - \\hat \\mu)^2 \\right),  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat \\sigma^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0} \\right)^{-1} \n",
    "$$\n",
    "\n",
    "***\n",
    "**This shows that the Gaussian is conjugate to itself**\n",
    "\n",
    "***\n",
    "Other way to show this is to use the [property of Gaussian pdf multiplication](http://www.tina-vision.net/docs/memos/2003-003.pdf)\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu_1, \\sigma_1^2) \\mathcal{N}(x|\\mu_2, \\sigma_2^2) = C \\mathcal{N}\\left(x\\bigg\\rvert \\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\\left( \\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2}\\right), \\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\\right)\n",
    "$$\n",
    "\n",
    "where $C$ is a scaling constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(10, 4))\n",
    "x = np.linspace(-10, 10, num=10000)\n",
    "def update(mu, sigma2, alpha, beta, N):\n",
    "    np.random.seed(0); xi = mu + np.sqrt(sigma2)*np.random.randn(N)\n",
    "    ax.cla(); ax.set_xlim([-5, 5]);\n",
    "    likelihood = gaussian_pdf(x, np.mean(xi), np.sqrt(sigma2/N))\n",
    "    ax.scatter(mu, 0, c='k', s=100, zorder=100)\n",
    "    ax.plot(x, likelihood, label='MLE'); ax.fill_between(x, 0, likelihood, alpha=0.5)\n",
    "    prior = gaussian_pdf(x, alpha, np.sqrt(beta)) \n",
    "    ax.plot(x, prior, label='prior'); ax.fill_between(x, 0, prior, alpha=0.5)\n",
    "    s2_pos = (N/sigma2 + 1./beta)**-1\n",
    "    mu_pos = (np.sum(xi)/sigma2 + alpha/beta)*s2_pos;\n",
    "    posterior = gaussian_pdf(x, mu_pos, np.sqrt(s2_pos));\n",
    "    ax.plot(x, posterior, label='posterior'); ax.fill_between(x, 0, posterior, alpha=0.5)\n",
    "    plt.legend()    \n",
    "\n",
    "interact(update, \n",
    "         mu=FloatSlider_nice(description=r\"$\\mu$\", min=-3, max=3, value=2.), \n",
    "         sigma2=FloatSlider_nice(description=r\"$\\sigma^2$\", min=0.1, max=10., value=1.),\n",
    "         alpha=FloatSlider_nice(description=r\"$\\alpha$\", min=-3, max=3, value=0.), \n",
    "         beta=FloatSlider_nice(description=r\"$\\beta$\", min=0.1, max=10., value=1.),\n",
    "         N=SelectionSlider_nice(options=[1, 2, 5, 10, 20, 50, 100], value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Conjugate priors when $\\sigma^2$ is unknown\n",
    "\n",
    "Up to now we assumed that the variance  $\\sigma^2$ was known \n",
    "- Assuming that the mean $\\mu$ is known the conjugate prior for the variance is an inverse-Gamma distribution\n",
    "$$\n",
    "\\sigma^2 \\sim \\text{IG}(\\alpha, \\beta), \\qquad p (x|\\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{-\\alpha-1} e^{-\\frac{\\beta}{x}}\n",
    "$$\n",
    "- If both the mean and variance are unknown \n",
    "    - Multiplying the normal prior and the IG prior does not yield a conjugate prior (assumes independence of $\\mu$ and $\\sigma$)\n",
    "    - In this case the conjugate prior is hierarchical\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    p(x_i|\\mu, \\sigma^2) &= \\mathcal{N}(\\mu, \\sigma^2)  \\nonumber \\\\\n",
    "    p(\\mu|\\sigma^2) &= \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0)  \\nonumber \\\\\n",
    "    p(\\sigma^2) &= \\text{IG}(\\alpha, \\beta)  \\nonumber\n",
    "    \\end{align}\n",
    "    $$\n",
    "    - And it is called normal-inverse-gamma (NIG), a four parameter distribution \n",
    "    \n",
    "    \n",
    "***\n",
    "An extremely good document by Kevin Murphy on conjugate priors for the Gaussian dist: https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Extra: Mean of the posterior\n",
    "\n",
    "Other point estimate that can be used to characterize the posterior is\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\mathbb{E}[\\theta|\\{x\\}] = \\int \\theta p(\\theta| \\{x\\}) d\\theta,\n",
    "$$\n",
    "\n",
    "*i.e.* the mean or expected value of the posterior.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Future topics\n",
    "\n",
    "- Empirical Bayes: Model in which the hyperparameters are estimated from data instead of fixed before-hand\n",
    "- Hierarchical Bayes: Priors have hyperpriors with hyperhyperparameters\n",
    "- Markov Chain Monte Carlo (MCMC): Algorithm to sample from a distribution. We will use it to learn complex Bayesian models\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Sneak peak: Probabilistic programming with [PyMC3](https://docs.pymc.io/)\n",
    "\n",
    "***\n",
    "\n",
    "PyMC3 is a library to learn bayesian models using MCMC and VI (variational inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "np.random.seed(0)\n",
    "N, mu_t, sigma_t = 100, 10, 2\n",
    "x = mu_t + sigma_t*np.random.randn(N)\n",
    "print(\"MLE: %f %f\" %(np.mean(x), np.std(x)/np.sqrt(N)))\n",
    "with pm.Model() as demo:\n",
    "    #mu_0 = pm.Normal('mu0', mu=0, sd=10, shape=1)\n",
    "    sigma = pm.HalfNormal('s', sd=100, shape=1)\n",
    "    mu = pm.Normal('mu', mu=0, sd=100, shape=1)\n",
    "    x_observed = pm.Normal('x_obs', mu=mu, sd=sigma, observed=x)\n",
    "    trace = pm.sample(draws=10000, tune=2000, init='advi', n_init=20000, cores=4, chains=2, \n",
    "                      live_plot=True)\n",
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, figsize=(8, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "help(pm.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*** \n",
    "<a id=\"appendix\"></a>\n",
    "\n",
    "## Appendix: Gaussian distribution\n",
    "***\n",
    "\n",
    "- The Gaussian dist. has domain in $\\mathbb{R}$, it has two parameters $\\mu$ and $\\sigma^2$ and is rather easy to interpret\n",
    "\n",
    "- The Gaussian/Normal probability density function (PDF) is defined as \n",
    "$$\n",
    "f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left ( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n",
    "$$\n",
    "where $\\mu$ is the mean and $\\sigma^2$ is the variance\n",
    "\n",
    "- The Gaussian dist. is symmetric around $\\mu$ and it has only one mode (unimodal) centered at $\\mu$\n",
    "\n",
    "- The cumulative density function (CDF) of a Gaussian is\n",
    "$$\n",
    "F(x) = \\int_{-\\infty}^x  f(z) dz = \\frac{1}{2} \\left ( 1 + \\text{erf} \\left(\\frac{x-\\mu}{\\sigma \\sqrt{2}} \\right) \\right)\n",
    "$$\n",
    "where the error function (erf) is \n",
    "$$\n",
    "\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^x \\exp(-t^2) dt\n",
    "$$\n",
    "\n",
    "- Using $\\int \\exp(-a(x+b)^2) dx = \\sqrt{\\frac{\\pi}{a}}$ we can easily see that\n",
    "$$\n",
    "\\int f(x|\\mu, \\sigma^2) dx = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int \\exp \\left ( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx = 1\n",
    "$$\n",
    "\n",
    "- The first order moment of a Gaussian R.V. is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[X] &= \\int x f(x|\\mu, \\sigma^2) dx  \\nonumber \\\\ \n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int x \\exp \\left ( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx  \\nonumber \\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int  (z+\\mu) \\exp \\left ( -\\frac{z^2}{2\\sigma^2}\\right) dz  \\nonumber \\\\\n",
    "&= \\mu  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "and its second order moment is \n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\int x^2 f(x|\\mu, \\sigma^2) dx = \\mu^2 + \\sigma^2\n",
    "$$\n",
    "\n",
    "- The variance of a Gaussian can be defined from its first and second moment as\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\text{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathbb{E}[(X - \\mathbb{E}[X])^2] \n",
    "$$\n",
    "\n",
    "- In the Gaussian dist. the mode (maximum of the pdf) coincides with the $\\mu$\n",
    "\n",
    "- Tends to $\\delta(x-\\mu)$ for $\\sigma^2 \\rightarrow 0$\n",
    "\n",
    "- **Central Limit Theorem:** The sum of independent RVs is approximately Gaussian distributed\n",
    "\n",
    "- Maximum entropy distribution when the mean and variance are specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "dt=1e-4; x = np.arange(-5, 5, step=dt)\n",
    "\n",
    "def update(xi, xf):\n",
    "    for axis in ax:\n",
    "        axis.cla(); \n",
    "        axis.set_xlim([-5, 5]);\n",
    "    ax[0].plot(x, gaussian_pdf(x)); ax[1].plot(x, gaussian_cdf(x));\n",
    "    xrange = np.arange(xi, xf, step=dt)\n",
    "    ax[0].fill_between(xrange, 0, gaussian_pdf(xrange), alpha=0.5)\n",
    "    ax[1].scatter([xi, xf], [gaussian_cdf(xi), gaussian_cdf(xf)], s=100, c='k', zorder=100)\n",
    "    ax[1].text(xi+0.5, gaussian_cdf(xi), \"Init\"); ax[1].text(xf+0.5, gaussian_cdf(xf), \"End\")\n",
    "    ax[0].set_title(\"$\\int_{x_i}^{x_f} f(x) dx$ = %0.4f\" %(np.sum(gaussian_pdf(xrange))*dt))\n",
    "    area = gaussian_cdf(xf) - gaussian_cdf(xi)\n",
    "    ax[1].set_title(\"$F(x_f) - F(x_i)$ = %0.4f\" %(area if area >= 0 else 0))\n",
    "\n",
    "interact(update, \n",
    "         xi=FloatSlider_nice(description=\"$x_i$\", min=-5, max=5), \n",
    "         xf=FloatSlider_nice(description=\"$x_f$\", min=-5, max=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appendix: Multivariate Gaussian distribution\n",
    "\n",
    "\n",
    "- It has domain in $\\mathbb{R}^D$ and has parameters $\\mu \\in \\mathbb{R}^D$ and $\\Sigma \\in \\mathbb{R}^{D\\times D}$. The covariance $\\Sigma$ is defined as \n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix} \n",
    "\\Sigma_{11} & \\Sigma_{12} & \\ldots \\Sigma_{1D} \\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}^2 & \\ldots \\Sigma_{2D} \\\\\n",
    "\\vdots & \\vdots & \\ddots \\vdots \\\\\n",
    "\\Sigma_{D1} & \\Sigma_{D2} & \\ldots \\Sigma_{DD} \\\\\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "it is a square symmetric matrix and \n",
    "$$\n",
    "\\Sigma_{ij} = \\mathbb{E} \\left [ (X_i-\\mathbb{E}[X_i]) (X_j-\\mathbb{E}[X_j])\\right] \n",
    "$$\n",
    "\n",
    "- Its probability density function is\n",
    "\n",
    "$$\n",
    "f(x| \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^{D} |\\Sigma|}} \\exp \\left( - \\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n",
    "$$\n",
    "\n",
    "- Diagonal covariance. If $\\Sigma = I \\sigma^2$ with $\\sigma^2 \\in \\mathbb{R}^D$\n",
    "\n",
    "$$\n",
    "f(x| \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^{D} \\prod_{d=1}^D \\sigma_d^2}} \\exp \\left( - \\frac{1}{2} \\sum_{d=1}^D \\frac{(x_d-\\mu_d)^2}{\\sigma_d^2} \\right)\n",
    "$$\n",
    "\n",
    "- Isotropic or spherical covariance. If $\\Sigma = I \\sigma^2$ with $\\sigma^2 \\in \\mathbb{R}$\n",
    "\n",
    "$$\n",
    "f(x| \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^{D} \\sigma^{2D}}} \\exp \\left( - \\frac{1}{2\\sigma^2} \\sum_{d=1}^D (x_d-\\mu_d)^2 \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appendix: Exponential family\n",
    "\n",
    "Distributions with a pdf having the following form\n",
    "\n",
    "$$\n",
    "p(x|\\theta) = h(x) \\exp \\left( \\eta(\\theta) \\cdot T(x) - A(\\theta) \\right),\n",
    "$$\n",
    "\n",
    "where $\\eta(\\theta)$ and $T(x)$ are vectors with length $|\\theta|$. $\\eta$ is called the natural parameter vector.\n",
    "\n",
    "In the Gaussian case we can recognize\n",
    "\n",
    "$$\n",
    "T(x) = \\left[ x , x^2 \\right] , \\qquad \\eta(\\mu, \\sigma^2) = \\left[ \\frac{\\mu}{\\sigma^2} , -\\frac{1}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "h(x) = \\frac{1}{\\sqrt{2\\pi}}, \\qquad A(\\mu, \\sigma^2) =  \\frac{\\mu^2}{2\\sigma^2} + \\log(\\sigma)\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x|\\theta) &= \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( \\frac{x\\mu}{\\sigma^2}  -\\frac{x^2}{2\\sigma^2}  -\\frac{\\mu^2}{2\\sigma^2} - \\log(\\sigma) \\right)  \\nonumber \\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left( -\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right)  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "An important property is that a pdf in the exponential family has a conjugate prior\n",
    "\n",
    "$$\n",
    "p(\\theta|\\alpha, \\beta)  \\propto \\exp \\left ( \\alpha \\eta(\\theta) - \\beta A(\\theta) \\right),\n",
    "$$\n",
    "where $\\alpha \\in \\mathbb{R}^{|\\theta|}$ and $\\beta>0$ are the parameters of the prior."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
