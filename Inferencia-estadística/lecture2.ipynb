{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and matplotlib configuration\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "from ipywidgets import interact, Button, Output, Box\n",
    "from IPython.display import display\n",
    "from style import *\n",
    "\n",
    "# Others\n",
    "from scipy.special import erf\n",
    "gaussian_pdf = lambda x, mu=0, s=1: np.exp(-0.5*(x-mu)**2/s**2)/(s*np.sqrt(2*np.pi))\n",
    "gaussian_cdf = lambda x, mu=0, s=1: 0.5 + 0.5*erf((x-mu)/(s*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universidad Austral de Chile\n",
    "\n",
    "# INFO337 - Herramientas estadísticas para la investigación\n",
    "\n",
    "# Statistical inference \n",
    "\n",
    "### A course of the masters in informatics program\n",
    "\n",
    "### https://github.com/magister-informatica-uach/INFO337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"index\"></a>\n",
    "\n",
    "# Contents\n",
    "***\n",
    "\n",
    "1. Bayesian approach on parametric modeling\n",
    "1. [Maximum a posteriori](#MAP)\n",
    "1. [Analytical posterior](#Bayes)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. Hastie, Tibshirani and Friedman, \"The elements of statistical learning\" 2nd Ed., *Springer*, **Chapter 8**\n",
    "1. Murphy, \"Machine Learning: A Probabilistic Perspective\", *MIT Press*, 2012, **Chapter 5**\n",
    "1. Ivezic, Connolly, VanderPlas and Gray, \"Statistic, Data Mining, and Machine Learning in Astronomy\", *Princeton University Press*, 2014, **Chapters 4 and 5**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"bayesian\"></a>\n",
    "\n",
    "# Bayesian approach to parametric modeling\n",
    "\n",
    "***\n",
    "\n",
    "**Recap:** The Bayesian premise\n",
    "- Inference is made by producing probability density functions (pdf): **posterior**\n",
    "- Model the uncertainty of the data, experiment, parameters, etc. as a **joint pdf**\n",
    "- $\\theta$ is a R.V., *i.e.* it follows a distribution: **prior**\n",
    "\n",
    "The Bayes theorem and the law of total probability tell us\n",
    "\n",
    "$$\n",
    "p(\\theta| \\{x\\}) = \\frac{p(\\{x\\}, \\theta)}{p(\\{x\\})}= \\frac{p(\\{x\\}|\\theta) p(\\theta)}{\\int p(\\{x\\}|\\theta) p(\\theta) d\\theta} \\propto p(\\{x\\}|\\theta) p(\\theta),\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- In Bayesian model fitting we seek the **posterior** (parameters given the data) \n",
    "- The posterior is build from the **likelihood**, **prior** and **evidence** (marginal data likelihood)\n",
    "- The posterior can be small if either the likelihood or the prior are small\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "### Why/When should I use the Bayesian formalism?\n",
    "\n",
    "- In many cases the Bayesian inference will not differ much from MLE\n",
    "- In general the Bayesian inference is harder to compute and requires more sophisticated methods\n",
    "\n",
    "Then? \n",
    "- We can integrate unknown/missing/uninteresting (nuisance) parameters\n",
    "- Principled way of injecting prior knowledge (regularization)\n",
    "- Built-in uncertainty measure on parameters and predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### The Bayesian inference procedure\n",
    "\n",
    "1. Formulate data likelihood\n",
    "1. Choose a prior\n",
    "1. Build a joint distribution (relation of all parameters)\n",
    "1. Determine the posterior using Bayes Theorem\n",
    "1. Find MAP and credible regions\n",
    "1. Do hypothesis test\n",
    "1. **Criticize:** Evaluate how appropriate the model is and suggest improvements\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Priors\n",
    "\n",
    "Priors summarize what we know about the parameters before-hand, for example\n",
    "- a parameter is bounded/unbounded (Normal/Cauchy)\n",
    "- a parameter is positive (Half-normal, Half-Cauchy, Lognormal, Inverse Gamma)\n",
    "- a parameter is positive-semidefinite (Inverse Wishart, LKJ)\n",
    "- a parameter follows a simplex (Dirichlet)\n",
    "\n",
    "Priors can be \n",
    "- Informative, *e.g.* my parameter is $\\mathcal{N}(\\theta|\\mu=5.4, \\sigma^2=0.1)$\n",
    "- Weakly informative, *e.g.* my parameter is $\\mathcal{N}(\\theta|\\mu=0, \\sigma^2=100.)$\n",
    "- Uninformative (objective), *e.g.* my parameter is positive\n",
    "\n",
    "Priors should \n",
    "- add positive probabilistic weights on possible values\n",
    "- no weight to impossible values\n",
    "- help regularize the solution\n",
    "\n",
    "Other guidelines to select priors:\n",
    "- **Conjugate priors:** Given a likelihood the posterior has the same distribution as the prior (more on this later)\n",
    "- Maximum entropy principle [Murphy 4.1.4]\n",
    "\n",
    "\n",
    "### More on priors\n",
    "- Murphy 5.4\n",
    "- [Stan prior choice recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"MAP\"></a>\n",
    "\n",
    "# Maximum *a posteriori* (MAP) estimation\n",
    "\n",
    "***\n",
    "\n",
    "In the Bayesian setting the best \"point estimate\" of the parameters of the model is given by the MAP \n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\text{arg} \\max_\\theta p(\\theta|\\{x\\}) =  \\text{arg} \\max_\\theta p(\\{x\\}| \\theta) p(\\theta),\n",
    "$$\n",
    "\n",
    "where we \"omit\" the evidence because it does not depend on $\\theta$\n",
    "\n",
    "Applying the logarithm (monotonic) we can decouple the likelihood from the prior\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\text{arg} \\max_\\theta \\log p(\\{x\\}| \\theta) + \\log p(\\theta),\n",
    "$$\n",
    "\n",
    "\n",
    "- MAP estimation is also referred as penalized MLE \n",
    "- MAP is still a point estimate: poor's man Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Example: MAP of the mean of a Gaussian dist.\n",
    "\n",
    "We want to find the MAP for the weight of your professor. \n",
    "\n",
    "Assuming that the likelihood is Gaussian with known variance we have\n",
    "\n",
    "$$\n",
    "\\log p(\\{x\\}|\\theta) = \\log L (\\mu)  = - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2, \n",
    "$$\n",
    "\n",
    "and further assuming that the true weight has a Gaussian prior $p(\\theta)=\\mathcal{N}(\\mu|\\mu_0, \\sigma^2_0)$\n",
    "\n",
    "$$\n",
    "\\log p(\\theta) = -\\frac{1}{2} \\log 2 \\pi \\sigma^2_0 - \\frac{1}{2 \\sigma^2_0}  (\\mu - \\mu_0)^2,\n",
    "$$\n",
    "\n",
    "then we set the derivative to zero\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\mu} \\log p(\\{x\\}|\\theta) + \\log p(\\theta) =   \\frac{1}{\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)  - \\frac{1}{ \\sigma^2_0}  (\\mu - \\mu_0) = 0,\n",
    "$$\n",
    "and we get the MAP estimate\n",
    "\n",
    "$$\n",
    "\\hat \\mu_{\\text{map}} =  \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0} \\right)^{-1} \\left(\\frac{N}{\\sigma^2} \\bar x + \\frac{1}{\\sigma^2_0} \\mu_0 \\right),\n",
    "$$\n",
    "where $\\bar x = \\frac{1}{N} \\sum_{i=1}^N x_i$.\n",
    "\n",
    "**IMPORTANT:** Do not confuse $\\sigma^2$ (the noise variance) and $\\sigma^2_0$ (prior variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Particular cases \n",
    "\n",
    "- The MAP estimator for a  standard normal prior $\\mathcal{N}(\\mu| 0, 1)$ is\n",
    "\n",
    "$$\n",
    "\\hat \\mu_{\\text{map}} =  \\left(\\frac{N}{\\sigma^2} + 1 \\right)^{-1} \\left(\\frac{N\\bar x}{\\sigma^2} \\right) = \\frac{1}{1 + \\sigma^2/N} \\bar x,\n",
    "$$\n",
    "\n",
    "note that  \n",
    "\n",
    "$$\n",
    "\\lim_{N \\to \\infty} \\hat \\mu_{\\text{map}} = \\bar x,\n",
    "$$\n",
    "\n",
    "which is the MLE solution\n",
    "\n",
    "\n",
    "- Similarly, the MAP estimator for a normal prior $\\mathcal{N}(\\mu| 0, \\sigma^2_0)$ with $\\sigma^2_0 \\to \\infty$\n",
    "\n",
    "$$\n",
    "\\hat \\mu_{\\text{map}} =  \\left(\\frac{N}{\\sigma^2} \\right)^{-1} \\left(\\frac{N\\bar x}{\\sigma^2} \\right) =  \\bar x,\n",
    "$$\n",
    "\n",
    "which is again the MLE solution\n",
    "\n",
    "> Can you explain the intuition behind this particular solutions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### General case\n",
    "\n",
    "Note that\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\mu_{\\text{map}}  &=  \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0} \\right)^{-1} \\left(\\frac{N\\bar x}{\\sigma^2}  + \\frac{\\mu_0}{\\sigma^2_0} \\right)  \\nonumber \\\\\n",
    "&=  \\frac{N \\bar x \\sigma^2_0 + \\mu_0 \\sigma^2}{N\\sigma^2_0+ \\sigma^2} = \\frac{\\bar x + \\mu_0 \\frac{\\sigma^2}{\\sigma^2_0 N}}{1 + \\frac{\\sigma^2}{\\sigma^2_0 N}}  \\nonumber \\\\\n",
    "&= w \\bar x + (1-w) \\mu_0, \\qquad w = \\frac{1}{1 + \\frac{\\sigma^2}{\\sigma^2_0 N}}  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> The MAP estimate of the mean is a weighted average between $\\mu_0$ and $\\bar x$ (MLE solution)\n",
    "\n",
    "If either $\\sigma^2_0$ or $N$ are large wrt $\\sigma^2$, then $w=1$ and the MLE is recovered\n",
    "\n",
    "\n",
    "**Reflect on the following:** The prior has more influence when\n",
    "- You have few samples\n",
    "- Your samples are noisy\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*** \n",
    "### Extra: MAP intepretation as a penalized MLE/regularized LS\n",
    "\n",
    "The MAP estimate of the mean of a Gaussian dist with known variance using a zero-mean normal prior is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\mu_{\\text{map}}  &= \\text{arg} \\max_\\mu  \\log p(\\{x\\}| \\mu, \\sigma^2) + \\log p(\\mu) \\nonumber \\\\\n",
    "&= \\text{arg} \\max_\\mu   - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 -  \\frac{1}{2\\sigma_0^2} \\mu^2 \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\mu \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 +  \\frac{1}{2\\sigma_0^2} \\mu^2 \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\mu \\|x-\\mu\\|^2  + \\lambda \\|\\mu \\|^2, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\lambda = \\frac{\\sigma^2}{\\sigma_0^2}$. \n",
    "\n",
    "We recognize the last equation as a regularized least squares problem\n",
    "- A Gaussian prior yields a L2 regularizer (ridge regression)\n",
    "- A Laplacian prior yields a L1 regularizer (LASSO)\n",
    "\n",
    "> For more in these see \"3.4 Shrinkage methods\", page 61, Hastie, Tibshirani, Friedman\n",
    "\n",
    "We will review ridge regression in this course (future class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credible Interval (CI) and High Posterior Density (HPD) regions\n",
    "\n",
    "We might be interested in measuring the confidence of the MAP point estimate\n",
    "\n",
    "In the bayesian setting this is done by **measuring the width of the posterior**\n",
    "\n",
    "The $100(1-\\alpha)$ % CI of $\\theta$ is a contiguous region $[\\theta_{l}, \\theta_{u}]$ such that\n",
    "\n",
    "$$\n",
    "P(\\theta_{l}< \\theta < \\theta_{u}) = 1 - \\alpha\n",
    "$$\n",
    "\n",
    "We have to either know the functional form of the posterior (Analytical) or that we can sample from it (MCMC)\n",
    "\n",
    "The HPD is an alternative to CI that is better when we have multiple modes\n",
    "\n",
    "> See Murphy 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "<a id=\"Bayes\"></a>\n",
    "\n",
    "\n",
    "# Analytical posterior with conjugate prior \n",
    "\n",
    "The MAP is only a point estimate\n",
    "\n",
    "For the mean of a Gaussian distribution we can get the full posterior analytically\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta |\\{x\\}) &\\propto p(\\{x\\} |\\theta ) p(\\theta ) \\nonumber \\\\\n",
    "&\\propto \\exp \\left ( \\frac{1}{2\\sigma^2} \\sum_i (x_i - \\mu)^2 \\right) \\exp \\left ( \\frac{1}{2\\sigma_0^2} (\\mu - \\mu_0)^2 \\right) \\nonumber \\\\\n",
    "&\\propto \\exp \\left ( -\\frac{1}{2 \\hat \\sigma^2} (\\mu - \\hat \\mu_{\\text{map}} )^2 \\right),  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat \\sigma^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0} \\right)^{-1} \n",
    "$$\n",
    "\n",
    "> The Gaussian is conjugate to itself\n",
    "\n",
    "Other way to show this is to use the [property of Gaussian pdf multiplication](http://www.tina-vision.net/docs/memos/2003-003.pdf)\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu_1, \\sigma_1^2) \\mathcal{N}(x|\\mu_2, \\sigma_2^2) = C \\mathcal{N}\\left(x\\bigg\\rvert \\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\\left( \\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2}\\right), \\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\\right)\n",
    "$$\n",
    "\n",
    "where $C$ is a scaling constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(10, 4))\n",
    "x = np.linspace(-10, 10, num=10000)\n",
    "def update(mu, s2, mu0, s20, N):\n",
    "    np.random.seed(0); \n",
    "    xi = mu + np.sqrt(s2)*np.random.randn(N)\n",
    "    ax.cla(); ax.set_xlim([-5, 5]);\n",
    "    # likelihood\n",
    "    likelihood = gaussian_pdf(x, np.mean(xi), np.sqrt(s2/N)) #Assumming we know s2\n",
    "    ax.scatter(mu, 0, c='k', s=100, zorder=100)\n",
    "    ax.plot(x, likelihood, label='MLE'); \n",
    "    ax.fill_between(x, 0, likelihood, alpha=0.5)\n",
    "    # prior\n",
    "    prior = gaussian_pdf(x, mu0, s20) \n",
    "    ax.plot(x, prior, label='prior'); \n",
    "    ax.fill_between(x, 0, prior, alpha=0.5)\n",
    "    s2_pos = (N/s2 + 1./s20)**-1\n",
    "    mu_pos = (np.sum(xi)/s2 + mu0/s20)*s2_pos;\n",
    "    posterior = gaussian_pdf(x, mu_pos, np.sqrt(s2_pos));\n",
    "    ax.plot(x, posterior, label='posterior'); ax.fill_between(x, 0, posterior, alpha=0.5)\n",
    "    plt.legend()    \n",
    "\n",
    "interact(update, \n",
    "         mu=FloatSlider_nice(description=r\"$\\mu$\", min=-3, max=3, value=2.), \n",
    "         s2=FloatSlider_nice(description=r\"$\\sigma^2$\", min=0.1, max=10., value=1.),\n",
    "         mu0=FloatSlider_nice(description=r\"$\\mu_0$\", min=-3, max=3, value=0.), \n",
    "         s20=FloatSlider_nice(description=r\"$\\sigma_0^2$\", min=0.1, max=10., value=1.),\n",
    "         N=SelectionSlider_nice(options=[1, 2, 5, 10, 20, 50, 100], value=10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we assumed that the variance $\\sigma^2$ was known \n",
    "\n",
    "### Conjugate priors when $\\sigma^2$ is unknown\n",
    "\n",
    "Assuming that the mean $\\mu$ is known the conjugate prior for the variance is an inverse-Gamma distribution\n",
    "\n",
    "$$\n",
    "p(\\sigma^2) = \\text{IG}(\\sigma^2| \\alpha_0, \\beta_0) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} x^{-\\alpha_0-1} e^{-\\frac{\\beta_0}{x}}\n",
    "$$\n",
    "\n",
    "And the resulting posterior is also \n",
    "\n",
    "$$\n",
    "\\text{IG}\\left(\\sigma^2| \\alpha_N , \\beta_N  \\right)\n",
    "$$\n",
    "\n",
    "with\n",
    "- $ \\alpha_N = \\alpha_0 + N/2$\n",
    "- $\\beta_N = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^N (x_i - \\mu)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import invgamma as dist\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "def update(a,b):\n",
    "    ax.cla()\n",
    "    x = np.linspace(dist.ppf(0.01, a, scale=b), dist.ppf(0.99, a, scale=b), 100)\n",
    "    px = dist.pdf(x, a, scale=b)\n",
    "    ax.plot(x, px, linewidth=2)\n",
    "    ax.set_xlim([np.amin(x)*0.9, np.amax(x)*1.1])\n",
    "    ax.set_ylim([np.amin(px)*0.9, np.amax(px)*1.1])  \n",
    "    \n",
    "interact(update, a=FloatSlider_nice(min=0.01, max=10.0), \n",
    "         b=FloatSlider_nice(min=0.01, max=10.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both $\\alpha$ and $\\beta$ encode the strength of the prior the following parameterization is broadly used\n",
    "\n",
    "$$\n",
    "p(\\sigma^2) = \\text{IG}(\\sigma^2| \\alpha, \\beta) = \\text{IG}\\left(\\sigma^2| \\frac{\\nu}{2}, \\frac{\\nu \\sigma_0^2}{2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma_0^2$ controls the value of the prior and $\\nu$ the strength\n",
    "\n",
    "This is also closely related to the [inverse chi-square distribution](https://en.wikipedia.org/wiki/Inverse-chi-squared_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conjugate priors when $\\mu$ and $\\sigma^2$ are unknown\n",
    "\n",
    "- Multiplying the normal prior and the IG prior does not yield a conjugate prior (assumes independence of $\\mu$ and $\\sigma$)\n",
    "- In this case the conjugate prior is hierarchical\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_i|\\mu, \\sigma^2) &= \\mathcal{N}(\\mu, \\sigma^2)  \\nonumber \\\\\n",
    "p(\\mu|\\sigma^2) &= \\mathcal{N}(\\mu_0, \\sigma^2/\\lambda_0)  \\nonumber \\\\\n",
    "p(\\sigma^2) &= \\text{IG}(\\alpha, \\beta)  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is called **normal-inverse-gamma (NIG)**, a four parameter distribution \n",
    "\n",
    "The NIG prior is\n",
    "\n",
    "$$\n",
    "p(\\mu, \\sigma^2) = \\text{NIG}(\\mu_0, \\lambda_0, \\alpha_0, \\beta_0) = \\mathcal{N}(\\mu|\\mu_0 , \\sigma^2/\\lambda_0) \\text{IG}(\\sigma^2|\\alpha_0, \\beta_0)\n",
    "$$\n",
    "\n",
    "An the posterior is also NIG\n",
    "\n",
    "$$\n",
    "p(\\mu, \\sigma^2|\\{x\\}) =  \\text{NIG}(\\mu_n, \\lambda_n, \\alpha_n, \\beta_n)\n",
    "$$  \n",
    "\n",
    "where\n",
    "\n",
    "- $\\lambda_n = \\lambda_0 + N$\n",
    "- $\\mu_n = \\lambda_n^{-1} \\left ( \\lambda_0 \\mu_0  + N \\bar x \\right)$\n",
    "- $\\alpha_n = \\alpha_0 + N/2$\n",
    "- $\\beta_n = \\beta_0 + 0.5\\mu_0^2\\lambda_0 + 0.5\\sum_i x_i^2 - 0.5\\lambda_n \\mu_n^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conjugate priors for multivariate Gaussian \n",
    "\n",
    "We use the Inverse Wishart (IW), a multidimensional generalization of Inverse Gamma\n",
    "\n",
    "IW is a distribution over positive semi-definite matrices: covariance\n",
    "\n",
    "See Murphy 4.5 & 4.6 for more details\n",
    "\n",
    "\n",
    "#### More resources\n",
    "- Document by Kevin Murphy on conjugate priors for the Gaussian dist: https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\n",
    "\n",
    "### More examples on conjugate priors, Bayesian updates and a bit on model selection\n",
    "\n",
    "https://github.com/magister-informatica-uach/INFO3XX/blob/master/0_probabilities_inference.ipynb\n",
    "\n",
    "### More on Bayesian model selection\n",
    "\n",
    "[Murphy 5.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Sneak peak: Probabilistic programming with [PyMC3](https://docs.pymc.io/)\n",
    "\n",
    "***\n",
    "\n",
    "PyMC3 is a library to learn bayesian models using MCMC and VI (variational inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "np.random.seed(0)\n",
    "N, mu_t, sigma_t = 100, 10, 2\n",
    "x = mu_t + sigma_t*np.random.randn(N)\n",
    "print(\"MLE mu: %f %f\" %(np.mean(x), np.std(x)/np.sqrt(N)))\n",
    "\n",
    "with pm.Model() as demo:\n",
    "    #mu_0 = pm.Normal('mu0', mu=0, sd=10, shape=1)\n",
    "    sigma = pm.HalfNormal('s', sd=100, shape=1)\n",
    "    mu = pm.Normal('mu', mu=0, sd=100, shape=1)\n",
    "    x_observed = pm.Normal('x_obs', mu=mu, sd=sigma, observed=x)\n",
    "    trace = pm.sample(draws=10000, tune=2000, init='advi', n_init=20000, cores=4, chains=2, \n",
    "                      live_plot=True)\n",
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, figsize=(8, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Future topics\n",
    "\n",
    "- Empirical Bayes: Model in which the hyperparameters are estimated from data instead of fixed before-hand\n",
    "- Hierarchical Bayes: Hyper-priors assigned to the parameters of the priors\n",
    "- Markov Chain Monte Carlo (MCMC): Algorithm to sample from a distribution. We will use it to learn complex Bayesian models\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Extra: Mean of the posterior\n",
    "\n",
    "Other point estimate that can be used to characterize the posterior is\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\mathbb{E}[\\theta|\\{x\\}] = \\int \\theta p(\\theta| \\{x\\}) d\\theta,\n",
    "$$\n",
    "\n",
    "*i.e.* the mean or expected value of the posterior.\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
