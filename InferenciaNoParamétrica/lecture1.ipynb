{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de inferencia paramétrica\n",
    "\n",
    "- Queremos estimar los parámetros $\\mu$ y $\\sigma^2$ de la población\n",
    "- A partir de las observaciones construimos una estadístico muestral: $\\bar X = \\sum_i X_i/N$\n",
    "- Si **asumimos** que la población se distribuye normal entonces: $\\bar X = \\mathcal{N}(\\mu, \\sigma^2/N)$, es decir que \n",
    "    - En promedio la media muestral equivale a la media de la población\n",
    "    - A mayor número de muestras menor es la varianza de nuestro estimador\n",
    "- Para tomar decisiones basados en $\\bar X$ usamos un estadístico de test: $t = \\frac{\\bar X - \\mu}{\\sqrt{\\hat \\sigma^2/N}}$ (o estadístico Z si $\\sigma$ es conocido)\n",
    "- Dado que conocemos la distribución de $\\bar X$ podemos calcular la distribución de $t$ bajo la hipótesis nula (e.g. $\\mu=0$) y calcular el p-value \n",
    "\n",
    "> p-value: pbb de observar un valor más grande que t si la hipótesis nula fuera cierta\n",
    "\n",
    "### El análisis descansa en los supuestos sobre la distribución de la población\n",
    "\n",
    "¿Qué ocurre en el ejemplo anterior si el supuesto sobre la distribución población estaba equivocado?\n",
    "\n",
    "La distribución muestral será erronea, la distribución nula será erronea y el p-value será erroneo\n",
    "\n",
    "### Horror ¿Qué hago?\n",
    "\n",
    "1. (Mantén la calma)\n",
    "1. En la práctica los tests parámetricos funcionan con \"ligeras\" desviaciones de los supuestos\n",
    "1. Podemos comprobar los supuestos antes de hacer el análisis\n",
    "1. Si los supuestos no se cumplen podemos usar **inferencia no paramétrica**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluando normalidad\n",
    "\n",
    "La normalidad es un supuesto usado en muchos procedimientos parámetricos: t-test, ANOVA, regresion lineal\n",
    "\n",
    "Tengamos en cuanta las características principales de la distribución normal\n",
    "- Unimodal con media igual a su moda\n",
    "- Simétrica en torno a la media\n",
    "- Concentrada en torno a la media: ~68% a $\\pm \\sigma$, ~95% a $\\pm 2\\sigma$, ...\n",
    "\n",
    "Si tenemos suficientes muestras podriamos observar el histograma y comprobar visualmente si esto se cumple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = scipy.stats.norm(loc=4, scale=2).rvs(1000)\n",
    "data2 = scipy.stats.gamma(a=2, scale=1).rvs(1000)\n",
    "data3 = scipy.stats.uniform(loc=2, scale=1).rvs(1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(7, 2.5), tight_layout=True)\n",
    "for i, data in enumerate([data1, data2, data3]):\n",
    "    #data = (data - np.mean(data))/np.std(data)\n",
    "    ax[i].hist(data, bins=10, density=True)\n",
    "    ax[i].set_xlabel('data'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentos estadísticos\n",
    "\n",
    "En el caso particular de la normal sabemos que el tercer momento (simetría) debiese ser cero\n",
    "\n",
    "Podemos calcular la simetría usando [`scipy.stats.skew`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html), para el caso particular de la normal puede ser util [`scipy.stats.skewtest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skewtest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate([data1, data2, data3]):\n",
    "    display(scipy.stats.skew(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cuarto momento (kurtosis) también tiene un valor particular en el caso de la distribución normal\n",
    "\n",
    "La función [`scipy.stats.kurtosis`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosis.html) tiene dos definiciones de Kurtosis\n",
    "\n",
    "La definición por defecto (Fisher) nos da cero en el caso de que la distribución sea normal\n",
    "\n",
    "Puede ser también util [`scipy.stats.kurtosistest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosistest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate([data1, data2, data3]):\n",
    "    display(scipy.stats.kurtosis(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability plots\n",
    "\n",
    "Otra opción gráfica son los *probability plots* (Chambers et al., 1983) \n",
    "\n",
    "Se calculan los cuantiles (qq) o percentiles (pp) de dos muestras o de una muestra y una distribución teórica\n",
    "\n",
    "Luego se grafican. Mientras más se parezca el resultado a una linea recta más similares son las distribuciones\n",
    "\n",
    "Para mayores detalles teóricos: https://itl.nist.gov/div898/handbook/eda/section3/probplot.htm\n",
    "\n",
    "Podemos usar `scipy.stats.probplot` para comparar una muestra con una distribución teórica \n",
    "\n",
    "- En este caso la distribución teórica será la normal estándar\n",
    "- Dado que la prueba es basada en rangos no necesitamos entregar parámetros de localización (media) o escala (varianza)\n",
    "- Cualquier parámetro de otro tipo (shape) se debe indicar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(7, 2.5), tight_layout=True)\n",
    "for i, data in enumerate([data1, data2, data3]):\n",
    "    #data = (data - np.mean(data))/np.std(data)\n",
    "    (osm, osr), (w, b, r2) = scipy.stats.probplot(data, dist=\"norm\", fit=True);\n",
    "    ax[i].plot(osm, osr)\n",
    "    ax[i].plot(np.arange(-3, 4), np.arange(-3, 4)*w + b, 'k--', alpha=0.25)\n",
    "    ax[i].set_title('data'+str(i));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuestro primer test no-paramétrico:  Kolmogorov-Smirnov (KS)\n",
    "\n",
    "Este test examina la distancia entre las funciones de distribución acumulada (CDF) de dos muestras\n",
    "\n",
    "- Solo se puede usar para distribuciones univariadas continuas\n",
    "- Si se compara una muestra empírica con una CDF teórica: Test de KS de una muestra\n",
    "- Si se comparan dos muestras empíricas: Test de KS de dos muestras\n",
    "\n",
    "> Test no-paramétrico: No asume distribuciones\n",
    "\n",
    "El estadístico de test es para KS de una muestra es\n",
    "\n",
    "$$\n",
    "D_n = \\sup_x |F_n(x) - F(x)|\n",
    "$$\n",
    "\n",
    "donde $F(x) = \\int_{-\\infty}^x f(x) dx = P(X<x)$ es la CDF teórica y \n",
    "\n",
    "$$\n",
    "F_n(x) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{1}[X_i<x]\n",
    "$$\n",
    "\n",
    "es la distribución acumulada empírica (ECDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    n = x.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return(x,y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(7, 2.5), tight_layout=True)\n",
    "for i, data in enumerate([data1, data2, data3]):\n",
    "    # ECDF\n",
    "    data = (data - np.mean(data))/np.std(data)\n",
    "    x, y = ecdf(data)\n",
    "    ax[i].plot(x, y)\n",
    "    # CDF standard normal\n",
    "    x_t = np.linspace(np.amin(data), np.amax(data), num=100)\n",
    "    y_t = scipy.stats.norm.cdf(x_t)    \n",
    "    ax[i].plot(x_t, y_t, 'k--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $D_n$ es la distancia más larga entre todas las distancias de la CDF teórica y empírica\n",
    "\n",
    "Las hipótesis de la prueba KS de una muestra son\n",
    "\n",
    "- $\\mathcal{H}_0$: Los datos siguen la distribución teórica especificada\n",
    "- $\\mathcal{H}_A$: Los datos no siguen la distribución teórica especificada\n",
    "\n",
    "Si $D_n$ es menor que el valor crítico no podemos rechazar $H_0$\n",
    "\n",
    "Podemos usar `scipy.stats.kstest` para probar nuestros datos contra la distribución normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate([data1, data2, data3]):\n",
    "    data = (data - np.mean(data))/np.std(data)\n",
    "    Dn, pvalue = scipy.stats.kstest(data, scipy.stats.norm.cdf)\n",
    "    print(i, Dn, pvalue)\n",
    "    if pvalue < 0.05:\n",
    "        display(\"Rechazo la hipótesis nula: nuestros datos no siguen la distribución especificada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemas de Kolmogorov-Smirnov\n",
    "\n",
    "- Solo para distribuciones continuas y univariadas\n",
    "- Es más sensible en el centro que en las colas de la distribución\n",
    "- Se debe especificar completamente la distribución teórica\n",
    "\n",
    "Mejora de KS que le da más ponderación a las colas: [Test de Anderson-Darling](https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm), con implementación en [`scipy.stats.anderson`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad \n",
    "\n",
    "Use las herramientas que acabamos de estudiar para probar la normalidad de los siguientes datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('mistery_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforme los datos usando logaritmo y repita el procedimiento\n",
    "\n",
    "¿A que distribución corresponde?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia no paramétrica\n",
    "\n",
    "Inferencia: Estimar parámetros o probar hipótesis sobre una población\n",
    "\n",
    "En inferencia **no parámetrica** usamos estadísticos cuya distribución **no depende de supuestos sobre la distribución de la población**\n",
    "\n",
    "- No paramétrico $=$ No asumimos distribución para la población \n",
    "- No paramétrico $\\neq$ Libre de supuestos (e.g. continuidad, muestras independientes)\n",
    "- No paramétrico $\\neq$ Sin parámetros (e.g. la cantidad de bines de un histograma)\n",
    "\n",
    "Trade-off: \n",
    "\n",
    "- Si sus supuestos se cumplen, los métodos parámetricos son la ópcion de mayor poder estadístico (sensibilidad)\n",
    "- Los métodos no-parámetricos tienen un poder igual o menor pero no requieren de supuestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
