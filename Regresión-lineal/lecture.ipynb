{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and matplotlib configuration\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import animation\n",
    "from ipywidgets import interact, Button, Output, Box\n",
    "from IPython.display import display\n",
    "from style import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Universidad Austral de Chile\n",
    "\n",
    "# INFO337 - Herramientas estadísticas para la investigación\n",
    "\n",
    "# Linear regression\n",
    "\n",
    "### A course of the masters in informatics program\n",
    "\n",
    "### https://github.com/magister-informatica-uach/INFO337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"index\"></a>\n",
    "\n",
    "# Contents\n",
    "***\n",
    "\n",
    "1. [Introduction](#section1)\n",
    "1. [The simplest linear model](#section2)\n",
    "1. [Ordinary least squares](#section3)\n",
    "1. [Overfitting and regularization](#section4)\n",
    "1. [Extra topics](#section5)\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. Bishop, \"Pattern recognition and machine learning\", **Chapter 3**\n",
    "1. Hastie, Tibshirani and Friedman, \"The elements of statistical learning\" 2nd Ed., *Springer*, **Chapter 3**\n",
    "1. Murphy, \"Machine Learning: A Probabilistic Perspective\", *MIT Press*, 2012, **Chapter 7**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "***\n",
    "<a id=\"section1\"></a>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "***\n",
    "\n",
    "- Many problems can be posed as \"finding a relation\" between factors/variables\n",
    "- This can be interpreted as predicting and/or explaining a variable given others\n",
    "    - Sales given money spent in advertising\n",
    "    - Chance to rain in Valdivia given temperature, pressure and humidity\n",
    "    - Gasoline consumption of a car given acceleration, weight and number of cylinders\n",
    "    - Chance to get lung cancer given number of smoked cigarettes per day, age and gender\n",
    "- We can ask\n",
    "    - Are the variable related?\n",
    "    - How strong/significant is the relationship?\n",
    "    - What is the nature of the relationship?\n",
    "- Answering these helps us understand the underlying processes behind the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "## Defining regression\n",
    "\n",
    "- **Regression** refers to a family of statistical methods to find **relationships** between **variables**\n",
    "- In general the relation is modeled as a function $g(\\cdot)$ that maps two types of variables\n",
    "    - The input variable $X$ is called **independent variable** or feature\n",
    "    - The output variable $Y$ is called **dependent variable**, response or target\n",
    "- The mapping or function $g$ is called **predictor** or **regressor**\n",
    "$$\n",
    "g: X \\to Y\n",
    "$$\n",
    "- The objective is to learn $g$ such that we can predict $Y$ given $X$, *i.e.* $\\mathbb{E}[Y|X]$ \n",
    "\n",
    "## A context for regression\n",
    "\n",
    "- **Regression** can be defined from an statistical perspective as a special case of model fitting (parameter estimation)\n",
    "- In many books **Regression** is defined from a pure-optimization perspective (deterministic)\n",
    "- **Regression** is considered part of the *supervised learning* paradigm\n",
    "- The difference between **Regression** and *classification* is the nature of the dependent variable (continuous vs categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "## Parametric vs non-parametric  regression\n",
    "\n",
    "Regression methods can be broadly classified as either parametric or non-parametric \n",
    "\n",
    "In parametric regression\n",
    "- We know the model of the regressor\n",
    "- The model has a finite number of parameters\n",
    "- The parameters of the model are all we need to do predictions \n",
    "- Simpler but with bigger assumptions (inductive bias)\n",
    "- **In this lecture we will focus on parametric regression**\n",
    "\n",
    "In nonparametric regression\n",
    "- There is no functional form for the regressor\n",
    "- It can have an infinite number of parameters (and a finite number of hyperparameters)\n",
    "- The regressor is defined from the training data\n",
    "- More flexible but requires more data to fit it\n",
    "- Examples: Splines, Support vector regression, Gaussian processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Parametric models for regression\n",
    "\n",
    "- Let $X$ be a continuous D-dimensional variable (feature)\n",
    "- Let $Y$ be a continuous unidimensional variable (target) \n",
    "- Let $\\{x_i, y_i\\}$ with $i=1,\\ldots,N$ be a set of $N$ *iid* observations of $X$ and $Y$\n",
    "- Let $g_\\theta$ be a model with a M-dimensional parameter $\\theta$ \n",
    "- Then we can define parametric regression as finding a value of $\\theta$ such that \n",
    "$$\n",
    "y_i \\approx g_\\theta(x_i),\\quad i=1,\\ldots, N\n",
    "$$\n",
    "\n",
    "### Linear models for regression\n",
    "\n",
    "- The simplest parametric model is the **linear model**\n",
    "- A linear model gives rise to **linear regression**\n",
    "- The linear model is linear on $\\theta$ but not necessarily on $X$\n",
    "- For example a model with unidimensional input\n",
    "$$\n",
    "g_\\theta \\left(x_i \\right) = \\theta_0 + \\theta_1 x_i  + \\theta_2 x_i^2,\n",
    "$$\n",
    "is a linear model and\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 \\log(x_i),\n",
    "$$\n",
    "is also a linear model but\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\log(x_i + \\theta_1),\n",
    "$$\n",
    "is not a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"section2\"></a>\n",
    "\n",
    "# The simplest linear model (two-dimensions)\n",
    "***\n",
    "\n",
    "If we consider a one-dimensional variable $x_i \\in \\mathbb{R}, i=1,\\ldots,N$, then the simplest linear model is\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 x_i\n",
    "$$\n",
    "\n",
    "which has $M=2$ parameters. \n",
    "\n",
    "This corresponds to a line in $\\mathbb{R}^2$ and we recognize\n",
    "\n",
    "- $\\theta_0$ as the intercept\n",
    "- $\\theta_1$ as the slope\n",
    "\n",
    "***\n",
    "If we consider a two-dimensional variable $x_i = (x_{i1}, x_{i2}) \\in \\mathbb{R}^2, i=1,\\ldots,N$ then we obtain\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2}\n",
    "$$\n",
    "\n",
    "which has $M=3$ parameters. This corresponds to a plane in $\\mathbb{R}^3$\n",
    "\n",
    "***\n",
    "\n",
    "The most general form assumes a D-dimensional variable $x_i = (x_{i1}, x_{i2}, \\ldots, x_{iD}), i=1,\\ldots,N$ \n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}\n",
    "$$\n",
    "\n",
    "which has $M=D+1$ parameters, which corresponds to an hyperplane in $\\mathbb{R}^M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fitting the simplest linear model\n",
    "\n",
    "- Assuming that we have $\\{x_i, y_i\\}_{i=1,\\ldots,N}$ *iid* observations from unidimensional variables X and Y\n",
    "- How do we find $\\theta_0$ and $\\theta_1$ such that $y_i \\approx \\theta_0 + \\theta_1 x_i, \\forall i$?\n",
    "- We can start by writing the squared residual (error) as \n",
    "$$\n",
    "E_i^2 = (y_i - \\theta_0 - \\theta_1 x_i)^2\n",
    "$$\n",
    "- We would like to make the sum of squared errors as small as we possible so we seek\n",
    "$$\n",
    "\\min_{\\theta} L = \\sum_{i=1}^N E_i^2 = \\sum_{i=1}^N (y_i - \\theta_0 - \\theta_1 x_i)^2,\n",
    "$$\n",
    "where $L$, the sum of squares errors, is a our loss/cost function\n",
    "\n",
    "***\n",
    "\n",
    "From here we can do\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{d\\theta_0} &= -2 \\sum_{i=1}^N (y_i - \\theta_0 - \\theta_1 x_i) \\nonumber \\\\\n",
    "&= -2 \\sum_{i=1}^N y_i +  2 N\\theta_0 + 2 \\theta_1 \\sum_{i=1}^N x_i = 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{d\\theta_1} &= -2 \\sum_{i=1}^N (y_i - \\theta_0 - \\theta_1 x_i) x_i \\nonumber \\\\\n",
    "&= -2 \\sum_{i=1}^N y_i x_i +  2 \\theta_0 \\sum_{i=1}^N x_i + 2 \\theta_1 \\sum_{i=1}^N x_i^2 = 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "a system of two equations and two unknowns\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} N & \\sum_i x_i \\\\ \\sum_i x_i & \\sum_i x_i^2\\\\\\end{pmatrix} \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}  = \\begin{pmatrix} \\sum_i y_i \\\\ \\sum_i x_i y_i \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "whose solution is \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} \\hat \\theta_0 \\\\ \\hat \\theta_1 \\end{pmatrix}  = \n",
    "\\frac{1}{N\\sum_i x_i^2 - (\\sum_i x_i)^2}\\begin{pmatrix} \\sum_i x_i^2 & -\\sum_i x_i \\\\ -\\sum_i x_i & N\\\\\\end{pmatrix}  \n",
    "\\begin{pmatrix} \\sum_i y_i \\\\ \\sum_i x_i y_i \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "where we assume that the determinant complies with: $N\\sum_i x_i^2 - (\\sum_i x_i)^2 \\neq 0$\n",
    "\n",
    "Note that in this case\n",
    "\n",
    "$$\n",
    "\\hat \\theta_1 = \\frac{\\text{Cov}[x, y]}{\\text{Var}[x]}, \\hat \\theta_0 = \\bar y - \\hat \\theta_1 \\bar x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig = plt.figure(figsize=(10, 7))\n",
    "theta = np.random.randn(2); display(repr(theta))\n",
    "x = np.linspace(-5, 5, num=10);\n",
    "def update(rseed, sigma):\n",
    "    np.random.seed(rseed); \n",
    "    y = theta[0] + theta[1]*x + sigma*np.random.randn(len(x))\n",
    "    sum_x2 = np.sum(x**2); sum_x = np.sum(x)\n",
    "    sum_xy = np.sum(x*y); sum_y = np.sum(y)\n",
    "    theta_hat = np.dot([[sum_x2, -sum_x],[-sum_x, len(x)]], [sum_y, sum_xy])/(len(x)*sum_x2 - sum_x**2)\n",
    "    display(repr(theta_hat))\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax.set_ylim([-10, 10])\n",
    "    ax.scatter(x, y, c='r', s=100, label='data')\n",
    "    ax.vlines(x, theta_hat[0] + theta_hat[1]*x, y, 'r')\n",
    "    ax.plot(x, theta[0] + theta[1]*x, 'b--', linewidth=4, alpha=0.75, label='underlying'); \n",
    "    ax.plot(x, theta_hat[0] + theta_hat[1]*x, 'k-', linewidth=4, label='model'); plt.legend();\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0))\n",
    "    ax.plot(x, np.zeros_like(x), 'k--', alpha=0.5)\n",
    "    ax.scatter(x, y - theta_hat[0] - theta_hat[1]*x, c='r', s=100);\n",
    "    ax.set_ylabel('Residuals'); \n",
    "    \n",
    "interact(update, rseed=IntSlider_nice(), sigma=SelectionSlider_nice(options=[0.1, 0.5, 1, 5.]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quality of the linear fit\n",
    "\n",
    "Can we measure how strong is $y$ related to $\\hat y = \\hat \\theta_0 + \\hat \\theta_1 x$?\n",
    "\n",
    "YES: **Coefficient of determination** or $r^2$\n",
    "$$\n",
    "r^2 = 1 - \\frac{\\sum_i (y_i - \\hat y_i)^2}{\\sum_i (y_i - \\bar y_i)^2}\n",
    "$$\n",
    "- one minus the Sum of residuals divided by the variance of y\n",
    "- $r$ is equivalent to Pearson's correlation coefficient\n",
    "- $r^2 \\in [0, 1]$\n",
    "- If $r^2 = 1$, the data points are fitted perfectly by the model. The regressor accounts for all of the variation in y\n",
    "- If $r^2 = 0$, the regression line is horizontal. The regressor accounts for none of the variation in y\n",
    "- If the relation is strong but non-linear it will not be detected by $r^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"section3\"></a>\n",
    "\n",
    "# Ordinary Least Squares (OLS)\n",
    "***\n",
    "\n",
    "- Now we will generalize the previous solution\n",
    "- Assuming that we have $\\{x_i, y_i\\}_{i=1,\\ldots,N}$ *iid* observations from unidimensional  Y and **D-dimensional variable** X\n",
    "- How do we find $\\theta$  such that $y_i \\approx \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}, \\forall i$?\n",
    "- We can write the sum of squared errors (residuals) as\n",
    "$$\n",
    "\\min_\\theta L = \\sum_{i=1}^N (y_i - \\theta_0 - \\sum_{j=1}^D \\theta_j x_{ij})^2\n",
    "$$\n",
    "\n",
    "- For simplicity consider the matrices\n",
    "$$\n",
    "X = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1D} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{ND} \\end{pmatrix},  Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}, \\theta =  \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_D \\end{pmatrix}\n",
    "$$\n",
    "- We can write the cost function in matrix form as\n",
    "$$\n",
    "\\min_\\theta  L = \\| Y - X \\theta \\|^2 = (Y - X \\theta)^T (Y - X \\theta)\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "From here we can do\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\theta} = -(Y - X \\theta)^T X =  -X^T (Y - X \\theta) = 0\n",
    "$$\n",
    "\n",
    "to obtain the **normal equations**\n",
    "\n",
    "$$\n",
    "X^T X \\theta  = X^T Y\n",
    "$$\n",
    "\n",
    "and if $X^T X$ is invertible then\n",
    "\n",
    "$$\n",
    "\\hat \\theta = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "which  is called the **least squares (LS) estimator** of $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.close('all'); fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "theta = [4, 3, 2]; display(repr(theta))\n",
    "x = np.linspace(-5, 5, num=10);\n",
    "X, Y = np.meshgrid(x, x)\n",
    "XY1 = np.stack((np.ones_like(X.ravel()), X.ravel(), Y.ravel())).T\n",
    "def update(rseed, sigma):\n",
    "    np.random.seed(rseed); \n",
    "    Z_clean = theta[0] + theta[1]*X + theta[2]*Y \n",
    "    Z = Z_clean + sigma*np.random.randn(len(x), len(x))\n",
    "    param, MSE, rank, singval = np.linalg.lstsq(XY1, Z.ravel(), rcond=None)\n",
    "    display(repr(param))\n",
    "    ax.cla();\n",
    "    ax.scatter(X, Y, Z, s=100, c='r', label='data')\n",
    "    ax.plot_wireframe(X, Y, Z_clean, label='underlying')\n",
    "    ax.plot_wireframe(X, Y, param[0] + param[1]*X + param[2]*Y, color='k', label='model')\n",
    "    plt.legend()\n",
    "    \n",
    "interact(update, rseed=IntSlider_nice(), sigma=SelectionSlider_nice(options=[0.1, 1, 10.]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the two-dim linear model we found these normal equations\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} N & \\sum_i x_i \\\\ \\sum_i x_i & \\sum_i x_i^2\\\\\\end{pmatrix} \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}  = \\begin{pmatrix} \\sum_i y_i \\\\ \\sum_i x_i y_i \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "and they can be written as\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} 1 & 1 & \\ldots & 1 \\\\ x_1 & x_2 & \\ldots & x_N \\end{pmatrix} \n",
    "\\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_N \\end{pmatrix} \n",
    "\\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}  &= \n",
    "\\begin{pmatrix} 1 & 1 & \\ldots & 1 \\\\ x_1 & x_2 & \\ldots & x_N \\end{pmatrix} \n",
    "\\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix} \\nonumber \\\\\n",
    "X^T X \\theta &= X^T Y \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "the same form as the general case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "### Numerical considerations of the LS solution\n",
    "\n",
    "The LS estimator is\n",
    "\n",
    "$$\n",
    "\\hat \\theta = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "- The LS estimator is a batch solution, *i.e.* it uses all observations at once\n",
    "    - If N is large it might be impossible to obtain it\n",
    "    - In that case it might be better to use stochastic gradient descent \n",
    "- $X^{\\dagger} = (X^T X)^{-1} X^T $ is known as the *Moore-Penrose* pseudo-inverse\n",
    "    - $X^{\\dagger}$ is the inverse for matrices that are not squared\n",
    "    - If X is squared and invertible then $X^{\\dagger} = (X^T X)^{-1} X^T  = X^{-1} (X^T)^{-1} X^T = X^{-1}$\n",
    "- The LS estimator is valid only if we can invert $X^T X$\n",
    "    - $X^T X$ is a squared symmetric $M\\times M$ matrix\n",
    "    - If $(X^T X)^{-1}$ exists the LS solution is unique and the problem is *well-posed*\n",
    "    - If $X^T X$ is positive definite, *i.e.* $z^T X^T X z > 0, \\forall z$ then its inverse exist\n",
    "    - $X^T X$ being positive definite is equivalent to each of the following\n",
    "        - The columns of $X$ are Linearly Independent (LI)\n",
    "        - All the eigenvalues of $X^T X$ are positive\n",
    "        - The Cholesky decomposition of $X^T X$ exists\n",
    "- **Trick of the trade:** Add a small value to the diagonal of $X^T X$ to make it invertible\n",
    "    - We will see later that this is equivalent to regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quality of D-dimensional linear fit\n",
    "\n",
    "In this case the **Coefficient of determination** or $r^2$ can be written as\n",
    "$$\n",
    "\\begin{align}\n",
    "r^2 &= 1 - \\frac{\\sum_i (y_i - \\hat y_i)^2}{\\sum_i (y_i - \\bar y_i)^2} \\nonumber \\\\\n",
    "&= 1 - \\frac{Y^T(I-X(X^TX)^{-1}X^T)Y}{Y^T (I - \\frac{1}{N} \\mathbb{1}^T \\mathbb{1} ) Y} \\nonumber \\\\\n",
    "&= 1 - \\frac{SS_{res}}{SS_{total}} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbb{1} = (1, 1, \\ldots, 1)$\n",
    "- It has the same interpretation as explained before\n",
    "- Review the previous class on ANOVA and reflect on the similarities, How do we get F distributions from $SS_{res}$ and $SS_{total}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "# Statistical view of OLS\n",
    "\n",
    "- Up to now we have viewed regression from a deterministic (optimization) perspective\n",
    "- To understand the properties and do inference we seek an statistical interpretation\n",
    "- Assuming that we have $\\{x_i, y_i\\}_{i=1,\\ldots,N}$ iid observations from unidimensional  Y and **D-dimensional variable** X\n",
    "- Let's assume that our measurements of $Y$ consists of the **true model** plus **white Gaussian noise**, *i.e.*\n",
    "\n",
    "$$\n",
    "y_i = \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij} + \\varepsilon_i, \\forall i \\quad \\text{and} \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "- Then we can write the log likelihood of $\\theta$ as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log L(\\theta) &= \\log \\prod_{i=1}^N \\mathcal{N}(y_i | (1, x_{i1}, \\ldots, x_{iD}) \\theta, \\sigma^2) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\log \\mathcal{N}(y_i | (1, x_{i1}, \\ldots, x_{iD}) \\theta, \\sigma^2) \\nonumber \\\\\n",
    "&= -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - (1, x_{i1}, \\ldots, x_{iD}) \\theta)^2,\\nonumber \\\\\n",
    "&= -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### Maximum Likelihood of Spherical Gaussian and Least Squares\n",
    "\n",
    "Given that we know $\\sigma > 0$ then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_\\theta \\log L(\\theta) &= -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) \\nonumber \\\\\n",
    "&= - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "We can change the sign and transform *max* to *min*:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_\\theta \\log L(\\theta) &=  \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) \\nonumber \\\\\n",
    "\\implies \\hat \\theta &= (X^T X)^{-1} X^T Y \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> In this case the MLE solution is equivalent to least squares\n",
    "\n",
    "> When using the sum of squares as cost function we are assuming that the noise is Spherical Gaussian distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "# Statistical properties of OLS\n",
    "\n",
    "- Let $\\varepsilon = (\\varepsilon_1, \\varepsilon_2, \\ldots, \\varepsilon_N)$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\forall i$ and assume that X is not random\n",
    "- Is the estimator unbiased?\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat \\theta] &= \\mathbb{E}[(X^T X)^{-1} X^T Y] \\nonumber \\\\\n",
    "&= \\mathbb{E}[(X^T X)^{-1} X^T (X \\theta + \\varepsilon)] \\nonumber \\\\\n",
    "&= \\mathbb{E}[\\theta] + (X^T X)^{-1} X^T \\mathbb{E}[\\varepsilon] \\\\\n",
    "& = \\mathbb{E}[\\theta]\n",
    "\\end{align}\n",
    "$$\n",
    "YES! \n",
    "- What is the variance of the estimator? \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[(\\hat \\theta - \\mathbb{E}[\\hat\\theta])(\\hat \\theta - \\mathbb{E}[\\hat\\theta])^T] &= \\mathbb{E}[((X^T X)^{-1} X^T \\varepsilon) ((X^T X)^{-1} X^T \\varepsilon)^T] \\nonumber \\\\\n",
    "&= (X^T X)^{-1} X^T  \\mathbb{E}[\\varepsilon \\varepsilon^T] X ((X^T X)^{-1})^T  \\nonumber \\\\\n",
    "&= (X^T X)^{-1} X^T  \\mathbb{E}[(\\varepsilon-0) (\\varepsilon-0)^T] X (X^T X)^{-1}  \\nonumber \\\\\n",
    "& =\\sigma^2 (X^T X)^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "because the variance of $\\varepsilon$ is $I\\sigma^2$\n",
    "- Typically we estimate the variance of the noise using the unbiased estimator\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2 &= \\frac{1}{N-D-1} \\sum_{i=1}^N (y_i - \\theta_0 - \\sum_{j=1}^D \\theta_j x_{ij})^2 \\nonumber \\\\\n",
    "& = \\frac{1}{N-D-1} (Y-X\\theta)^T (Y-X\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "- (Hastie, 3.2.2) **The Gauss-Markov Theorem:** The least squares estimate of $\\theta$ have the smallest variance among all unbiased estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Inference with OLS\n",
    "\n",
    "- Previously we found the expected value and the variance of $\\theta$\n",
    "- From the properties of MLE we know that\n",
    "$$\n",
    "    \\hat \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2 (X^T X)^{-1})\n",
    "$$\n",
    "and the estimator of the variance will be proportional to\n",
    "$$\n",
    "\\hat \\sigma^2 \\sim  \\frac{1}{(N-M)}\\sigma^2 \\chi_{N-M}^2\n",
    "$$\n",
    "- With this we have all the ingredients to find confidence intervals and do hypothesis test on $\\hat \\theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Hypothesis test on the regression coefficients\n",
    "\n",
    "To assess the significance of our model we might try to reject the following *hypotheses*\n",
    "\n",
    "- One of the parameters (slopes) is zero\n",
    "\n",
    "    $\\mathcal{H}_0: \\theta_i = 0$\n",
    "    \n",
    "    $\\mathcal{H}_A: \\theta_i \\neq 0$\n",
    "    \n",
    "    \n",
    "- All parameters are zero \n",
    "\n",
    "    $\\mathcal{H}_0: \\theta_1 = \\theta_2 = \\ldots = \\theta_D = 0$\n",
    "\n",
    "    $\\mathcal{H}_A:$ At least one parameter is not zero\n",
    "\n",
    "\n",
    "- A subset of the parameters are zero \n",
    "\n",
    "    $\\mathcal{H}_0: \\theta_i = \\theta_j =0 $\n",
    "\n",
    "    $\\mathcal{H}_A:$ $\\theta_i \\neq 0 $ or $\\theta_j \\neq 0 $\n",
    "\n",
    "\n",
    "> We can do this using t-test, f-test or ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Read more on this at:\n",
    "- https://onlinecourses.science.psu.edu/stat501/node/295/\n",
    "- http://reliawiki.org/index.php/Simple_Linear_Regression_Analysis\n",
    "\n",
    "\n",
    "We can trust the test only if our assumptions are true\n",
    "\n",
    "- Relation between X and Y is linear\n",
    "- Errors/noise are *iid* \n",
    "- Errors/noise follows a multivariate normal with covariance $I\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What if the assumptions are incorrect\n",
    "\n",
    "We have to prove them right: **Residual analysis**\n",
    "\n",
    "1. Check the residuals for normality, Are there outliers that we should remove?\n",
    "1. Check for absence of correlation in the residuals\n",
    "1. Do the errors have different variance ?\n",
    "\n",
    "\n",
    "\n",
    "For the latter we can use the  **Weighted Least Squares** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Weighted Least Squares (WLS)\n",
    "***\n",
    "\n",
    "- Before we assumed that the noise was homoscedastic (constant variance). We will generalize to the heteroscedastic case\n",
    "- Assuming that we have $\\{x_i, y_i\\}_{i=1,\\ldots,N}$ *iid* observations from unidimensional  Y and **D-dimensional variable** X\n",
    "- How do we find $\\theta$  such that $y_i \\approx \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}, \\forall i$?\n",
    "- Let's assume that our measurements of $Y$ consists of the **true model** plus **Gaussian noise** with changing variance, *i.e.*\n",
    "\n",
    "$$\n",
    "    y_i = \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij} + \\varepsilon_i, \\forall i \\quad \\text{and} \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)\n",
    "$$\n",
    "\n",
    "- In this case the maximum likelihood solution is \n",
    "$$\n",
    "\\hat \\theta = (X^T \\Sigma^{-1}X)^{-1} X^T \\Sigma^{-1} Y\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix} \n",
    "\\sigma_1^2 & 0 &\\ldots & 0 \\\\\n",
    "0 & \\sigma_2^2 &\\ldots & 0 \\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots \\\\\n",
    "0 & 0 &\\ldots & \\sigma_N^2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- Note that in this case\n",
    "$$\n",
    "\\hat \\theta \\sim \\mathcal{N}( \\theta,  (X^T X)^{-1} X^T  \\Sigma X (X^T X)^{-1} )\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear models and Basis functions\n",
    "\n",
    "- Can we estimate a model that is not an hyperplane? YES!\n",
    "\n",
    "- The most general form of a linear model is\n",
    "$$\n",
    "g_\\theta(x_i) = \\sum_{j=0}^M \\theta_j \\phi_j(x_i),\n",
    "$$\n",
    "where $\\phi_j: \\mathbb{R}^D \\to \\mathbb{R}$ is a set of basis functions\n",
    "- Note that M and D are not related as before\n",
    "\n",
    "### Example: Polynomials\n",
    "\n",
    "- Unidimensional variable $x_i \\in \\mathbb{R}, i=1,\\ldots,N$\n",
    "- Basis function is \n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_0 (x) &= 1 \\nonumber \\\\\n",
    "\\phi_1 (x) &= x \\nonumber \\\\\n",
    "\\phi_2 (x) &= x^2 \\nonumber \\\\\n",
    "& \\vdots \\nonumber \\\\\n",
    "\\phi_M (x) &= x^M \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- This yields a M-degree polynomial model\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\ldots + \\theta_M x_i^M, \\forall i\n",
    "$$\n",
    "\n",
    "- It is linear in the parameters but non-linear in the input\n",
    "\n",
    "### Example: Transformations\n",
    "\n",
    "- Unidimensional variable $x_i \\in \\mathbb{R}, i=1,\\ldots,N$\n",
    "- Basis function is \n",
    "\n",
    "$$\n",
    "\\phi_j(x_i) = \\cos(2\\pi j x_i/P)\n",
    "$$\n",
    "\n",
    "- This yields\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 \\cos(2\\pi x_{i}/P) + \\theta_2 \\cos(4\\pi x_{i}/P) + \\ldots + \\theta_M \\cos(2M\\pi x_{i}/P), \\forall i\n",
    "$$\n",
    "\n",
    "- A periodic model that is linear on $\\theta$\n",
    "\n",
    "### Example: Interactions\n",
    "\n",
    "- Bidimensional variable $x_i = (x_{i1}, x_{i2}), i=1,\\ldots,N$\n",
    "- Basis function is \n",
    "\n",
    "$$\n",
    "\\phi_j(x_i) = \n",
    "\\begin{cases} \n",
    "1 & j=0 \\\\ x_{i1} & j=1 \\\\ \n",
    "x_{i2} & j=2 \\\\ x_{i1} x_{i2} & j=3 \\\\ \n",
    "x_{i1}^2 & j=4 \\\\ x_{i2}^2 & j=5 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "- A basis with interactions between variables up to second-degree\n",
    "\n",
    "### Example: The simplest linear model\n",
    "\n",
    "- D-dimensional variable $x_i = (x_{i1}, x_{i2}, \\ldots, x_{iD}), i=1,\\ldots,N$\n",
    "- Basis function is \n",
    "\n",
    "\n",
    "$$\n",
    "\\phi_j(x_i) = \\begin{cases} 1 & j=0 \\\\ x_{ij} & j \\in [1,D] \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "- This yields\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\ldots + \\theta_M x_{iD}, \\forall i\n",
    "$$\n",
    "\n",
    "- Which is the simplest linear model we reviewed before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "## Practical example: Polynomial regression\n",
    "\n",
    "- Play with $M$ the polynomials degree and $\\sigma$ the variance of the noise\n",
    "- What happens when $M > N$? Does it depend on $\\sigma$?\n",
    "- What happens to the prediction on the test data point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gen_basis(x, M=1):\n",
    "    X2 = list()\n",
    "    for i in range(M):\n",
    "        X2.append(x**i)\n",
    "    return np.vstack(X2).T\n",
    "\n",
    "plt.close('all'); fig = plt.figure(figsize=(10, 7))\n",
    "x = np.linspace(-5, 6, num=11); \n",
    "x2 = np.linspace(-5, 6, num=100);\n",
    "theta = [10, -2, -0.3, 0.1]\n",
    "X = gen_basis(x, M=len(theta))\n",
    "print(repr(theta))\n",
    "\n",
    "def update(sigma, rseed, M):\n",
    "    np.random.seed(rseed); Y = np.dot(X, theta) + sigma*np.random.randn(len(x))\n",
    "    X2 = gen_basis(x, M)\n",
    "    theta_hat = np.linalg.solve(np.dot(X2[:10, :].T, X2[:10, :]), np.dot(X2[:10, :].T, Y[:10]))\n",
    "    print(repr(theta_hat))    \n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax.scatter(x[:10], Y[:10], c='r', s=100, label='train data')\n",
    "    ax.scatter(x[10:], Y[10:], c='g', s=100, label='test data')\n",
    "    ax.vlines(x[:10], np.dot(X2[:10, :], theta_hat), Y[:10], 'r')  \n",
    "    ax.vlines(x[10:], np.dot(X2[10:, :], theta_hat), Y[10:], 'g') \n",
    "    ax.plot(x, np.dot(X, theta), 'b--', linewidth=4, label='underlying')\n",
    "    ax.plot(x2, np.dot(gen_basis(x2, M), theta_hat), 'k-', linewidth=4, label='model')\n",
    "    ax.set_ylim([-5, 15]); plt.legend()\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0))\n",
    "    ax.plot(x, np.zeros_like(x), 'k--', alpha=0.5)\n",
    "    ax.scatter(x, Y - np.dot(X2, theta_hat), c='k', s=100); \n",
    "    \n",
    "interact(update, rseed=IntSlider_nice(), M=SelectionSlider_nice(options=[1, 2, 3, 4, 5, 7, 10]), \n",
    "         sigma=SelectionSlider_nice(options=[0.1, 1, 2, 5]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "<a id=\"section4\"></a>\n",
    "\n",
    "# Overfitting and regularization\n",
    "***\n",
    "\n",
    "- In the previous example $M$ represents the complexity of the model\n",
    "- In general more complex models give more flexibility to fit the data\n",
    "- But too much complexity causes **overfitting**\n",
    "    - the model fits the noise\n",
    "    - we can't extract the underlying behavior \n",
    "    - the model **does not generalize** to new data\n",
    "- Ways to avoid overfitting\n",
    "    - Using low complexity models\n",
    "    - Set complexity using cross-validation \n",
    "    - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## The Bias-Variance trade-off\n",
    "\n",
    "- Let's assume that the data is $y = f(x) + \\varepsilon$ (true model + Gaussian noise)\n",
    "- We use a linear model to find $f(x)$ as $\\hat y = \\sum_j \\theta_j \\phi_j(x)$\n",
    "- We can measure the quality of our model with the Mean Square Error (MSE)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[(y - \\hat y)^2] &= \\mathbb{E}[y^2 -2 y \\hat y +\\hat y^2] \\nonumber \\\\\n",
    "&= \\mathbb{E}[(f+\\varepsilon)^2 -2 (f+\\varepsilon) \\hat y +\\hat y^2] \\nonumber \\\\\n",
    "&= \\mathbb{E}[(f^2 +2 f \\varepsilon + \\varepsilon^2 -2 (f+\\varepsilon) \\hat y +\\hat y^2] \\nonumber \\\\\n",
    "&= \\mathbb{E}[\\varepsilon^2] + f^2  -2 f \\mathbb{E}[\\hat y]  +\\mathbb{E}[\\hat y^2]  \\pm \\mathbb{E}[\\hat y]^2  \\nonumber \\\\\n",
    "&= \\mathbb{E}[\\varepsilon^2] + (f - \\mathbb{E}[\\hat y])^2  +\\mathbb{E}[(\\hat y - \\mathbb{E}[\\hat y])^2]  \\nonumber \\\\\n",
    "&= \\sigma^2 + (f - \\mathbb{E}[\\hat y])^2  + \\text{Var}[\\hat y]  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "- The MSE can be decomposed as irreducible error + squared bias of the estimator + variance of the estimator\n",
    "- The MSE can be small of either the bias or the variance are small\n",
    "- Typically, the more complex the model the lower the bias and the higher the variance it attains \n",
    "- The Gauss-Markov theorem says that OLS has the minimum variance of the unbiased estimator. \n",
    "- But unbiased is not necessarily good (overfit). In some cases we may want to trade bias for variance\n",
    "- This is achieved by penalizing the complexity of the model: **Regularization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "\n",
    "# Bayesian Least Squares and Ridge regression\n",
    "\n",
    "\n",
    "If we assume a Gaussian likelihood and a Gaussian prior we can write the log joint as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p({x}, \\theta) &= \\log \\prod_{i=1}^N \\mathcal{N}(y_i | (1, x_{i1}, \\ldots, x_{iD}) \\theta, \\sigma^2) + \\log \\prod_{j=1}^M \\mathcal{N}(\\theta_j | 0, \\sigma_0^2) \\nonumber \\\\\n",
    "&= -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) -\\frac{M}{2} \\log(2\\pi \\sigma_0^2) - \\frac{1}{2\\sigma_0^2} \\|\\theta\\|^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The MAP estimator of $\\theta$ is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p({x}, \\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\max_\\theta  - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) - \\frac{1}{2\\sigma_0^2} \\|\\theta\\|^2 \\nonumber \\\\\n",
    "&= \\text{arg}\\min_\\theta  (Y-X\\theta)^T (Y - X\\theta) + \\lambda \\|\\theta\\|^2 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\lambda = \\frac{\\sigma^2}{\\sigma_0^2}$\n",
    "\n",
    "The solution is obtained by taking the derivative on $\\theta$\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} (Y-X\\theta)^T (Y - X\\theta) + \\lambda \\|\\theta\\|^2  = -X^T (Y - X\\theta) + \\lambda \\theta = 0\n",
    "$$\n",
    "\n",
    "and finally\n",
    "\n",
    "$$\n",
    "\\hat \\theta = (X^T X + \\lambda I)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "- This is known as Ridge regression and Tikhonov regularization\n",
    "- We are forcing the solution to be smooth (have small L2 norm)\n",
    "- This can help to avoid overfitting with complex models \n",
    "- It can also help when $X^T X$ is not invertible\n",
    "    - Adding a $\\lambda$ to the diagonal of $X^T X$ makes the eigenvalues positive (unique solution)\n",
    "- (There is no free lunch) How to choose $\\lambda$?\n",
    "    - Cross-validation: minimize validation error\n",
    "    - L-curve: Plot $ \\log (Y-X\\theta)^T (Y - X\\theta)$ vs $ \\log \\|\\theta\\|^2$ and find the elbow\n",
    "- Note that different priors yield different regularization schemes\n",
    "    - A Laplacian prior yields a L1 norm which forces the solution to be sparse (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = np.linspace(-5, 6, num=50); x_plot = np.linspace(-5, 6, num=200); \n",
    "model = np.sin(x)*x + 0.1*x**2\n",
    "print(repr(theta))\n",
    "\n",
    "def update(sigma, rseed, lamb, M):\n",
    "    np.random.seed(rseed); \n",
    "    y = model + sigma*np.random.randn(len(x))\n",
    "    regressor = make_pipeline(PolynomialFeatures(M), Ridge(normalize=True, alpha=lamb))\n",
    "    regressor.fit(x.reshape(-1, 1), y)\n",
    "    ax.cla(); ax.plot(x, model, 'b--', linewidth=4, label='underlying')\n",
    "    ax.plot( x_plot , regressor.predict( x_plot .reshape(-1, 1)), 'k-', linewidth=4, label='model')\n",
    "    ax.scatter(x, y, c='r', s=30, label='data', zorder=100); plt.legend()\n",
    "\n",
    "    \n",
    "interact(update, rseed=IntSlider_nice(), M=SelectionSlider_nice(options=[1, 2, 3, 5, 10, 20]), \n",
    "         sigma=SelectionSlider_nice(options=[0.1, 1, 2, 5]),\n",
    "         lamb=SelectionSlider_nice(options=[0.0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1., 100000.]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[&larr; Go back to the index](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "***\n",
    "<a id=\"section5\"></a>\n",
    "\n",
    "# Extra topics\n",
    "***\n",
    "\n",
    "Suggestions if you want to go deeper \n",
    "- (Hastie 3.4 and 3.8) L1 regularization and Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "- Robust regression: Least absolute regression and M-estimators for data with outliers (non-Gaussian)\n",
    "- (Hastie 6 & Bishop 6) Kernel (non-parametric) regression  \n",
    "- Some of these topics can be found at [Huijse, Regresión](https://docs.google.com/presentation/d/1UUpK4zSdzRcS79V7_wU9nXe-sR7qYLEWhbmid-Rfp1k/edit#slide=id.g28044c0f85_0_34)\n",
    "\n",
    "\n",
    "# Independence and correlation\n",
    "\n",
    "Two variables can have zero correlation but still be dependent\n",
    "\n",
    "> Linear regression (correlation) is sensitive to linear relationships\n",
    "\n",
    "More generally, to test indepedence we could use:\n",
    "\n",
    "$$\n",
    "p(x,y) = p(x)p(y)\n",
    "$$\n",
    "\n",
    "Several methods are based on this\n",
    "- Shannon's **Mutual Information**\n",
    "$$\n",
    "I(X,Y) = \\int \\int f_{XY}(x,y) \\log \\frac{f_{XY}(x,y)}{f_{X}(x) f_Y(y)} dx dy\n",
    "$$\n",
    "- [Correlation distance](https://arxiv.org/pdf/0803.4101.pdf)\n",
    "$$\n",
    "R(X,Y) = \\int \\int |f_{XY}(x,y)  - f_{X}(x) f_Y(y)| dx dy\n",
    "$$\n",
    "\n",
    "Although these require that we estimate the joint and the marginals (KDE, Histogram, Parametric)\n",
    "\n",
    "For categorical variables we can use the **chi square test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
