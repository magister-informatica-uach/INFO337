
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8. Markov Chain Monte Carlo &#8212; Herramientas estadísticas para la investigación</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="7. Gaussian Mixture Models" href="../9_mixture_models/lecture.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Herramientas estadísticas para la investigación</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Statistical modeling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2_statistical_modeling/part1.html">
   1. Parametric modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2_statistical_modeling/part2.html">
   2. Nonparametric modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2_statistical_modeling/part3.html">
   3. Bayesian modeling
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical thinking
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4_nonparametric_inference/lecture.html">
   4. Inferencia estadística con tests no-paramétricos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5_linear_regression/lecture.html">
   5. Linear regression
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exploratory analysis
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../9_mixture_models/lecture.html">
   7. Gaussian Mixture Models
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian modeling
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   8. Markov Chain Monte Carlo
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/lectures/11_MCMC/lecture.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/magister-informatica-uach/INFO337"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/magister-informatica-uach/INFO337/master?urlpath=tree/lectures/11_MCMC/lecture.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recap-the-bayesian-setting">
   8.1. Recap: The Bayesian setting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-bayesian-landscape">
     8.1.1. The Bayesian landscape
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monte-carlo-methods">
   8.2. Monte Carlo methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markov-chain">
   8.3. Markov Chain
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markov-chain-monte-carlo-mcmc">
   8.4. Markov Chain Monte Carlo (MCMC)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metropolis-hastings-mh">
     8.4.1. Metropolis Hastings (MH)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hamiltonian-monte-carlo">
     8.4.2. Hamiltonian Monte-Carlo
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistic-programming">
   8.5. Probabilistic programming
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-pymc3-tutorial">
   8.6. A PyMC3 tutorial
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-specification">
     8.6.1. Model specification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#creating-random-variables">
       8.6.1.1. Creating random variables
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bounded-distributions">
       8.6.1.2. Bounded distributions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-distributions">
       8.6.1.3. Other distributions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#specifying-the-likelihood">
       8.6.1.4. Specifying the likelihood
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-fitting">
     8.6.2. Model fitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mcmc-sampling">
     8.6.3. MCMC sampling
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#check-posteriors-with-traceplots">
       8.6.3.1. Check posteriors with traceplots
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#use-diagnostics-to-check-the-convergence-of-the-chains">
       8.6.3.2. Use diagnostics to check the convergence of the chains
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#draw-posterior-predictive-distribution">
       8.6.3.3. Draw posterior predictive distribution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#second-example-mixture-of-gaussians">
   8.7. Second example: Mixture of Gaussians
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-predictive-checks-ppc">
     8.7.1. Posterior predictive checks (PPC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#useful-tips-for-mcmc">
     8.7.2. Useful tips for MCMC
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#last-example-multilabel-logistic-softmax-regression">
       8.7.2.1. Last example: Multilabel logistic (softmax) regression
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evolution-of-the-cost-function-elbo">
       8.7.2.2. Evolution of the cost function (ELBO)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#accuracy-on-train-and-test">
       8.7.2.3. Accuracy on train and test
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#compute-100-samples-for-the-test-set-predictions">
       8.7.2.4. Compute 100 samples for the test set predictions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#check-predictions-and-uncertainty">
       8.7.2.5. Check predictions and uncertainty
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#approximate-posteriors-for-the-parameters">
       8.7.2.6. Approximate posteriors for the parameters
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> notebook
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">FloatSlider</span><span class="p">,</span> <span class="n">SelectionSlider</span><span class="p">,</span> <span class="n">IntSlider</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running PyMC3 v</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="markov-chain-monte-carlo">
<h1><span class="section-number">8. </span>Markov Chain Monte Carlo<a class="headerlink" href="#markov-chain-monte-carlo" title="Permalink to this headline">¶</a></h1>
<p>References for today’s lecture:</p>
<ol class="simple">
<li><p>Davidson-Pilon, “<a class="reference external" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">Bayesian methods for hackers</a>”, <em>Addison Wesley</em>, 2016, <strong>Chapter 2 and 3</strong></p></li>
</ol>
<div class="section" id="recap-the-bayesian-setting">
<h2><span class="section-number">8.1. </span>Recap: The Bayesian setting<a class="headerlink" href="#recap-the-bayesian-setting" title="Permalink to this headline">¶</a></h2>
<p>Reminder from previous classes:</p>
<ul class="simple">
<li><p><strong>Probability:</strong> Represents how believable an event is</p>
<ul>
<li><p>How confident we are that the event occurs</p></li>
</ul>
</li>
<li><p>Our belief of a certain event <span class="math notranslate nohighlight">\(A\)</span> is the <strong>prior probability</strong> <span class="math notranslate nohighlight">\(p(A)\)</span></p></li>
<li><p>We collect evidence <span class="math notranslate nohighlight">\(X\)</span> to <strong>update</strong> our belief on <span class="math notranslate nohighlight">\(A\)</span> forming a <strong>posterior</strong> <span class="math notranslate nohighlight">\(p(A|X)\)</span></p></li>
<li><p>How? Bayes Theorem</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(A|X) = \frac{p(X|A)p(A)}{p(X)} \propto p(X|A)p(A)
\]</div>
<ul class="simple">
<li><p>In general the larger the amount of evidence the less influence the prior has</p></li>
</ul>
<p>Model/parameter estimation:</p>
<ul class="simple">
<li><p>Maximum likelihood (MLE): Point estimate</p></li>
<li><p>Maximum a posterior (MAP): Point estimate plus prior</p></li>
<li><p>Bayes: Full posterior distribution</p></li>
</ul>
<p>One more time: Bayes Theorem + law of total probability:
$<span class="math notranslate nohighlight">\(
p(A|X) = \frac{p(X|A)p(A)}{p(X)} = \frac{p(X|A)p(A)}{\int p(X, A) dA} = \frac{p(X|A)p(A)}{\int p(X|A)p(A) dA}
\)</span>$</p>
<ul class="simple">
<li><p>We propose the <strong>prior</strong> and a <strong>likelihood</strong>: Our assumptions on the data and parameter distributions</p></li>
<li><p>The <strong>evidence</strong> is … usually intractable</p>
<ul>
<li><p>We are integrating on all the possible values of the parameters, remember GMM?</p></li>
</ul>
</li>
<li><p>Are we done? What are our options?</p>
<ul>
<li><p>Use priors/likelihoods so that posterior is analytical (conjugate)</p></li>
<li><p>Approximate inference (Variational Bayes)</p></li>
<li><p><strong>Today:</strong> Monte-Carlo Markov Chain (MCMC)</p></li>
</ul>
</li>
</ul>
<div class="section" id="the-bayesian-landscape">
<h3><span class="section-number">8.1.1. </span><a class="reference external" href="http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb#The-Bayesian-landscape">The Bayesian landscape</a><a class="headerlink" href="#the-bayesian-landscape" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In the following example we draw two-dimensional Poisson distributed data</p></li>
<li><p>We consider exponential priors for the rate <span class="math notranslate nohighlight">\(\lambda\)</span></p></li>
<li><p>This is essentially a two dimensional surface in parameter space</p></li>
<li><p>The prior sets the initial shape</p></li>
<li><p>The observations warp the surface</p></li>
<li><p>To find the best parameters we need to explore this possibly high-dim space</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">update_plot</span><span class="p">(</span><span class="n">rseed</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">rseed</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cla</span><span class="p">();</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cla</span><span class="p">();</span>
    <span class="n">lambda_1_true</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">lambda_2_true</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">lambda_1_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                           <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">lambda_2_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                          <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">.01</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">likelihood_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">_x</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">_x</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">likelihood_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">_y</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">_y</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">likelihood_x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">likelihood_y</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span>


    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">exp_y</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">exp_x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">exp_y</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lambda_2_true</span><span class="p">,</span> <span class="n">lambda_1_true</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior Landscape&quot;</span><span class="p">)</span>

    <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">L</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">jet</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lambda_2_true</span><span class="p">,</span> <span class="n">lambda_1_true</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Landscape warped </span><span class="se">\n</span><span class="s2">by </span><span class="si">%d</span><span class="s2"> data observation.&quot;</span> <span class="o">%</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span>

<span class="n">interact</span><span class="p">(</span><span class="n">update_plot</span><span class="p">,</span> 
         <span class="n">N</span><span class="o">=</span><span class="n">SelectionSlider</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]),</span>
         <span class="n">rseed</span><span class="o">=</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">100</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="monte-carlo-methods">
<h2><span class="section-number">8.2. </span>Monte Carlo methods<a class="headerlink" href="#monte-carlo-methods" title="Permalink to this headline">¶</a></h2>
<p>Monte carlo methods obtain numerical results via repeated random sampling</p>
<p>One of its most important case uses is: Monte-carlo integration</p>
<p>Let’s say we want to the expected value of a function <span class="math notranslate nohighlight">\(g\)</span> on a random variable <span class="math notranslate nohighlight">\(X \sim f\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[g(X)] = \int g(x) f(x) \,dx
\]</div>
<p>The Monte Carlo estimator of this integral is</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[g(X)] \approx \hat g_N = \frac{1}{N} \sum_{i=1}^N g(x_i) \quad x_i \sim f(x)
\]</div>
<p>and due to the CLT we have</p>
<div class="math notranslate nohighlight">
\[
\hat g_N \sim \mathcal{N} \left( \mathbb{E}[g(X)], \sigma_N^2/N \right)
\]</div>
<p>We could use this to estimate very hard integrals (posteriors)</p>
<p><strong>Example:</strong> Estimating the value of PI using Monte Carlo</p>
<p>We throw random “balls” into the unitary square and we count how many fall inside the circle</p>
<p>If we divide the area of the circle and the square we get <span class="math notranslate nohighlight">\(\pi\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">&lt;=</span> <span class="mf">0.</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">);</span> 
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">rseed</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">rseed</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">cla</span><span class="p">();</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Reds</span><span class="p">);</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">xr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">N_inside</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">xr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]))[</span><span class="mi">0</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%0.4f</span><span class="s2">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="mf">4.</span><span class="o">*</span><span class="n">N_inside</span><span class="o">/</span><span class="n">N</span><span class="p">))</span>

<span class="n">interact</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> 
         <span class="n">N</span><span class="o">=</span><span class="n">SelectionSlider</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">]),</span> 
         <span class="n">rseed</span><span class="o">=</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">100</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1_000</span><span class="p">,</span> <span class="mi">10_000</span><span class="p">,</span> <span class="mi">100_000</span><span class="p">,</span> <span class="mi">1_000_000</span><span class="p">])</span>
<span class="n">N_inside</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">N_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">xr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N_</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">N_inside</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">xr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]))[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">N_inside</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="markov-chain">
<h2><span class="section-number">8.3. </span>Markov Chain<a class="headerlink" href="#markov-chain" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Consider a system, i.e. a set of states <span class="math notranslate nohighlight">\(X\)</span> that evolve in time (steps)</p></li>
<li><p>A system can be modeled with a <strong>Markov Chain</strong>: stochastic process for sequences</p></li>
<li><p>Given that it fits the <strong>Markov property</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(X_n|X_{n-1},\ldots, X_0) = p(X_n|X_{n-1})
\]</div>
<p><em>i.e</em> the probability of the future state is conditionally independant of the past given the present</p>
<p>Markov chains that are irreducible will converge in the long term to a stationary distribution</p>
</div>
<div class="section" id="markov-chain-monte-carlo-mcmc">
<h2><span class="section-number">8.4. </span>Markov Chain Monte Carlo (MCMC)<a class="headerlink" href="#markov-chain-monte-carlo-mcmc" title="Permalink to this headline">¶</a></h2>
<p>The problem with MC integration is that sometimes we can’t draw from distribution <span class="math notranslate nohighlight">\(f\)</span>. Also randomly sampling in all the space is very inefficient</p>
<p>Instead we could use Markov Chain Monte Carlo, a family of algorithms that learn the transition probabilities of a markov chain so that it converges to a desired distribution</p>
<p>In our case the desired distribution is the bayesian posterior</p>
<a class="reference internal image-reference" href="../../_images/is_mcmc.png"><img alt="../../_images/is_mcmc.png" src="../../_images/is_mcmc.png" style="width: 500px;" /></a>
<p>MCMC searches the space in a less naive way. A sequence of samples is called a <strong>trace</strong></p>
<blockquote>
<div><p>Random walk with a generated <strong>step</strong> ¿How to select the step?</p>
</div></blockquote>
<p>There are several proposal algoritms two of the most popular ones are:</p>
<div class="section" id="metropolis-hastings-mh">
<h3><span class="section-number">8.4.1. </span>Metropolis Hastings (MH)<a class="headerlink" href="#metropolis-hastings-mh" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Random walker that moves on all dimensions simultaneously</p></li>
<li><p>A candidate step is drawn from an arbitrary but symmetric distribution <span class="math notranslate nohighlight">\(x^{new} \sim g(x^{new}|x_t)\)</span></p></li>
<li><p>We accept the step if <span class="math notranslate nohighlight">\(f(x^{new})/f(x^t)\)</span> is equal or larger than a certain threshold</p></li>
<li><p><span class="math notranslate nohighlight">\(f(\cdot)\)</span> needs only to be proportional to the target distribution (evidence is canceled in the ratio)</p></li>
<li><p>Repeat many times until convergence</p></li>
</ul>
</div>
<div class="section" id="hamiltonian-monte-carlo">
<h3><span class="section-number">8.4.2. </span>Hamiltonian Monte-Carlo<a class="headerlink" href="#hamiltonian-monte-carlo" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Family of step proposing methods that use momentum (derivatives)</p></li>
<li><p>Only for continuous variables</p></li>
<li><p>Cost more than MH (single iteration) but require less iterations</p></li>
<li><p><a class="reference external" href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html</a></p></li>
</ul>
</div>
</div>
<div class="section" id="probabilistic-programming">
<h2><span class="section-number">8.5. </span>Probabilistic programming<a class="headerlink" href="#probabilistic-programming" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>PP: Doing statistics (Bayesian inference) using the tools of computer science</p></li>
<li><p>PP languages: unify general purpose programming with probabilistic modeling</p></li>
<li><p>Python friendly PP frameworks/libraries:</p>
<ul>
<li><p><a class="reference external" href="https://docs.pymc.io/notebooks/getting_started.html">PyMC3</a>: Black-box VI, MH, Gibbs, NUTS sampler. Uses theano</p></li>
<li><p><a class="reference external" href="https://pystan.readthedocs.io/en/latest/">PyStan</a>: Python interface for <a class="reference external" href="http://mc-stan.org/">Stan platform</a></p></li>
<li><p><a class="reference external" href="http://edwardlib.org/">Edward</a>: Black-box VI, neural networks. Uses tensorflow</p></li>
<li><p><a class="reference external" href="http://pyro.ai">Pyro</a>: Black-box VI, neural networks, MCMC. Uses pytorch</p></li>
<li><p><a class="reference external" href="http://dfm.io/emcee/current/">emcee</a>: Pure python implementation of the Affine invariant MCMC ensemble sampler</p></li>
<li><p><a class="reference external" href="http://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/">http://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/</a></p></li>
</ul>
</li>
</ul>
<p><a href="https://arxiv.org/abs/1809.10756"><img alt="../../_images/PP.png" src="../../_images/PP.png" /></a></p>
<p>PP runs in two directions!</p>
</div>
<div class="section" id="a-pymc3-tutorial">
<h2><span class="section-number">8.6. </span>A PyMC3 tutorial<a class="headerlink" href="#a-pymc3-tutorial" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial we will review how to do</p>
<ol class="simple">
<li><p>Model definition</p></li>
<li><p>Fitting</p></li>
<li><p>Convergence checks</p></li>
<li><p>Posterior analysis</p></li>
</ol>
<p>with PyMC3</p>
<p>We start with an example of Bayesian linear regression (class 2)</p>
<ul class="simple">
<li><p>Gaussian noise with variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></p></li>
<li><p>One independent variables: <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>Three parameters <span class="math notranslate nohighlight">\(\beta\)</span>: intercept plus one coefficient per covariate</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Y = \beta X + \epsilon 
\]</div>
<div class="math notranslate nohighlight">
\[
\epsilon \sim \mathcal{N}(0, \sigma^2)
\]</div>
<div class="math notranslate nohighlight">
\[
Y \sim \mathcal{N}(\mu, \sigma^2)
\]</div>
<div class="math notranslate nohighlight">
\[
\mu = \beta_0 + \beta_1 X
\]</div>
<p>Credit: <a class="reference external" href="https://docs.pymc.io/notebooks/getting_started.html">https://docs.pymc.io/notebooks/getting_started.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">beta_true</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">30</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="n">sigma_true</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span> 
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-specification">
<h3><span class="section-number">8.6.1. </span>Model specification<a class="headerlink" href="#model-specification" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">with</span></code> keyword we create a context from <code class="docutils literal notranslate"><span class="pre">pm.Model()</span></code></p></li>
<li><p>Within this context we will set priors, likelihood, etc</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">my_model</span><span class="p">:</span>
    <span class="c1"># Priors</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="creating-random-variables">
<h4><span class="section-number">8.6.1.1. </span>Creating random variables<a class="headerlink" href="#creating-random-variables" title="Permalink to this headline">¶</a></h4>
<p>We have set two stochastic variables with normal and half normal distributions, respectively</p>
<blockquote>
<div><p>These are the priors for <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span></p>
</div></blockquote>
<p>For <span class="math notranslate nohighlight">\(\beta\)</span> we used a normal prior. The constructor for normal is</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>pm.Normal(name=&#39;beta&#39;, mu=0, sd=10, shape=3, testval=None)
</pre></div>
</div>
<p>where</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code> (string): Unique identifier, in this case ‘beta’</p></li>
<li><p>mu and sd (floats): Mean and standard deviation of the normal distribution in this case 0 and 10, respectively</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shape</span></code>: Specifies the dimensionality, in this case we create 3 univariate normals</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">testval</span></code>: Gives initial values</p></li>
</ol>
<p>For <span class="math notranslate nohighlight">\(\sigma\)</span> we use a half-normal prior</p>
<p>This variable is non-negative so we have to use a non-negative prior (<em>e.g.</em> Gamma) or a bounded prior</p>
</div>
<div class="section" id="bounded-distributions">
<h4><span class="section-number">8.6.1.2. </span>Bounded distributions<a class="headerlink" href="#bounded-distributions" title="Permalink to this headline">¶</a></h4>
<p>One can create arbitrary bounded distributions with</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x = pm.Bound(pm.Normal, lower=0.0)(&#39;x&#39;, mu=1.0, sd=3.0)
</pre></div>
</div>
</div>
<div class="section" id="other-distributions">
<h4><span class="section-number">8.6.1.3. </span>Other distributions<a class="headerlink" href="#other-distributions" title="Permalink to this headline">¶</a></h4>
<p>Check the list available distributions <a class="reference external" href="https://docs.pymc.io/api/distributions.html">here</a></p>
</div>
<div class="section" id="specifying-the-likelihood">
<h4><span class="section-number">8.6.1.4. </span>Specifying the likelihood<a class="headerlink" href="#specifying-the-likelihood" title="Permalink to this headline">¶</a></h4>
<p>Now we can specify the likelihood of the model</p>
<p>First, to continue working in the previously defined context we use</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>with my_model:
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">my_model</span><span class="p">:</span>
    <span class="n">X_shared</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="c1"># Likelihood</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X_shared</span><span class="p">)</span>
    <span class="n">Y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Y_obs&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The likelihood is a random variable with the keyword <code class="docutils literal notranslate"><span class="pre">observed</span></code> defined</p>
<blockquote>
<div><p>In <code class="docutils literal notranslate"><span class="pre">observed</span></code> we pass the data. It can be a numpy ndarray or a pandas data frame</p>
</div></blockquote>
<p>By giving <code class="docutils literal notranslate"><span class="pre">beta</span></code> and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> as parameter for <code class="docutils literal notranslate"><span class="pre">Y_obs</span></code> we automatically create a parent-child relation</p>
<p>The following attributes have been created</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">free_RVs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">deterministics</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">my_model</span><span class="o">.</span><span class="n">observed_RVs</span><span class="p">)</span>
<span class="n">my_model</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="model-fitting">
<h3><span class="section-number">8.6.2. </span>Model fitting<a class="headerlink" href="#model-fitting" title="Permalink to this headline">¶</a></h3>
<p>In PyMC3 we can do MAP to get point estimates, VI to do approximate inference and MCMC for posterior sampling</p>
<ul class="simple">
<li><p>MAP and VI follow an optimization approach.</p></li>
<li><p>They are generally faster than MCMC but return less information</p>
<ul>
<li><p>point estimate with no uncertainty</p></li>
<li><p>approximate factorized distribution (only continuous variables)</p></li>
</ul>
</li>
<li><p>MAP and VI can be used to find reasonable initial states for MCMC</p></li>
<li><p>For very complex model and large number of observations we may not be able to do MCMC at all</p></li>
</ul>
<p><strong>MAP estimate</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">my_model</span><span class="p">:</span>
    <span class="n">map_estimate</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">find_MAP</span><span class="p">(</span><span class="n">progressbar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">map_estimate</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="mcmc-sampling">
<h3><span class="section-number">8.6.3. </span>MCMC sampling<a class="headerlink" href="#mcmc-sampling" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>To do MCMC first you have to specify a step method (MH, Gibbs, NUTS, etc)</p></li>
<li><p>PyMC3 have very good default options</p>
<ul>
<li><p>No-U-Turn sampler is the default option for continuous parameters</p></li>
<li><p>MH is the default for discrete parameters</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">pm.sample</span></code> is the main MCMC interface</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> 
       <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
       <span class="n">init</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> 
       <span class="n">n_init</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span> 
       <span class="n">start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
       <span class="n">trace</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
       <span class="n">chains</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
       <span class="n">cores</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
       <span class="n">tune</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> 
       <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">my_model</span><span class="p">:</span>
    <span class="c1"># draw 500 posterior samples</span>
    <span class="c1"># trace = pm.sample(draws=2000, start=map_estimate) # Start from MAP</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;advi&#39;</span><span class="p">)</span> <span class="c1"># Start from VI</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="check-posteriors-with-traceplots">
<h4><span class="section-number">8.6.3.1. </span>Check posteriors with traceplots<a class="headerlink" href="#check-posteriors-with-traceplots" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">],</span> <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textsize</span><span class="o">=</span><span class="mi">12</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Transform trace to dataframe</span>
<span class="n">df_trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">trace_to_dataframe</span><span class="p">(</span><span class="n">trace</span><span class="p">)[[</span><span class="s1">&#39;beta__0&#39;</span><span class="p">,</span> <span class="s1">&#39;beta__1&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">]]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">plotting</span><span class="o">.</span><span class="n">scatter_matrix</span><span class="p">(</span><span class="n">df_trace</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="use-diagnostics-to-check-the-convergence-of-the-chains">
<h4><span class="section-number">8.6.3.2. </span>Use diagnostics to check the convergence of the chains<a class="headerlink" href="#use-diagnostics-to-check-the-convergence-of-the-chains" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The Gelman-Rubin diagnostic tests for lack of convergence</p>
<ul class="simple">
<li><p>Compares the variance between multiple chains to the variance within each chain.</p></li>
<li><p>Values greater than one indicate that one or more chains have not yet converged.</p></li>
</ul>
</div>
<div class="section" id="draw-posterior-predictive-distribution">
<h4><span class="section-number">8.6.3.3. </span>Draw posterior predictive distribution<a class="headerlink" href="#draw-posterior-predictive-distribution" title="Permalink to this headline">¶</a></h4>
<p>In practice, for a regressor, we want the posterior predictive distribution, i.e. the prediction <strong>y</strong> for a new sample <strong>x</strong> given the training dataset</p>
<div class="math notranslate nohighlight">
\[
p(\textbf{y}|\textbf{x}, \mathcal{D}) = \int p(\textbf{y}|\textbf{x},\theta) p(\theta| \mathcal{D}) \,d\theta 
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">with</span> <span class="n">my_model</span><span class="p">:</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_test</span><span class="p">})</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                                                          <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="s2">&quot;Y_obs&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span> 

<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">]:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span> 

<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">perc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">95</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">perc</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">perc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="second-example-mixture-of-gaussians">
<h2><span class="section-number">8.7. </span>Second example: Mixture of Gaussians<a class="headerlink" href="#second-example-mixture-of-gaussians" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>In this example we will try to infer the parameters of a mixture of two 1d Gaussians</p></li>
<li><p>Let’s cerate some data</p></li>
</ul>
<p>Ref: <a class="reference external" href="http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb">http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu_true</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">std_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">p_true</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">200</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">p_true</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">p_true</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu_true</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">std_true</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">)),</span>
                       <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu_true</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">std_true</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">))))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">x_plot</span> <span class="o">-</span> <span class="n">mu_true</span><span class="p">[</span><span class="n">k</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">std_true</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">std_true</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Model specification</strong></p>
<p>We create priors for the center, dispersion and weights of the Gaussians</p>
<p>Ref: <a class="reference external" href="https://docs.pymc.io/notebooks/gaussian_mixture_model.html">https://docs.pymc.io/notebooks/gaussian_mixture_model.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Prior on concentration</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]),</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Use this to avoid singularities (empty clusters)</span>
    <span class="n">p_min_potential</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Potential</span><span class="p">(</span><span class="s1">&#39;p_min_potential&#39;</span><span class="p">,</span> 
                                   <span class="n">T</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">.1</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># Prior on standard deviations</span>
    <span class="n">sds</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;sds&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Prior on means</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;centers&quot;</span><span class="p">,</span> 
                        <span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> 
                        <span class="n">sd</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span> 
                        <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Use this to avoid identifiability problems</span>
    <span class="n">order_means_potential</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Potential</span><span class="p">(</span><span class="s1">&#39;order_means_potential&#39;</span><span class="p">,</span>
                                         <span class="n">T</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">centers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">centers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    
    <span class="n">center_i</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;center_i&#39;</span><span class="p">,</span> <span class="n">centers</span><span class="p">[</span><span class="n">z</span><span class="p">])</span>
    <span class="n">sd_i</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;sd_i&#39;</span><span class="p">,</span> <span class="n">sds</span><span class="p">[</span><span class="n">z</span><span class="p">])</span>
    
    <span class="c1"># Likelihood</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">center_i</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd_i</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Deterministic variables allow us to track custom variables in the traces</p>
<p><code class="docutils literal notranslate"><span class="pre">pm.Deterministic('name',</span> <span class="pre">var)</span></code></p>
<p>Potentials allow us to add an arbitrary factor the the likelihood</p>
<p><code class="docutils literal notranslate"><span class="pre">pm.Potential('name',</span> <span class="pre">var)</span></code></p>
<p>In this case we add potentials to</p>
<ul class="simple">
<li><p>Penalize solutions with  empty clusters</p></li>
<li><p>Forcing that the “first” gaussian is always to the left (remember z flipping)</p></li>
</ul>
<p><strong>MCMC</strong></p>
<p>We can use different step functions for different parameters</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">step1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">(</span><span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="n">sds</span><span class="p">,</span> <span class="n">centers</span><span class="p">])</span>
    <span class="n">step2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">CategoricalGibbsMetropolis</span><span class="p">(</span><span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="n">z</span><span class="p">])</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="p">[</span><span class="n">step1</span><span class="p">,</span> <span class="n">step2</span><span class="p">],</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cores</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">step</span></code>: In this case we have specified the step functions for each variable</p></li>
<li><p>Burn-in period <code class="docutils literal notranslate"><span class="pre">tune</span></code>: N first steps of the chain are discarded from the trace to build posteriors from the converged zone</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">chains</span></code>: We can also specify the amount of chains and cores</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">varnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="s1">&#39;centers&#39;</span><span class="p">,</span> <span class="s1">&#39;sds&#39;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;centers&#39;</span><span class="p">,</span> <span class="s1">&#39;sds&#39;</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">autocorrplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">varnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;centers&#39;</span><span class="p">,</span> <span class="s1">&#39;sds&#39;</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>A chain that is exploring the space well will exhibit very high autocorrelation.</p></li>
<li><p>Low autocorrelation is a sufficient condition for converged MCMC</p></li>
</ul>
<div class="section" id="posterior-predictive-checks-ppc">
<h3><span class="section-number">8.7.1. </span>Posterior predictive checks (PPC)<a class="headerlink" href="#posterior-predictive-checks-ppc" title="Permalink to this headline">¶</a></h3>
<p>Another way to validate a model is to generate data from the posterior draws and compare with the original distribution</p>
<p>This is done with <code class="docutils literal notranslate"><span class="pre">pm.sample_posterior_predictive</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors.kde</span> <span class="kn">import</span> <span class="n">KernelDensity</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
<span class="n">mean_score</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">kde</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;obs&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">kde</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">x_plot</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">mean_score</span> <span class="o">+=</span> <span class="n">score</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">mean_score</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>    
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="useful-tips-for-mcmc">
<h3><span class="section-number">8.7.2. </span><a class="reference external" href="http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb#Useful-tips-for-MCMC">Useful tips for MCMC</a><a class="headerlink" href="#useful-tips-for-mcmc" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Intelligent starting values</p></li>
<li><p>Choose your priors well!</p></li>
</ul>
<div class="section" id="last-example-multilabel-logistic-softmax-regression">
<h4><span class="section-number">8.7.2.1. </span>Last example: Multilabel logistic (softmax) regression<a class="headerlink" href="#last-example-multilabel-logistic-softmax-regression" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>wget -nc http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span> 
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>  
<span class="n">label</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Create train and test index</span>
<span class="n">sss</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">sss</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">logreg</span><span class="p">:</span>
    
    <span class="n">data_shared</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">train_idx</span><span class="p">,</span> <span class="p">:])</span>
    <span class="c1"># Priors</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">data_shared</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">label_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s1">&#39;labels_obs&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">label</span><span class="p">[</span><span class="n">train_idx</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>In this case we will try ADVI to get a starting point</p>
<p><strong>Automatic differentiation Variational inference (ADVI)</strong></p>
<p><a class="reference external" href="https://docs.pymc.io/notebooks/variational_api_quickstart.html">PyMC Variational API</a></p>
<ol class="simple">
<li><p>Select ‘advi’ or ‘fullrank_advi’ as method to fit the model</p></li>
<li><p>Choose number of iterations</p></li>
<li><p>Get a trace from the fitted model</p></li>
<li><p>Study the convergence of the objective function and traced parameters</p></li>
<li><p>Inspect traces and posteriors with diagnostic plot</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">logreg</span><span class="p">:</span>    
    <span class="n">inference</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">ADVI</span><span class="p">()</span>
    <span class="c1">#inference = pm.FullRankADVI()</span>
    
    <span class="n">approx</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="n">inference</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> 
                    <span class="n">obj_optimizer</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">))</span>
    <span class="n">advi_trace</span> <span class="o">=</span> <span class="n">approx</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>     
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evolution-of-the-cost-function-elbo">
<h4><span class="section-number">8.7.2.2. </span>Evolution of the cost function (ELBO)<a class="headerlink" href="#evolution-of-the-cost-function-elbo" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">approx</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="accuracy-on-train-and-test">
<h4><span class="section-number">8.7.2.3. </span>Accuracy on train and test<a class="headerlink" href="#accuracy-on-train-and-test" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_probs</span> <span class="o">=</span> <span class="n">approx</span><span class="o">.</span><span class="n">sample_node</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">test_probs</span> <span class="o">=</span> <span class="n">approx</span><span class="o">.</span><span class="n">sample_node</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">more_replacements</span><span class="o">=</span><span class="p">{</span><span class="n">data_shared</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:]})</span>

<span class="n">test_acc</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="o">==</span>  <span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]))</span>
    <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="o">==</span>  <span class="n">label</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]))</span>
<span class="n">train_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train: </span><span class="si">%f</span><span class="s2"> </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_acc</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test: </span><span class="si">%f</span><span class="s2"> </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_acc</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="compute-100-samples-for-the-test-set-predictions">
<h4><span class="section-number">8.7.2.4. </span>Compute 100 samples for the test set predictions<a class="headerlink" href="#compute-100-samples-for-the-test-set-predictions" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_predictions</span> <span class="o">=</span> <span class="n">approx</span><span class="o">.</span><span class="n">sample_node</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">more_replacements</span><span class="o">=</span><span class="p">{</span><span class="n">data_shared</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:]},</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">test_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">p_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">p_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="check-predictions-and-uncertainty">
<h4><span class="section-number">8.7.2.5. </span>Check predictions and uncertainty<a class="headerlink" href="#check-predictions-and-uncertainty" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">data_reduced</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">marker</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">]):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_reduced</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span><span class="o">==</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_reduced</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span><span class="o">==</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                         <span class="n">c</span><span class="o">=</span><span class="n">p_mean</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span><span class="o">==</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> 
                         <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_reduced</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span><span class="o">==</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_reduced</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span><span class="o">==</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                         <span class="n">c</span><span class="o">=</span><span class="n">p_std</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span><span class="o">==</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> 
                         <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
                         <span class="n">vmax</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">p_std</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span><span class="o">==</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]));</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;mean p&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;std p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="approximate-posteriors-for-the-parameters">
<h4><span class="section-number">8.7.2.6. </span>Approximate posteriors for the parameters<a class="headerlink" href="#approximate-posteriors-for-the-parameters" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">trace_to_dataframe</span><span class="p">(</span><span class="n">advi_trace</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">plotting</span><span class="o">.</span><span class="n">scatter_matrix</span><span class="p">(</span><span class="n">df_trace</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="s1">&#39;kde&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "info183"
        },
        kernelOptions: {
            kernelName: "info183",
            path: "./lectures/11_MCMC"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'info183'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../9_mixture_models/lecture.html" title="previous page"><span class="section-number">7. </span>Gaussian Mixture Models</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Pablo Huijse and Eliana Scheihing<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>