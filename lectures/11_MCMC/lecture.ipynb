{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, FloatSlider, SelectionSlider, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import pymc3 as pm\n",
    "print('Running PyMC3 v{}'.format(pm.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chain Monte Carlo\n",
    "\n",
    "References for today's lecture:\n",
    "\n",
    "1. Davidson-Pilon, \"[Bayesian methods for hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\", *Addison Wesley*, 2016, **Chapter 2 and 3**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: The Bayesian setting\n",
    "\n",
    "Reminder from previous classes:\n",
    "\n",
    "- **Probability:** Represents how believable an event is\n",
    "     - How confident we are that the event occurs\n",
    "- Our belief of a certain event $A$ is the **prior probability** $p(A)$\n",
    "- We collect evidence $X$ to **update** our belief on $A$ forming a **posterior** $p(A|X)$\n",
    "- How? Bayes Theorem\n",
    "\n",
    "$$\n",
    "p(A|X) = \\frac{p(X|A)p(A)}{p(X)} \\propto p(X|A)p(A)\n",
    "$$\n",
    "\n",
    "- In general the larger the amount of evidence the less influence the prior has\n",
    "\n",
    "\n",
    "\n",
    "Model/parameter estimation:\n",
    "\n",
    "- Maximum likelihood (MLE): Point estimate\n",
    "- Maximum a posterior (MAP): Point estimate plus prior\n",
    "- Bayes: Full posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One more time: Bayes Theorem + law of total probability:\n",
    "$$\n",
    "p(A|X) = \\frac{p(X|A)p(A)}{p(X)} = \\frac{p(X|A)p(A)}{\\int p(X, A) dA} = \\frac{p(X|A)p(A)}{\\int p(X|A)p(A) dA}\n",
    "$$\n",
    "\n",
    "- We propose the **prior** and a **likelihood**: Our assumptions on the data and parameter distributions\n",
    "- The **evidence** is ... usually intractable \n",
    "    - We are integrating on all the possible values of the parameters, remember GMM?\n",
    "- Are we done? What are our options?\n",
    "    - Use priors/likelihoods so that posterior is analytical (conjugate)\n",
    "    - Approximate inference (Variational Bayes)\n",
    "    - **Today:** Monte-Carlo Markov Chain (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [The Bayesian landscape](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb#The-Bayesian-landscape)\n",
    "\n",
    "- In the following example we draw two-dimensional Poisson distributed data\n",
    "- We consider exponential priors for the rate $\\lambda$\n",
    "- This is essentially a two dimensional surface in parameter space\n",
    "- The prior sets the initial shape \n",
    "- The observations warp the surface\n",
    "- To find the best parameters we need to explore this possibly high-dim space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 4), tight_layout=True)\n",
    "\n",
    "def update_plot(rseed, N):\n",
    "    np.random.seed(rseed); ax[0].cla(); ax[1].cla();\n",
    "    lambda_1_true = 1; lambda_2_true = 3\n",
    "    data = np.concatenate([scipy.stats.poisson.rvs(lambda_1_true, size=(N, 1)),\n",
    "                           scipy.stats.poisson.rvs(lambda_2_true, size=(N, 1))\n",
    "                          ], axis=1)\n",
    "\n",
    "    x = y = np.linspace(.01, 5, 100)\n",
    "    likelihood_x = np.array([scipy.stats.poisson.pmf(data[:, 0], _x)\n",
    "                            for _x in x]).prod(axis=1)\n",
    "    likelihood_y = np.array([scipy.stats.poisson.pmf(data[:, 1], _y)\n",
    "                            for _y in y]).prod(axis=1)\n",
    "    L = np.dot(likelihood_x[:, None], likelihood_y[None, :])\n",
    "\n",
    "\n",
    "    exp_x = scipy.stats.expon.pdf(x, loc=0, scale=3)\n",
    "    exp_y = scipy.stats.expon.pdf(x, loc=0, scale=10)\n",
    "    M = np.dot(exp_x[:, None], exp_y[None, :])\n",
    "\n",
    "    im = ax[0].imshow(M, interpolation='none', origin='lower',\n",
    "                    cmap=plt.cm.jet, extent=(0, 5, 0, 5))\n",
    "    ax[0].scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\n",
    "    ax[0].set_xlim(0, 5); ax[0].set_ylim(0, 5)\n",
    "    ax[0].set_title(\"Prior Landscape\")\n",
    "\n",
    "    im = ax[1].imshow(M * L, interpolation='none', origin='lower',\n",
    "                    cmap=plt.cm.jet, extent=(0, 5, 0, 5))\n",
    "\n",
    "    ax[1].scatter(lambda_2_true, lambda_1_true, c=\"k\", s=50, edgecolor=\"none\")\n",
    "    ax[1].set_title(\"Landscape warped \\nby %d data observation.\" % N)\n",
    "    ax[1].set_xlim(0, 5); ax[1].set_ylim(0, 5);\n",
    "\n",
    "interact(update_plot, \n",
    "         N=SelectionSlider(options=[1, 10, 100]),\n",
    "         rseed=IntSlider(min=0, max=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo methods\n",
    "\n",
    "Monte carlo methods obtain numerical results via repeated random sampling\n",
    "\n",
    "One of its most important case uses is: Monte-carlo integration\n",
    "\n",
    "Let's say we want to the expected value of a function $g$ on a random variable $X \\sim f$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g(X)] = \\int g(x) f(x) \\,dx\n",
    "$$\n",
    "\n",
    "The Monte Carlo estimator of this integral is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g(X)] \\approx \\hat g_N = \\frac{1}{N} \\sum_{i=1}^N g(x_i) \\quad x_i \\sim f(x)\n",
    "$$\n",
    "\n",
    "and due to the CLT we have\n",
    "\n",
    "$$\n",
    "\\hat g_N \\sim \\mathcal{N} \\left( \\mathbb{E}[g(X)], \\sigma_N^2/N \\right)\n",
    "$$\n",
    "\n",
    "We could use this to estimate very hard integrals (posteriors)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Estimating the value of PI using Monte Carlo\n",
    "\n",
    "We throw random \"balls\" into the unitary square and we count how many fall inside the circle\n",
    "\n",
    "If we divide the area of the circle and the square we get $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def func(x, y):\n",
    "    return (x-0)**2 + (y-0)**2 -1. <= 0.\n",
    "\n",
    "x = np.linspace(0, 1, num=1000); \n",
    "X, Y = np.meshgrid(x, x)\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "\n",
    "def update(N, rseed):\n",
    "    np.random.seed(rseed);\n",
    "    ax.cla(); ax.contourf(X, Y, func(X, Y), cmap=plt.cm.Reds); \n",
    "    ax.set_aspect('equal')\n",
    "    xr = np.random.rand(N, 2)\n",
    "    ax.scatter(xr[:, 0], xr[:, 1], s=1, c='k', alpha=1)\n",
    "    N_inside = len(np.where(func(xr[:, 0], xr[:, 1]))[0])\n",
    "    print(\"%0.4f\" %(4.*N_inside/N))\n",
    "\n",
    "interact(update, \n",
    "         N=SelectionSlider(options=[1, 10, 100, 1000, 10000, 100000]), \n",
    "         rseed=IntSlider(min=0, max=100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.array([1, 10, 100, 1_000, 10_000, 100_000, 1_000_000])\n",
    "N_inside = np.zeros_like(N)\n",
    "for i, N_ in enumerate(N):\n",
    "    xr = np.random.rand(N_, 2)\n",
    "    N_inside[i] = len(np.where(func(xr[:, 0], xr[:, 1]))[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(N, 4*N_inside/N)\n",
    "ax.set_xscale('log')\n",
    "ax.axhline(np.pi, c='r', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Markov Chain\n",
    "\n",
    "- Consider a system, i.e. a set of states $X$ that evolve in time (steps)\n",
    "- A system can be modeled with a **Markov Chain**: stochastic process for sequences\n",
    "- Given that it fits the **Markov property**\n",
    "\n",
    "$$\n",
    "p(X_n|X_{n-1},\\ldots, X_0) = p(X_n|X_{n-1})\n",
    "$$\n",
    "\n",
    "*i.e* the probability of the future state is conditionally independant of the past given the present\n",
    "\n",
    "Markov chains that are irreducible will converge in the long term to a stationary distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "The problem with MC integration is that sometimes we can't draw from distribution $f$. Also randomly sampling in all the space is very inefficient\n",
    "\n",
    "Instead we could use Markov Chain Monte Carlo, a family of algorithms that learn the transition probabilities of a markov chain so that it converges to a desired distribution\n",
    "\n",
    "In our case the desired distribution is the bayesian posterior\n",
    "\n",
    "<img src=\"img/is_mcmc.png\" width=\"500\">\n",
    "\n",
    "\n",
    "MCMC searches the space in a less naive way. A sequence of samples is called a **trace**\n",
    "\n",
    "> Random walk with a generated **step** Â¿How to select the step?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are several proposal algoritms two of the most popular ones are:\n",
    "\n",
    "### Metropolis Hastings (MH)\n",
    "\n",
    "- Random walker that moves on all dimensions simultaneously\n",
    "- A candidate step is drawn from an arbitrary but symmetric distribution $x^{new} \\sim g(x^{new}|x_t)$\n",
    "- We accept the step if $f(x^{new})/f(x^t)$ is equal or larger than a certain threshold\n",
    "- $f(\\cdot)$ needs only to be proportional to the target distribution (evidence is canceled in the ratio)\n",
    "- Repeat many times until convergence\n",
    "\n",
    "\n",
    "### Hamiltonian Monte-Carlo\n",
    "\n",
    "- Family of step proposing methods that use momentum (derivatives)\n",
    "- Only for continuous variables\n",
    "- Cost more than MH (single iteration) but require less iterations\n",
    "- http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic programming\n",
    "\n",
    "- PP: Doing statistics (Bayesian inference) using the tools of computer science \n",
    "- PP languages: unify general purpose programming with probabilistic modeling\n",
    "- Python friendly PP frameworks/libraries:\n",
    "    - [PyMC3](https://docs.pymc.io/notebooks/getting_started.html): Black-box VI, MH, Gibbs, NUTS sampler. Uses theano\n",
    "    - [PyStan](https://pystan.readthedocs.io/en/latest/): Python interface for [Stan platform](http://mc-stan.org/)\n",
    "    - [Edward](http://edwardlib.org/): Black-box VI, neural networks. Uses tensorflow\n",
    "    - [Pyro](http://pyro.ai): Black-box VI, neural networks, MCMC. Uses pytorch\n",
    "    - [emcee](http://dfm.io/emcee/current/): Pure python implementation of the Affine invariant MCMC ensemble sampler\n",
    "    - http://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1809.10756\"><img src=\"img/PP.png\"></a>\n",
    "\n",
    "PP runs in two directions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A PyMC3 tutorial\n",
    "\n",
    "\n",
    "In this tutorial we will review how to do\n",
    "\n",
    "1. Model definition\n",
    "1. Fitting\n",
    "1. Convergence checks\n",
    "1. Posterior analysis\n",
    "\n",
    "with PyMC3\n",
    "\n",
    "We start with an example of Bayesian linear regression (class 2)\n",
    "\n",
    "- Gaussian noise with variance $\\sigma^2$\n",
    "- One independent variables: $X$\n",
    "- Three parameters $\\beta$: intercept plus one coefficient per covariate\n",
    "\n",
    "$$\n",
    "Y = \\beta X + \\epsilon \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "Credit: https://docs.pymc.io/notebooks/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "beta_true, sigma_true, N = [1, 2.5], 1., 30\n",
    "X = np.random.randn(N)\n",
    "Y = beta_true[0] + beta_true[1]*X +  np.random.randn(N)*sigma_true\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.scatter(X, Y)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model specification\n",
    "\n",
    "- Using the `with` keyword we create a context from `pm.Model()`\n",
    "- Within this context we will set priors, likelihood, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as my_model:\n",
    "    # Priors\n",
    "    beta = pm.Normal(name='beta', mu=0, sd=10, shape=2)\n",
    "    sigma = pm.HalfNormal('sigma', sd=1, testval=np.std(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Creating random variables\n",
    "\n",
    "We have set two stochastic variables with normal and half normal distributions, respectively\n",
    "\n",
    "> These are the priors for $\\beta$ and $\\sigma$\n",
    "\n",
    "For $\\beta$ we used a normal prior. The constructor for normal is\n",
    "\n",
    "    pm.Normal(name='beta', mu=0, sd=10, shape=3, testval=None)\n",
    "\n",
    "where\n",
    "1. `name` (string): Unique identifier, in this case 'beta'\n",
    "1. mu and sd (floats): Mean and standard deviation of the normal distribution in this case 0 and 10, respectively\n",
    "1. `shape`: Specifies the dimensionality, in this case we create 3 univariate normals\n",
    "1. `testval`: Gives initial values \n",
    "\n",
    "For $\\sigma$ we use a half-normal prior\n",
    "\n",
    "This variable is non-negative so we have to use a non-negative prior (*e.g.* Gamma) or a bounded prior\n",
    "\n",
    "#### Bounded distributions\n",
    "One can create arbitrary bounded distributions with\n",
    "\n",
    "    x = pm.Bound(pm.Normal, lower=0.0)('x', mu=1.0, sd=3.0)\n",
    "\n",
    "\n",
    "#### Other distributions\n",
    "Check the list available distributions [here](https://docs.pymc.io/api/distributions.html)\n",
    "\n",
    "#### Specifying the likelihood\n",
    "\n",
    "Now we can specify the likelihood of the model\n",
    "\n",
    "First, to continue working in the previously defined context we use \n",
    "\n",
    "    with my_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with my_model:\n",
    "    X_shared = pm.Data('x', X)\n",
    "    # Likelihood\n",
    "    mu = pm.Deterministic('mu', beta[0] + beta[1]*X_shared)\n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sd=sigma, observed=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The likelihood is a random variable with the keyword `observed` defined\n",
    "\n",
    "> In `observed` we pass the data. It can be a numpy ndarray or a pandas data frame\n",
    "\n",
    "\n",
    "By giving `beta` and `sigma` as parameter for `Y_obs` we automatically create a parent-child relation\n",
    "\n",
    "The following attributes have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(my_model.free_RVs)\n",
    "print(my_model.deterministics)\n",
    "print(my_model.observed_RVs)\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model fitting\n",
    "\n",
    "In PyMC3 we can do MAP to get point estimates, VI to do approximate inference and MCMC for posterior sampling\n",
    "\n",
    "- MAP and VI follow an optimization approach. \n",
    "- They are generally faster than MCMC but return less information \n",
    "    - point estimate with no uncertainty\n",
    "    - approximate factorized distribution (only continuous variables)\n",
    "- MAP and VI can be used to find reasonable initial states for MCMC\n",
    "- For very complex model and large number of observations we may not be able to do MCMC at all \n",
    "\n",
    "**MAP estimate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with my_model:\n",
    "    map_estimate = pm.find_MAP(progressbar=True)\n",
    "map_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MCMC sampling\n",
    "\n",
    "- To do MCMC first you have to specify a step method (MH, Gibbs, NUTS, etc)\n",
    "- PyMC3 have very good default options\n",
    "    - No-U-Turn sampler is the default option for continuous parameters\n",
    "    - MH is the default for discrete parameters\n",
    "- `pm.sample` is the main MCMC interface\n",
    "\n",
    "```python\n",
    "sample(draws=500, \n",
    "       step=None, \n",
    "       init='auto', \n",
    "       n_init=200000, \n",
    "       start=None, \n",
    "       trace=None, \n",
    "       chains=None, \n",
    "       cores=None, \n",
    "       tune=500, \n",
    "       ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with my_model:\n",
    "    # draw 500 posterior samples\n",
    "    # trace = pm.sample(draws=2000, start=map_estimate) # Start from MAP\n",
    "    trace = pm.sample(draws=2000, init='advi') # Start from VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check posteriors with traceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, figsize=(7, 3), var_names=['beta', 'sigma'], combined=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, var_names=['beta', 'sigma'], figsize=(7, 2), textsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Transform trace to dataframe\n",
    "df_trace = pm.trace_to_dataframe(trace)[['beta__0', 'beta__1', 'sigma']]\n",
    "pd.plotting.scatter_matrix(df_trace, diagonal='kde', figsize=(6, 4))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use diagnostics to check the convergence of the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.summary(trace).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Gelman-Rubin diagnostic tests for lack of convergence \n",
    "- Compares the variance between multiple chains to the variance within each chain.\n",
    "- Values greater than one indicate that one or more chains have not yet converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw posterior predictive distribution\n",
    "\n",
    "In practice, for a regressor, we want the posterior predictive distribution, i.e. the prediction **y** for a new sample **x** given the training dataset\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}|\\textbf{x}, \\mathcal{D}) = \\int p(\\textbf{y}|\\textbf{x},\\theta) p(\\theta| \\mathcal{D}) \\,d\\theta \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(np.amin(X), np.amax(X), num=1000)\n",
    "with my_model:\n",
    "    pm.set_data({\"x\": x_test})\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace, samples=50, \n",
    "                                                          var_names=[\"mu\", \"Y_obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); \n",
    "\n",
    "for line in posterior_predictive[\"mu\"]:\n",
    "    ax.plot(x_test, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); \n",
    "\n",
    "mean = np.mean(posterior_predictive[\"mu\"], axis=0)\n",
    "perc = np.percentile(posterior_predictive[\"mu\"], q=(5, 95), axis=0)\n",
    "\n",
    "ax.plot(x_test, mean)\n",
    "ax.fill_between(x_test, perc[1], perc[0], alpha=0.5)\n",
    "ax.scatter(X, Y, c='k')\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second example: Mixture of Gaussians\n",
    "\n",
    "- In this example we will try to infer the parameters of a mixture of two 1d Gaussians\n",
    "- Let's cerate some data\n",
    "\n",
    "Ref: http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu_true = [-1, 2]\n",
    "std_true = [2, 0.75]\n",
    "p_true, N = 0.4, 200\n",
    "p = np.array([p_true, 1-p_true])\n",
    "np.random.seed(0)\n",
    "data = np.concatenate((scipy.stats.norm(loc=mu_true[0], scale=std_true[0]).rvs(size=int(p[0]*N)),\n",
    "                       scipy.stats.norm(loc=mu_true[1], scale=std_true[1]).rvs(size=int(p[1]*N))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "ax.hist(data, bins=20, alpha=0.8, density=True);\n",
    "x_plot = np.linspace(np.amin(data), np.amax(data), num=1000)\n",
    "for k in range(2):\n",
    "    ax.plot(x_plot, p[k]*np.exp(-0.5*(x_plot - mu_true[k])**2/std_true[k]**2)/(np.sqrt(2.*np.pi)*std_true[k]), 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model specification**\n",
    "\n",
    "We create priors for the center, dispersion and weights of the Gaussians\n",
    "\n",
    "Ref: https://docs.pymc.io/notebooks/gaussian_mixture_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Prior on concentration\n",
    "    p = pm.Dirichlet('p', a=np.array([0.1, 0.1]), shape=2)\n",
    "    # Use this to avoid singularities (empty clusters)\n",
    "    p_min_potential = pm.Potential('p_min_potential', \n",
    "                                   T.switch(T.min(p) < .1, -np.inf, 0))\n",
    "    z = pm.Categorical(\"z\", p, shape=data.shape[0])\n",
    "    # Prior on standard deviations\n",
    "    sds = pm.Uniform(\"sds\", lower=0, upper=100, shape=2)\n",
    "    # Prior on means\n",
    "    centers = pm.Normal(\"centers\", \n",
    "                        mu=np.array([-1, 1]), \n",
    "                        sd=np.array([10, 10]), \n",
    "                        shape=2)\n",
    "    # Use this to avoid identifiability problems\n",
    "    order_means_potential = pm.Potential('order_means_potential',\n",
    "                                         T.switch(centers[1]-centers[0] < 0, -np.inf, 0))\n",
    "    \n",
    "    center_i = pm.Deterministic('center_i', centers[z])\n",
    "    sd_i = pm.Deterministic('sd_i', sds[z])\n",
    "    \n",
    "    # Likelihood\n",
    "    observations = pm.Normal(\"obs\", mu=center_i, sd=sd_i, observed=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Deterministic variables allow us to track custom variables in the traces\n",
    "\n",
    "`pm.Deterministic('name', var)`\n",
    "\n",
    "Potentials allow us to add an arbitrary factor the the likelihood\n",
    "\n",
    "`pm.Potential('name', var)`\n",
    "\n",
    "In this case we add potentials to\n",
    "- Penalize solutions with  empty clusters\n",
    "- Forcing that the \"first\" gaussian is always to the left (remember z flipping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MCMC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different step functions for different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    step1 = pm.Metropolis(vars=[p, sds, centers])\n",
    "    step2 = pm.CategoricalGibbsMetropolis(vars=[z])\n",
    "    trace = pm.sample(draws=4000, step=[step1, step2], tune=1000, chains=2, cores=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - `step`: In this case we have specified the step functions for each variable\n",
    " - Burn-in period `tune`: N first steps of the chain are discarded from the trace to build posteriors from the converged zone\n",
    " - `chains`: We can also specify the amount of chains and cores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, figsize=(9, 6), combined=True, varnames=['p', 'centers', 'sds']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.summary(trace, var_names=['centers', 'sds', 'p']).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plots.autocorrplot(trace, figsize=(7, 4), varnames=['centers', 'sds', 'p']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A chain that is exploring the space well will exhibit very high autocorrelation. \n",
    "- Low autocorrelation is a sufficient condition for converged MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Posterior predictive checks (PPC)\n",
    "\n",
    "Another way to validate a model is to generate data from the posterior draws and compare with the original distribution\n",
    "\n",
    "This is done with `pm.sample_posterior_predictive` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(samples=1000, trace=trace, model=model)\n",
    "\n",
    "x_plot = np.linspace(np.amin(data), np.amax(data), num=1000)\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "ax.hist(data, bins=20, alpha=0.8, density=True);\n",
    "mean_score = 0.0\n",
    "for i in range(100):\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(ppc['obs'][i, :].reshape(-1, 1))\n",
    "    score = np.exp(kde.score_samples(x_plot.reshape(-1, 1)))\n",
    "    mean_score += score\n",
    "    ax.plot(x_plot, score, 'k', alpha=0.05)\n",
    "ax.plot(x_plot, mean_score/100, 'k', lw=4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Useful tips for MCMC](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb#Useful-tips-for-MCMC)\n",
    "\n",
    "- Intelligent starting values\n",
    "- Choose your priors well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Last example: Multilabel logistic (softmax) regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!wget -nc http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data  \n",
    "label = iris.target\n",
    "\n",
    "# Create train and test index\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3)\n",
    "train_idx, test_idx = next(sss.split(data, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as logreg:\n",
    "    \n",
    "    data_shared = pm.Data('X', data[train_idx, :])\n",
    "    # Priors\n",
    "    alpha = pm.Normal(name='alpha', mu=0, sd=10, shape=(3, ))\n",
    "    beta = pm.Normal(name='beta', mu=0, sd=10, shape=(4, 3))\n",
    "    p = T.nnet.softmax(data_shared.dot(beta) + alpha)\n",
    "    label_obs = pm.Categorical('labels_obs', p=p, observed=label[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case we will try ADVI to get a starting point\n",
    "\n",
    "**Automatic differentiation Variational inference (ADVI)**\n",
    "\n",
    "[PyMC Variational API](https://docs.pymc.io/notebooks/variational_api_quickstart.html)\n",
    "\n",
    "1. Select 'advi' or 'fullrank_advi' as method to fit the model\n",
    "1. Choose number of iterations\n",
    "1. Get a trace from the fitted model\n",
    "1. Study the convergence of the objective function and traced parameters\n",
    "1. Inspect traces and posteriors with diagnostic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with logreg:    \n",
    "    inference = pm.ADVI()\n",
    "    #inference = pm.FullRankADVI()\n",
    "    \n",
    "    approx = pm.fit(method=inference, n=30000, \n",
    "                    obj_optimizer=pm.adam(learning_rate=0.001))\n",
    "    advi_trace = approx.sample(draws=5000)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of the cost function (ELBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(approx.hist, alpha=.3); ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs = approx.sample_node(p)\n",
    "test_probs = approx.sample_node(p, more_replacements={data_shared: data[test_idx, :]})\n",
    "\n",
    "test_acc, train_acc = [], []\n",
    "for i in range(10):\n",
    "    test_acc.append(np.mean(test_probs.argmax(-1).eval() ==  label[test_idx]))\n",
    "    train_acc.append(np.mean(train_probs.argmax(-1).eval() ==  label[train_idx]))\n",
    "train_acc = np.array(train_acc)\n",
    "test_acc = np.array(test_acc)\n",
    "print(\"Train: %f %f\" % (np.mean(train_acc), np.std(train_acc)))\n",
    "print(\"Test: %f %f\" % (np.mean(test_acc), np.std(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute 100 samples for the test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = approx.sample_node(p, more_replacements={data_shared: data[test_idx, :]}, size=100).eval()\n",
    "display(test_predictions.shape)\n",
    "p_mean = np.mean(test_predictions, axis=0)\n",
    "p_std = np.std(test_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check predictions and uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2).fit(data)\n",
    "data_reduced = pca.transform(data[test_idx, :])\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(5, 6), sharex=True, sharey=True, tight_layout=True)\n",
    "for i in range(3):\n",
    "    for k, marker in enumerate(['x', 'o', 'd']):\n",
    "        ax[i, 0].scatter(data_reduced[label[test_idx]==k, 0], data_reduced[label[test_idx]==k, 1], \n",
    "                         c=p_mean[label[test_idx]==k, i], marker=marker, \n",
    "                         cmap=plt.cm.Blues, vmin=0, vmax=1);\n",
    "        ax[i, 1].scatter(data_reduced[label[test_idx]==k, 0], data_reduced[label[test_idx]==k, 1], \n",
    "                         c=p_std[label[test_idx]==k, i], marker=marker, \n",
    "                         cmap=plt.cm.Blues, vmin=0, \n",
    "                         vmax=np.amax(p_std[label[test_idx]==k, i]));\n",
    "    ax[i, 0].set_ylabel(labels[i])\n",
    "ax[0, 0].set_title('mean p')\n",
    "ax[0, 1].set_title('std p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate posteriors for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_trace = pm.trace_to_dataframe(advi_trace)\n",
    "pd.plotting.scatter_matrix(df_trace, diagonal='kde', figsize=(9, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFO183",
   "language": "python",
   "name": "info183"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
