
<!DOCTYPE html>

<html lang="es">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6. Parametric modeling &#8212; Herramientas estadísticas para la investigación</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" />
    <link rel="next" title="7. Nonparametric modeling" href="part2.html" />
    <link rel="prev" title="5. Teoremas Asintóticos" href="../1_TeoriaProbabilidades/parte5.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Herramientas estadísticas para la investigación</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Buscar este libro ..." aria-label="Buscar este libro ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Elementos de Teoría de Probabilidades
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../1_TeoriaProbabilidades/parte1.html">
   1. Elementos Básicos de Teoría de Probabilidades
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1_TeoriaProbabilidades/parte2.html">
   2. Variables Aleatorias
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1_TeoriaProbabilidades/parte3.html">
   3. Estadísticos principales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1_TeoriaProbabilidades/parte4.html">
   4. Variables aleatorias especiales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../1_TeoriaProbabilidades/parte5.html">
   5. Teoremas Asintóticos
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Modelamiento Estadístico
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Parametric modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="part2.html">
   7. Nonparametric modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="part3.html">
   8. Bayesian modeling
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Pensamiento Estadístico
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../3_InferenciaParametrica/parte1.html">
   9. Inferencia en Estadística Paramétrica
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../4_nonparametric_inference/lecture.html">
   10. Inferencia estadística con tests no-paramétricos
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Regresión Lineal
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../5_linear_regression/part1.html">
   11. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5_linear_regression/part2.html">
   12. Multivariate linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../5_linear_regression/part3.html">
   13. Linear models and Basis functions
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navegación de palanca" aria-controls="site-navigation"
                title="Navegación de palanca" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Descarga esta pagina"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/lectures/2_statistical_modeling/part1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Descargar archivo fuente" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Imprimir en PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/magister-informatica-uach/INFO337"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Repositorio de origen"><i
                    class="fab fa-github"></i>repositorio</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Modo de pantalla completa"
        title="Modo de pantalla completa"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/magister-informatica-uach/INFO337/master?urlpath=tree/lectures/2_statistical_modeling/part1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Lanzamiento Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contenido
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   6.1. Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-statistical-inference">
     6.1.1. What is statistical inference?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-difference-between-parametric-and-non-parametric-models">
     6.1.2. The difference between parametric and non-parametric models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-difference-between-frequentist-and-bayesian-inference">
     6.1.3. The difference between Frequentist and Bayesian inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequentist-approach-on-parametric-modeling">
   6.2. Frequentist approach on parametric modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-likelihood">
     6.2.1. The likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">
   6.3. Maximum Likelihood Estimation (MLE)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-mle-for-the-mean-of-a-gaussian-distribution">
     6.3.1. Example: MLE  for the mean of a Gaussian distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-mle-for-the-variance-of-a-gaussian-distribution">
     6.3.2. Example: MLE for the variance of a Gaussian distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-note-on-biased-and-unbiased-estimators">
     6.3.3. A note on biased and unbiased estimators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-mle-of-a-gaussian-mixture">
     6.3.4. Example: MLE of a Gaussian mixture
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimality-properties-and-uncertainty-of-mles">
   6.4. Optimality properties and uncertainty of MLEs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-cramer-rao-bound-for-the-mle-of-mu">
     6.4.1. Example: Cramer-Rao bound for the MLE of
     <span class="math notranslate nohighlight">
      \(\mu\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hypothesis-tests-based-on-the-likelihood">
   6.5. Hypothesis tests based on the likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wald-test">
     6.5.1. Wald-test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-likelihood-ratio-test-or-wilks-test">
     6.5.2. Log-likelihood ratio test or Wilks test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#criteria-for-model-comparison">
   6.6. Criteria for model comparison
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">holoviews</span> <span class="k">as</span> <span class="nn">hv</span>
<span class="n">hv</span><span class="o">.</span><span class="n">extension</span><span class="p">(</span><span class="s1">&#39;bokeh&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_57680</span><span class="o">/</span><span class="mf">3430701827.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">holoviews</span> <span class="k">as</span> <span class="nn">hv</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">hv</span><span class="o">.</span><span class="n">extension</span><span class="p">(</span><span class="s1">&#39;bokeh&#39;</span><span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;holoviews&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.signal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_57680</span><span class="o">/</span><span class="mf">4043567866.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">scipy.signal</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;scipy&#39;
</pre></div>
</div>
</div>
</div>
<div class="section" id="parametric-modeling">
<h1><span class="section-number">6. </span>Parametric modeling<a class="headerlink" href="#parametric-modeling" title="Enlazar permanentemente con este título">¶</a></h1>
<div class="section" id="introduction">
<h2><span class="section-number">6.1. </span>Introduction<a class="headerlink" href="#introduction" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Modeling is a key step in the pipeline of statistical inference. In this series of lectures we will learn how to fit parametric and non-parametric models to our data.</p>
<ul class="simple">
<li><p>In the parametric case we will focus on the classical frequentist method to fit models: maximum likelihood estimation (MLE)</p></li>
<li><p>Then we will review non-parametric modeling techniques such as the Histogram and Kernel density estimation</p></li>
<li><p>After that we will go back to parametric modeling and extend MLE using priors: Maximum a posteriori</p></li>
<li><p>Finally we will present key ideas on bayesian modeling that are to be further developed in a future series of lectures</p></li>
</ul>
<div class="section" id="what-is-statistical-inference">
<h3><span class="section-number">6.1.1. </span>What is statistical inference?<a class="headerlink" href="#what-is-statistical-inference" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Inference is:</p>
<blockquote>
<div><p>To draw conclusions from facts through a scientific premise</p>
</div></blockquote>
<p>In the particular case of <strong>Statistical inference</strong> we have</p>
<ul class="simple">
<li><p>Facts: Observed data</p></li>
<li><p>Premise: Probabilistic model</p></li>
<li><p>Conclusion: An unobserved quantity of interest</p></li>
<li><p>Objective: Quantify the uncertainty of the conclusion given the data and the premise</p></li>
</ul>
<p>The following are examples examples of statistical inference tasks:</p>
<ul class="simple">
<li><p><strong>Parameter estimation:</strong> What is the best estimate of a model parameter based on the observed data?</p></li>
<li><p><strong>Confidence estimation:</strong> How trustworthy is our point estimate?</p></li>
<li><p><strong>Hypothesis testing:</strong> Is the data consistent with a given hypothesis or model?</p></li>
</ul>
</div>
<div class="section" id="the-difference-between-parametric-and-non-parametric-models">
<h3><span class="section-number">6.1.2. </span>The difference between parametric and non-parametric models<a class="headerlink" href="#the-difference-between-parametric-and-non-parametric-models" title="Enlazar permanentemente con este título">¶</a></h3>
<p>To conduct inference we start by defining a statistical model</p>
<p>Models can be broadly classified as:</p>
<p><strong>Parametric:</strong></p>
<ul class="simple">
<li><p>It corresponds to an analytical function  (distribution) with free parameters</p></li>
<li><p>Has an <em>a-priori</em> fixed number of parameters</p></li>
<li><p>In general: Stronger assumptions, easier to interpret, faster to use</p></li>
</ul>
<p><strong>Non-parametric:</strong></p>
<ul class="simple">
<li><p>Distribution-free model but they do have parameters and assumptions (e.g. dependence)</p></li>
<li><p>The number of parameters depends on the amount of training data</p></li>
<li><p>In general: More flexible, harder to train</p></li>
</ul>
</div>
<div class="section" id="the-difference-between-frequentist-and-bayesian-inference">
<h3><span class="section-number">6.1.3. </span>The difference between Frequentist and Bayesian inference<a class="headerlink" href="#the-difference-between-frequentist-and-bayesian-inference" title="Enlazar permanentemente con este título">¶</a></h3>
<p>There are two main paradigms or perspectives for statistical inference: Frequentist (F) or classical and Bayesian (B). There are conceptual differences between these paradigms, for example</p>
<p><strong>Definition of probability:</strong></p>
<ul class="simple">
<li><p>F: Relative frequency of an event. An objective property of the real world</p></li>
<li><p>B: Degree of subjective belief. Probability statements can be made not only on data but also on parameters and models themselves</p></li>
</ul>
<p><strong>Interpretation of parameters:</strong></p>
<ul class="simple">
<li><p>F: They are unknown and fixed constants</p></li>
<li><p>B: They have distributions that quantify the uncertainty of our knowledge about them. We can compute expected values of the parameters</p></li>
</ul>
<p>In this lecture we will focus on the frequentist approach to modeling. We will review the bayesian perspective in a future lesson</p>
</div>
</div>
<div class="section" id="frequentist-approach-on-parametric-modeling">
<h2><span class="section-number">6.2. </span>Frequentist approach on parametric modeling<a class="headerlink" href="#frequentist-approach-on-parametric-modeling" title="Enlazar permanentemente con este título">¶</a></h2>
<p>In the case of <strong>parametric inference</strong> we assume that observations follow a certain distribution, <em>i.e.</em> observations are a realization of a random process (sampling)</p>
<p>The conceptual (iterative) steps of parametric inference are:</p>
<ol class="simple">
<li><p><strong>Model fitting:</strong> Find parameters by fitting data to the current model</p></li>
<li><p><strong>Model proposition:</strong> Propose a new model that accommodates important features of the data better than the previous one</p></li>
</ol>
<p>In the frequentist approach step 1 is typically solved using <strong>Maximum Likelihood Estimation (MLE)</strong>. Other frequentist alternatives are the Method of Moments (MoM) and the M-estimator. Only MLE is covered in this lecture</p>
<div class="section" id="the-likelihood">
<h3><span class="section-number">6.2.1. </span>The likelihood<a class="headerlink" href="#the-likelihood" title="Enlazar permanentemente con este título">¶</a></h3>
<p>The likelihood function is a quantitative description of our experiment (measuring process), and it is the  starting point for <strong>parametric modeling</strong> for both the frequentist and bayesian paradigms. In simple terms the likelihood tells us how good our model is with respect to the <strong>observed data</strong></p>
<p>Let’s now give a mathematical description of the likelihood</p>
<ul class="simple">
<li><p>Suppose we have an experiment that we model as a set of R.Vs <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_N\)</span></p></li>
<li><p>We also have observations/realizations from our R.Vs <span class="math notranslate nohighlight">\(\{x_i\} = x_1, x_2, \ldots, x_N\)</span></p></li>
<li><p>We assume that the R.Vs follow a certain joint probability distribution <span class="math notranslate nohighlight">\(f(x_1, x_2, \ldots, x_N | \theta)\)</span> with parameters <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
</ul>
<p>The likelihood function is then defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathcal{L}(\theta) &amp;= P(X_1=x_1, X_2=x_2, \ldots, X_N=x_n) \nonumber \\
&amp;= f(x_1, x_2, \ldots, x_N | \theta) \nonumber
\end{align}
\end{split}\]</div>
<p>which is a function of the parameters <span class="math notranslate nohighlight">\(\theta\)</span></p>
<p>For the examples in this lecture we will additionally assumme that our observations are <strong>independent and identically distributed</strong> (iid). With this we can simplify the previous expression as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathcal{L}(\theta) &amp;= f(x_1| \theta) \cdot f(x_2| \theta) \cdot \ldots \cdot f(x_N| \theta) \nonumber \\
&amp;= \prod_{i=1}^N f(x_i| \theta) \nonumber
\end{align}
\end{split}\]</div>
<p>The value of the likelihood itself does not hold much meaning, but it can be used to make comparisons between different values of the parameter vector <span class="math notranslate nohighlight">\(\theta\)</span>. <strong>The larger the likelihood the better the model</strong>, i.e. likelihood maximization allows us to find the best <span class="math notranslate nohighlight">\(\theta\)</span> for our data</p>
<p>Before continuing consider the following</p>
<div class="admonition warning">
<p class="admonition-title">Advertencia</p>
<p><strong>Likelihood is not probability</strong></p>
</div>
<ul class="simple">
<li><p>The likelihood of a set of RVs does not integrate (or sum in the discrete case) to unity, <em>i.e.</em> in general the likelihood is not a valid probability density function.</p></li>
<li><p>The likelihood by itself cannot be interpreted as a probability of <span class="math notranslate nohighlight">\(\theta\)</span>, it only tells us how likely is that <span class="math notranslate nohighlight">\(\{x_i\}\)</span> was generated by the distribution <span class="math notranslate nohighlight">\(f\)</span> with parameter <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
</ul>
</div>
</div>
<div class="section" id="maximum-likelihood-estimation-mle">
<h2><span class="section-number">6.3. </span>Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#maximum-likelihood-estimation-mle" title="Enlazar permanentemente con este título">¶</a></h2>
<p>In parametric modeling we are interested in finding <span class="math notranslate nohighlight">\(\theta\)</span> that best fit our observations.</p>
<p>One method to do this is <strong>MLE</strong>:</p>
<ul class="simple">
<li><p><strong>1</strong> Select a distribution (model) for the observations and formulate the likelihood <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span></p></li>
<li><p><strong>2</strong> Search for <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> given the data, <em>i.e.</em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat \theta = \text{arg} \max_\theta \mathcal{L}(\theta),
\]</div>
<p>where the point estimate <span class="math notranslate nohighlight">\(\hat \theta\)</span>  is called the <strong>maximum likelihood estimator</strong> of <span class="math notranslate nohighlight">\(\theta\)</span></p>
<p>After this we can</p>
<ul class="simple">
<li><p><strong>3</strong> Determine the confidence region of <span class="math notranslate nohighlight">\(\hat \theta\)</span> either analytically or numerically (bootstrap, cross-validation, etc)</p></li>
<li><p><strong>4</strong> Make conclusions about your model (hypothesis test)</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Importante</p>
<p>A wrong assumption in step 1 can ruin your inference. Evaluate how appropriate your model is, compare with other models and suggest incremental improvements</p>
</div>
<div class="section" id="example-mle-for-the-mean-of-a-gaussian-distribution">
<h3><span class="section-number">6.3.1. </span>Example: MLE  for the mean of a Gaussian distribution<a class="headerlink" href="#example-mle-for-the-mean-of-a-gaussian-distribution" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Let us consider a set of N measurements <span class="math notranslate nohighlight">\(\{x_i\}_{i=1,\ldots, N}\)</span> obtained from a sensor and that all were obtained under the same conditions</p>
<p>The sensor used to measure the data has an error that follows a Gaussian distribution with known variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></p>
<p>If the conditions are the same then the measurements can be viewed as noisy realizations of the true value <span class="math notranslate nohighlight">\(\mu\)</span></p>
<div class="math notranslate nohighlight">
\[
x_i = \mu + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2),
\]</div>
<p>and we can write the distribution of <span class="math notranslate nohighlight">\(x_i\)</span> as</p>
<div class="math notranslate nohighlight">
\[
f(x_i) = \mathcal{N}(x_i |\mu,\sigma^2) \quad \forall i
\]</div>
<p>Finally, the likelihood of <span class="math notranslate nohighlight">\(\mu\)</span> given the measurements and the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mu) = f(\{x_i\}| \mu, \sigma^2) = \prod_{i=1}^N f(x_i| \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \prod_{i=1}^N  \exp  \left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right)
\]</div>
<p>We can find <span class="math notranslate nohighlight">\(\mu\)</span> by maximizing the likelihood given <span class="math notranslate nohighlight">\(\{x_i\}\)</span></p>
<div class="admonition tip">
<p class="admonition-title">Truco</p>
<p>In many cases it is more practical to find the maximum of the <strong>logarithm of the likelihood</strong> (e.g. distributions from the exponential family). Logarithm is a monotonic function and its maximum is the same as its argument.</p>
</div>
<p>In this case the log likelihood is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log \mathcal{L} (\mu) &amp;= \log \prod_{i=1}^N f(x_i|\mu, \sigma^2) \nonumber \\
&amp;= \sum_{i=1}^N \log f(x_i|\mu, \sigma^2) \nonumber \\
&amp;= - \frac{1}{2} \sum_{i=1}^N \log 2\pi\sigma^2 - \frac{1}{2} \sum_{i=1}^N  \frac{(x_i-\mu)^2}{\sigma^2}  \nonumber  \\
&amp;=  - \frac{N}{2} \log 2\pi\sigma^2 - \frac{1}{2\sigma^{2}}   \sum_{i=1}^N (x_i-\mu)^2 \nonumber 
\end{align}
\end{split}\]</div>
<p>We maximize by making the derivative of the log likelihood equal to zero</p>
<div class="math notranslate nohighlight">
\[
\frac{d  \log \mathcal{L} (\mu)}{d\mu} =  \frac{1}{\sigma^{2}}  \sum_{i=1}^N (x_i-\mu) =0
\]</div>
<p>Finally the MLE of <span class="math notranslate nohighlight">\(\mu\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\hat \mu = \frac{1}{N} \sum_{i=1}^N x_i, \quad \sigma &gt;0,
\]</div>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Definition_of_the_sample_mean">sample mean</a> is the MLE estimator of the mean for a Gaussian likelihood</p>
</div>
<div class="section" id="example-mle-for-the-variance-of-a-gaussian-distribution">
<h3><span class="section-number">6.3.2. </span>Example: MLE for the variance of a Gaussian distribution<a class="headerlink" href="#example-mle-for-the-variance-of-a-gaussian-distribution" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Let’s say now that we don’t know the variance of the noise of the sensor</p>
<p>The MLE estimator of the variance can be obtained using the same procedure:</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L} (\mu, \sigma^2) =  - \frac{N}{2} \log 2\pi\sigma^2 - \frac{1}{2\sigma^{2}}   \sum_{i=1}^N (x_i-\mu)^2 
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{d  \log \mathcal{L} (\mu, \sigma^2)}{d\sigma^2} =  - \frac{N}{2} \frac{1}{\sigma^2} + \frac{1}{2\sigma^{4}}\sum_{i=1}^N (x_i-\mu)^2 =0
\]</div>
<div class="math notranslate nohighlight">
\[
\hat \sigma^2 = \frac{1}{N} \sum_{i=1}^N (x_i- \hat\mu)^2
\]</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>If the true mean is not known then this is a biased estimator of the true variance. MLE does not guarantee unbiased estimators</p>
</div>
<p>The following code example shows how the value of the MLEs of these parameters evolve as more data is observed</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># data from the sensor</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">80</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1">#x = 80 + 2*np.random.rand(1000)  # What happens if the data is not normal</span>

<span class="c1"># Computing the MLE</span>
<span class="n">Ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">16</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="n">hat_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">N</span><span class="p">])</span> <span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="n">Ns</span><span class="p">])</span>
<span class="n">hat_s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x</span><span class="p">[:</span><span class="n">N</span><span class="p">]</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">mu</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">hat_mu</span><span class="p">,</span> <span class="n">Ns</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Curve</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">hat_mu</span><span class="p">),</span> <span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="s1">&#39;hat mu&#39;</span><span class="p">)</span><span class="o">*</span><span class="n">hv</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="mi">80</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">s2_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Curve</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">hat_s2</span><span class="p">),</span> <span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="s1">&#39;hat s2&#39;</span><span class="p">)</span><span class="o">*</span><span class="n">hv</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="p">(</span><span class="n">mu_plot</span> <span class="o">+</span> <span class="n">s2_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">hv</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">Curve</span><span class="p">(</span><span class="n">logx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">350</span><span class="p">),</span> <span class="n">hv</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">line_dash</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_57680</span><span class="o">/</span><span class="mf">1155900807.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">mu_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Curve</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">hat_mu</span><span class="p">),</span> <span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="s1">&#39;hat mu&#39;</span><span class="p">)</span><span class="o">*</span><span class="n">hv</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="mi">80</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">s2_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Curve</span><span class="p">((</span><span class="n">Ns</span><span class="p">,</span> <span class="n">hat_s2</span><span class="p">),</span> <span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="s1">&#39;hat s2&#39;</span><span class="p">)</span><span class="o">*</span><span class="n">hv</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="p">(</span><span class="n">mu_plot</span> <span class="o">+</span> <span class="n">s2_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">hv</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">Curve</span><span class="p">(</span><span class="n">logx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">350</span><span class="p">),</span> <span class="n">hv</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">line_dash</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">))</span>

<span class="ne">NameError</span>: name &#39;hv&#39; is not defined
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="a-note-on-biased-and-unbiased-estimators">
<h3><span class="section-number">6.3.3. </span>A note on biased and unbiased estimators<a class="headerlink" href="#a-note-on-biased-and-unbiased-estimators" title="Enlazar permanentemente con este título">¶</a></h3>
<p>For a parameter <span class="math notranslate nohighlight">\(\theta\)</span> and an estimator <span class="math notranslate nohighlight">\(\hat \theta\)</span>, if</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[\hat \theta] = \theta,
\]</div>
<p>then <span class="math notranslate nohighlight">\(\hat \theta\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\theta\)</span></p>
<p><strong>Is the MLE of <span class="math notranslate nohighlight">\(\mu\)</span> unbiased?</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbb{E}[\hat \mu] &amp;= \mathbb{E} \left[ \frac{1}{N} \sum_{i=1}^N x_i \right]  \nonumber \\
&amp;= \frac{1}{N} \sum_{i=1}^N \mathbb{E}[x_i] = \frac{1}{N} \sum_{i=1}^N \mu = \mu  \nonumber
\end{align}
\end{split}\]</div>
<blockquote>
<div><p>The answer is YES</p>
</div></blockquote>
<p><strong>Is the MLE of <span class="math notranslate nohighlight">\(\sigma^2\)</span> unbiased?</strong></p>
<p>First lets expand the expression of the MLE of the variance</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat \sigma^2 &amp;= \frac{1}{N} \sum_{i=1}^N \left(x_i- \frac{1}{N}\sum_{j=1}^N x_j \right)^2 \nonumber \\
&amp;= \frac{1}{N} \sum_{i=1}^N x_i^2 - \frac{1}{N^2} \sum_{i=1}^N \sum_{j=1}^N x_i  x_j \nonumber \\
&amp;= \frac{1}{N} \sum_{i=1}^N x_i^2 - \frac{1}{N^2} \sum_{i=1}^N \sum_{j\neq i} x_i x_j - \frac{1}{N^2} \sum_{i=1}^N x_i^2 \nonumber  \\
&amp;= \frac{N-1}{N^2} \sum_{i=1}^N x_i^2 - \frac{1}{N^2} \sum_{i=1}^N \sum_{j \neq i} x_i x_j  \nonumber
\end{align}
\end{split}\]</div>
<p>Then applying the expected value operator we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbb{E}[\hat \sigma^2] &amp;= \frac{N-1}{N^2} \sum_{i=1}^N \mathbb{E} [x_i^2] - \frac{1}{N^2} \sum_{i=1}^N \sum_{j \neq i} \mathbb{E} [x_i] \mathbb{E} [x_j] \nonumber  \\
&amp;= \frac{N-1}{N} (\sigma^2 + \mu^2) - \frac{N-1}{N} \mu^2 \nonumber \\
&amp;= \frac{N-1}{N} \sigma^2 \neq \sigma^2  \nonumber 
\end{align}
\end{split}\]</div>
<blockquote>
<div><p>The answer is NO</p>
</div></blockquote>
<p><strong>Can we correct for the bias?</strong></p>
<blockquote>
<div><p>In this case, YES!</p>
</div></blockquote>
<p>If we multiply it by <span class="math notranslate nohighlight">\(\frac{N}{N-1}\)</span> we obtain the well known <a class="reference external" href="https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation">unbiased estimator of the variance</a></p>
<div class="math notranslate nohighlight">
\[
\hat \sigma_{u}^2 = \frac{N}{N-1} \hat \sigma^2 = \frac{1}{N-1} \sum_{i=1}^N (x_i- \hat\mu)^2
\]</div>
</div>
<div class="section" id="example-mle-of-a-gaussian-mixture">
<h3><span class="section-number">6.3.4. </span>Example: MLE of a Gaussian mixture<a class="headerlink" href="#example-mle-of-a-gaussian-mixture" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Let’s imagine that our <em>iid</em> data come from a mixture of Gaussians with K components</p>
<div class="math notranslate nohighlight">
\[
f(x_i|\pi,\mu,\sigma^2) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \sigma_k^2),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sum_{k=1}^K \pi_k = 1\)</span> and <span class="math notranslate nohighlight">\(\pi_k \in [0, 1] ~~ \forall k\)</span></p>
<p>We can write the log likelihood as</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(\pi,\mu,\sigma^2) = \sum_{i=1}^N \log \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \sigma_k^2)
\]</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>In this case we cannot obtain analytical expressions for the parameters by setting the derivative to zero. We have to resort to iterative methods/optimizers, e.g. gradient descent, expectation maximization</p>
</div>
<p>We will come back to this in a future class on expectation maximization and Gaussian mixture models</p>
</div>
</div>
<div class="section" id="optimality-properties-and-uncertainty-of-mles">
<h2><span class="section-number">6.4. </span>Optimality properties and uncertainty of MLEs<a class="headerlink" href="#optimality-properties-and-uncertainty-of-mles" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Assuming that the data truly comes from the specified model the MLE is</p>
<p><strong>Consistent:</strong> The estimate converge to the true parameter as data points increase</p>
<div class="math notranslate nohighlight">
\[
\lim_{N\to \infty} \hat \theta = \theta
\]</div>
<p><strong>Asymptotically normal:</strong> The distribution of the estimate approaches a normal centered at the true parameter.</p>
<div class="math notranslate nohighlight">
\[
\lim_{N\to \infty} p(\hat \theta) = \mathcal{N}(\hat \theta | \theta, \sigma_\theta^2),
\]</div>
<p>which is a consequence of the <strong>central limit theorem</strong></p>
<p>For <em>i.i.d.</em> <span class="math notranslate nohighlight">\(\{X_i\}, i=1,\ldots,N\)</span> with <span class="math notranslate nohighlight">\(\mathbb{E}[X] &lt; \infty\)</span> and <span class="math notranslate nohighlight">\(\text{Var}[X] &lt; \infty\)</span> then</p>
<div class="math notranslate nohighlight">
\[
\lim_{N\to\infty} \sqrt{N} (\bar X - \mathbb{E}[X]) = \mathcal{N}(0, \sigma^2)
\]</div>
<p>Because MLE have asymptotically normal distributions the log likelihood ratio have asymptotically a <em>chi-square</em> distributions (more about this later)</p>
<p><strong>Minimum variance:</strong> The estimate achieve the theoretical minimal variance given by the <strong>Cramer-Rao bound</strong>. This bound is the inverse of the expected Fisher information, <em>i.e</em> the second derivative of <span class="math notranslate nohighlight">\(- \log L\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_{nm}^2 =  \left (- \frac{d^2 \log \mathcal{L} (\theta)}{d\theta_n \theta_m} \bigg\rvert_{\theta = \hat\theta}\right)^{-1}
\]</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma_{nm}^2\)</span> is the minimum variance achieved by an unbiased estimator.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_{nn}^2\)</span> gives the marginal error bars</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\sigma_{nm} \neq 0 ~~ n\neq m\)</span>, then errors are correlated, <em>i.e</em> some combinations of parameters might be better determined than others</p></li>
</ul>
</div>
<div class="section" id="example-cramer-rao-bound-for-the-mle-of-mu">
<h3><span class="section-number">6.4.1. </span>Example: Cramer-Rao bound for the MLE of <span class="math notranslate nohighlight">\(\mu\)</span><a class="headerlink" href="#example-cramer-rao-bound-for-the-mle-of-mu" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Considering a Gaussian likelihood from the previous example</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L} (\mu, \sigma^2) =  - \frac{N}{2} \log 2\pi\sigma^2 - \frac{1}{2\sigma^{2}}   \sum_{i=1}^N (x_i-\mu)^2 
\]</div>
<p>What is the uncertainty of the MLE of <span class="math notranslate nohighlight">\(\mu\)</span>? In this case the Cramer-rao bound</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\sigma_{\hat\mu}^2  &amp;= \left (- \frac{d^2 \log \mathcal{L}(\mu, \sigma^2)}{d\mu^2} \bigg\rvert_{\mu=\hat\mu}\right)^{-1}  \nonumber \\
&amp;=  \left (- \frac{1}{\sigma^2} \frac{d}{d\mu}  \sum_{i=1}^N (x-\mu) \bigg\rvert_{\mu=\hat\mu}\right)^{-1}  \nonumber \\
&amp;=  \left ( \frac{N}{\sigma^2}  \bigg\rvert_{\mu=\hat\mu}\right)^{-1} = \frac{\sigma^2}{N}  \nonumber
\end{align}
\end{split}\]</div>
<p>an expression that is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Standard_error">standard error of the mean</a></p>
<p>Then we have</p>
<div class="math notranslate nohighlight">
\[
p(\hat \mu) \to \mathcal{N}(\hat \mu| \mu, \sigma^2/N)
\]</div>
<p>Let’s see how the MLE and its uncertainty changes as more data is observed</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mu_real</span><span class="p">,</span> <span class="n">s_real</span> <span class="o">=</span> <span class="mf">2.23142</span><span class="p">,</span> <span class="mf">1.124123</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mu_real</span> <span class="o">+</span> <span class="n">s_real</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># MLE and its standard error</span>
<span class="n">hat_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))])</span>
<span class="n">standard_error</span> <span class="o">=</span> <span class="n">s_real</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Curve</span><span class="p">((</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">hat_mu</span><span class="p">),</span> 
                   <span class="n">kdims</span><span class="o">=</span><span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="n">vdims</span><span class="o">=</span><span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated mu&#39;</span><span class="p">)</span>
<span class="n">se_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Spread</span><span class="p">((</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">hat_mu</span><span class="p">,</span> <span class="n">standard_error</span><span class="p">),</span> 
                    <span class="n">kdims</span><span class="o">=</span><span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Standard error&#39;</span><span class="p">)</span>
<span class="n">mu_real_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">mu_real</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">line_dash</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="p">(</span><span class="n">mu_plot</span><span class="o">*</span><span class="n">se_plot</span><span class="o">*</span><span class="n">mu_real_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">hv</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">Curve</span><span class="p">(</span><span class="n">logx</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_57680</span><span class="o">/</span><span class="mf">2828847862.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">mu_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Curve</span><span class="p">((</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">hat_mu</span><span class="p">),</span> 
<span class="g g-Whitespace">      </span><span class="mi">2</span>                    <span class="n">kdims</span><span class="o">=</span><span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="n">vdims</span><span class="o">=</span><span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated mu&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">se_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">Spread</span><span class="p">((</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">hat_mu</span><span class="p">,</span> <span class="n">standard_error</span><span class="p">),</span> 
<span class="g g-Whitespace">      </span><span class="mi">4</span>                     <span class="n">kdims</span><span class="o">=</span><span class="s1">&#39;Number of samples&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Standard error&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">mu_real_plot</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">mu_real</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">line_dash</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;hv&#39; is not defined
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="hypothesis-tests-based-on-the-likelihood">
<h2><span class="section-number">6.5. </span>Hypothesis tests based on the likelihood<a class="headerlink" href="#hypothesis-tests-based-on-the-likelihood" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Considering the asymptotic distributions shown before we can formulate a hypothesis test for the MLE of <span class="math notranslate nohighlight">\(\theta\)</span></p>
<p>We will present the Wald-test and the Wilks test</p>
<div class="section" id="wald-test">
<h3><span class="section-number">6.5.1. </span>Wald-test<a class="headerlink" href="#wald-test" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Suppose we wish to test</p>
<div class="math notranslate nohighlight">
\[
\mathcal{H}_0: \theta = \theta_0
\]</div>
<div class="math notranslate nohighlight">
\[
\mathcal{H}_A: \theta \neq \theta_0
\]</div>
<p>Under the null we can write</p>
<div class="math notranslate nohighlight">
\[
W = \frac{(\hat \theta - \theta_0)^2}{\left (- \frac{d^2 \log \mathcal{L} (\theta)}{d\theta^2} \bigg\rvert_{\theta = \hat\theta}\right)^{-1}} = (\hat \theta - \theta_0)^2 \sigma_{\hat \theta}^2 \to \chi^2_1
\]</div>
<p>The test statistic have a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with one degree of freedom</p>
<p>If <span class="math notranslate nohighlight">\(W\)</span> is greather than the <span class="math notranslate nohighlight">\((1-\alpha)100\%\)</span> quantile of <span class="math notranslate nohighlight">\(\chi^2_1\)</span> we reject the null hypothesis</p>
</div>
<div class="section" id="log-likelihood-ratio-test-or-wilks-test">
<h3><span class="section-number">6.5.2. </span>Log-likelihood ratio test or Wilks test<a class="headerlink" href="#log-likelihood-ratio-test-or-wilks-test" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Suppose we wish to test</p>
<div class="math notranslate nohighlight">
\[
\mathcal{H}_0: \theta = \theta_0
\]</div>
<div class="math notranslate nohighlight">
\[
\mathcal{H}_A: \theta =\theta_1
\]</div>
<p>We can write a ratio between likelihoods</p>
<div class="math notranslate nohighlight">
\[
\lambda(\mathcal{D}) = \frac{\mathcal{L}(\theta_0|\mathcal{D})}{\mathcal{L}(\theta_1|\mathcal{D})} 
\]</div>
<p>Asymptotically, under the null, we have</p>
<div class="math notranslate nohighlight">
\[
-2 \log \lambda(\mathcal{D}) \to \chi^2_1
\]</div>
<p>If <span class="math notranslate nohighlight">\(-2 \log \lambda(\mathcal{D})\)</span> is greather than the <span class="math notranslate nohighlight">\((1-\alpha)100\%\)</span> quantile of <span class="math notranslate nohighlight">\(\chi^2_1\)</span> we reject the null</p>
</div>
</div>
<div class="section" id="criteria-for-model-comparison">
<h2><span class="section-number">6.6. </span>Criteria for model comparison<a class="headerlink" href="#criteria-for-model-comparison" title="Enlazar permanentemente con este título">¶</a></h2>
<p>How to compare models with different number of parameters? In general the more number of parameters the better the fit (overfitting). The likelihood does not take into account the complexity (number of parameters) of the model</p>
<blockquote>
<div><p>How to score models taking into account their complexity?</p>
</div></blockquote>
<p>One option is to use the Akaike information criterion (AIC). For a model with <span class="math notranslate nohighlight">\(k\)</span> parameters and N data points the AIC is</p>
<div class="math notranslate nohighlight">
\[
\text{AIC} = -2 \log \mathcal{L}(\hat \theta) + 2k + \frac{2k(k+1)}{N-k-1},
\]</div>
<p>which one seeks to minimize. The AIC combines the likelihood (score) of the model and its complexity. The AIC is based on an asumptotic approximation which we will review in the future</p>
<p>This is also related to the idea of regularization, which will also be reviewed in future lectures</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lectures/2_statistical_modeling"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../1_TeoriaProbabilidades/parte5.html" title="previous page"><span class="section-number">5. </span>Teoremas Asintóticos</a>
    <a class='right-next' id="next-link" href="part2.html" title="next page"><span class="section-number">7. </span>Nonparametric modeling</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          Por Pablo Huijse and Eliana Scheihing<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>