
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5. Linear regression &#8212; Herramientas estadísticas para la investigación</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7. Gaussian Mixture Models" href="../9_mixture_models/lecture.html" />
    <link rel="prev" title="4. Inferencia estadística con tests no-paramétricos" href="../4_nonparametric_inference/lecture.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Herramientas estadísticas para la investigación</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Statistical modeling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../2_statistical_modeling/part1.html">
   1. Parametric modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2_statistical_modeling/part2.html">
   2. Nonparametric modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../2_statistical_modeling/part3.html">
   3. Bayesian modeling
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical thinking
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../4_nonparametric_inference/lecture.html">
   4. Inferencia estadística con tests no-paramétricos
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   5. Linear regression
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Exploratory analysis
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../9_mixture_models/lecture.html">
   7. Gaussian Mixture Models
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian modeling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../11_MCMC/lecture.html">
   8. Markov Chain Monte Carlo
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/lectures/5_linear_regression/lecture.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/magister-informatica-uach/INFO337"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/magister-informatica-uach/INFO337/master?urlpath=tree/lectures/5_linear_regression/lecture.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   5. Linear regression
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     5.1. Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-regression">
     5.2. Defining regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-context-for-regression">
       5.2.1. A context for regression
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#parametric-vs-non-parametric-regression">
       5.2.2. Parametric vs non-parametric  regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parametric-models-for-regression">
     5.3. Parametric models for regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-models-for-regression">
       5.3.1. Linear models for regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-simplest-linear-model-two-dimensions">
     5.4. The simplest linear model (two-dimensions)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-the-simplest-linear-model">
       5.4.1. Fitting the simplest linear model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-a-line-using-scipy-stats-linregress">
       5.4.2. Fitting a line using
       <code class="docutils literal notranslate">
        <span class="pre">
         scipy.stats.linregress
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quality-of-the-linear-fit">
       5.4.3. Quality of the linear fit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ordinary-least-squares-ols">
     5.5. Ordinary Least Squares (OLS)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fitting-an-hyperplane-using-np-linalg">
       5.5.1. Fitting an hyperplane using
       <code class="docutils literal notranslate">
        <span class="pre">
         np.linalg
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#numerical-considerations-of-the-ls-solution">
       5.5.2. Numerical considerations of the LS solution
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#quality-of-d-dimensional-linear-fit">
       5.5.3. Quality of D-dimensional linear fit
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-view-of-ols">
     5.6. Statistical view of OLS
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-likelihood-of-spherical-gaussian-and-least-squares">
       5.6.1. Maximum Likelihood of Spherical Gaussian and Least Squares
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#statistical-properties-of-ols">
       5.6.2. Statistical properties of OLS
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inference-with-ols">
       5.6.3. Inference with OLS
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hypothesis-test-on-the-regression-coefficients">
       5.6.4. Hypothesis test on the regression coefficients
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#checking-assumptions">
       5.6.5. Checking assumptions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighted-least-squares-wls">
     5.7. Weighted Least Squares (WLS)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-models-and-basis-functions">
     5.8. Linear models and Basis functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-polynomials">
       5.8.1. Example: Polynomials
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-transformations">
       5.8.2. Example: Transformations
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-interactions-between-variables">
       5.8.3. Example: Interactions between variables
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-the-simplest-linear-model">
       5.8.4. Example: The simplest linear model
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polynomial-regression">
     5.9. Polynomial regression
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting-and-regularization">
     5.10. Overfitting and regularization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-bias-variance-trade-off">
       5.10.1. The Bias-Variance trade-off
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-map-least-squares-and-ridge-regression">
     5.11. Bayesian (MAP) Least Squares and Ridge regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-ridge-regression-with-sklearn">
       5.11.1. Example: Ridge Regression with sklearn
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#independence-and-correlation">
     5.12. Independence and correlation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extra-topics">
     5.13. Extra topics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#causality-beyond-correlation">
   6. Causality: Beyond correlation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#judea-pearl-the-new-science-of-cause-and-effect">
     6.1. Judea Pearl - The New Science of Cause and Effect
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-why">
     6.2. Do Why
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> notebook
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">Button</span><span class="p">,</span> <span class="n">Output</span><span class="p">,</span> <span class="n">Box</span><span class="p">,</span> <span class="n">IntSlider</span><span class="p">,</span> <span class="n">SelectionSlider</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="linear-regression">
<h1><span class="section-number">5. </span>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<p>References for today’s lecture:</p>
<ol class="simple">
<li><p>Bishop, “Pattern recognition and machine learning”, <strong>Chapter 3</strong></p></li>
<li><p>Hastie, Tibshirani and Friedman, “The elements of statistical learning” 2nd Ed., <em>Springer</em>, <strong>Chapter 3</strong></p></li>
<li><p>Murphy, “Machine Learning: A Probabilistic Perspective”, <em>MIT Press</em>, 2012, <strong>Chapter 7</strong></p></li>
</ol>
<div class="section" id="introduction">
<h2><span class="section-number">5.1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Many problems can be posed as “finding a relation” between factors/variables</p>
<p>This can be interpreted as predicting and/or explaining a variable given others</p>
<ul class="simple">
<li><p>Sales given money spent in advertising</p></li>
<li><p>Chance to rain in Valdivia given temperature, pressure and humidity</p></li>
<li><p>Gasoline consumption of a car given acceleration, weight and number of cylinders</p></li>
<li><p>Chance to get lung cancer given number of smoked cigarettes per day, age and gender</p></li>
</ul>
<p>We could ask</p>
<ul class="simple">
<li><p>Are these variable related?</p></li>
<li><p>How strong/significant is the relationship?</p></li>
<li><p>What is the nature of the relationship?</p></li>
</ul>
<p>Answering these helps us <strong>understand the underlying processes behind the data</strong></p>
</div>
<div class="section" id="defining-regression">
<h2><span class="section-number">5.2. </span>Defining regression<a class="headerlink" href="#defining-regression" title="Permalink to this headline">¶</a></h2>
<p><strong>Regression</strong> refers to a family of statistical methods to find <strong>relationships</strong> between <strong>variables</strong></p>
<p>In general the relation is modeled as a function <span class="math notranslate nohighlight">\(g(\cdot)\)</span> that maps two types of variables</p>
<ul class="simple">
<li><p>The input variable <span class="math notranslate nohighlight">\(X\)</span> is called <strong>independent variable</strong> or feature</p></li>
<li><p>The output variable <span class="math notranslate nohighlight">\(Y\)</span> is called <strong>dependent variable</strong>, response or target</p></li>
</ul>
<p>The mapping or function <span class="math notranslate nohighlight">\(g\)</span> is called <strong>predictor</strong> or <strong>regressor</strong></p>
<div class="math notranslate nohighlight">
\[
g: X \to Y
\]</div>
<p>The objective is to learn <span class="math notranslate nohighlight">\(g\)</span> such that we can predict <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X\)</span>, <em>i.e.</em> <span class="math notranslate nohighlight">\(\mathbb{E}[Y|X]\)</span></p>
<div class="section" id="a-context-for-regression">
<h3><span class="section-number">5.2.1. </span>A context for regression<a class="headerlink" href="#a-context-for-regression" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Regression</strong> can be defined from an statistical perspective as a special case of model fitting (parameter estimation)</p></li>
<li><p>In many books <strong>Regression</strong> is defined from a pure-optimization perspective (deterministic)</p></li>
<li><p><strong>Regression</strong> is considered part of the <em>supervised learning</em> paradigm. The difference between <strong>Regression</strong> and <em>classification</em> is the nature of the dependent variable (continuous vs categorical)</p></li>
</ul>
</div>
<div class="section" id="parametric-vs-non-parametric-regression">
<h3><span class="section-number">5.2.2. </span>Parametric vs non-parametric  regression<a class="headerlink" href="#parametric-vs-non-parametric-regression" title="Permalink to this headline">¶</a></h3>
<p>Regression methods can be broadly classified as either parametric or non-parametric</p>
<p>In parametric regression (<strong>this lecture</strong>)</p>
<ul class="simple">
<li><p>We know the model of the regressor</p></li>
<li><p>The model has a finite number of parameters</p></li>
<li><p>The parameters of the model are all we need to do predictions</p></li>
<li><p>Simpler but with bigger assumptions (inductive bias)</p></li>
</ul>
<p>In nonparametric regression</p>
<ul class="simple">
<li><p>There is no functional form for the regressor</p></li>
<li><p>It can have an infinite number of parameters (and a finite number of hyperparameters)</p></li>
<li><p>The regressor is defined from the training data</p></li>
<li><p>More flexible but requires more data to fit it</p></li>
<li><p>Examples: Splines, Support vector regression, Gaussian processes</p></li>
</ul>
</div>
</div>
<div class="section" id="parametric-models-for-regression">
<h2><span class="section-number">5.3. </span>Parametric models for regression<a class="headerlink" href="#parametric-models-for-regression" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a continuous D-dimensional variable (feature) and <span class="math notranslate nohighlight">\(Y\)</span> be a continuous unidimensional variable (target)</p>
<p>Let <span class="math notranslate nohighlight">\(\{x_i, y_i\}\)</span> with <span class="math notranslate nohighlight">\(i=1,\ldots,N\)</span> be a set of <span class="math notranslate nohighlight">\(N\)</span> <em>iid</em> observations of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p>
<p>Let <span class="math notranslate nohighlight">\(g_\theta\)</span> be a model with a M-dimensional parameter <span class="math notranslate nohighlight">\(\theta\)</span></p>
<p>Then we can define parametric regression as finding a value of <span class="math notranslate nohighlight">\(\theta\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
y_i \approx g_\theta(x_i),\quad i=1,\ldots, N
\]</div>
<div class="section" id="linear-models-for-regression">
<h3><span class="section-number">5.3.1. </span>Linear models for regression<a class="headerlink" href="#linear-models-for-regression" title="Permalink to this headline">¶</a></h3>
<p>The simplest parametric model is the <strong>linear model</strong>. A linear model gives rise to <strong>linear regression</strong></p>
<p>The linear model is linear on <span class="math notranslate nohighlight">\(\theta\)</span> but not necessarily on <span class="math notranslate nohighlight">\(X\)</span></p>
<p>For example a model with unidimensional input</p>
<div class="math notranslate nohighlight">
\[
g_\theta \left(x_i \right) = \theta_0 + \theta_1 x_i  + \theta_2 x_i^2,
\]</div>
<p>is a linear model and</p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \theta_0 + \theta_1 \log(x_i),
\]</div>
<p>is also a linear model but</p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \theta_0 + \log(x_i + \theta_1),
\]</div>
<p>is not a linear model</p>
</div>
</div>
<div class="section" id="the-simplest-linear-model-two-dimensions">
<h2><span class="section-number">5.4. </span>The simplest linear model (two-dimensions)<a class="headerlink" href="#the-simplest-linear-model-two-dimensions" title="Permalink to this headline">¶</a></h2>
<p>If we consider a one-dimensional variable <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}, i=1,\ldots,N\)</span>, then the simplest linear model is</p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \theta_0 + \theta_1 x_i
\]</div>
<p>which has <span class="math notranslate nohighlight">\(M=2\)</span> parameters.</p>
<p>This corresponds to a line in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> and we recognize</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta_0\)</span> as the intercept</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_1\)</span> as the slope</p></li>
</ul>
<p>If we consider a two-dimensional variable <span class="math notranslate nohighlight">\(x_i = (x_{i1}, x_{i2}) \in \mathbb{R}^2, i=1,\ldots,N\)</span> then we obtain</p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \theta_0 + \theta_1 x_{i1} + \theta_2 x_{i2}
\]</div>
<p>which has <span class="math notranslate nohighlight">\(M=3\)</span> parameters. This corresponds to a plane in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span></p>
<p>The most general form assumes a D-dimensional variable <span class="math notranslate nohighlight">\(x_i = (x_{i1}, x_{i2}, \ldots, x_{iD}), i=1,\ldots,N\)</span></p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \theta_0 + \sum_{j=1}^D \theta_j x_{ij}
\]</div>
<p>which has <span class="math notranslate nohighlight">\(M=D+1\)</span> parameters, which corresponds to an hyperplane in <span class="math notranslate nohighlight">\(\mathbb{R}^M\)</span></p>
<div class="section" id="fitting-the-simplest-linear-model">
<h3><span class="section-number">5.4.1. </span>Fitting the simplest linear model<a class="headerlink" href="#fitting-the-simplest-linear-model" title="Permalink to this headline">¶</a></h3>
<p>Assuming that we have <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i=1,\ldots,N}\)</span> <em>iid</em> observations from unidimensional variables X and Y</p>
<blockquote>
<div><p>How do we find <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span> such that <span class="math notranslate nohighlight">\(y_i \approx \theta_0 + \theta_1 x_i, \forall i\)</span>?</p>
</div></blockquote>
<p>We can start by writing the squared residual (error) as</p>
<div class="math notranslate nohighlight">
\[
E_i^2 = (y_i - \theta_0 - \theta_1 x_i)^2
\]</div>
<p>We would like to make the sum of squared errors as small as we possible so we seek</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta} L = \sum_{i=1}^N E_i^2 = \sum_{i=1}^N (y_i - \theta_0 - \theta_1 x_i)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span>, the sum of squares errors, is a our loss/cost function</p>
<p>From here we can do</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{dL}{d\theta_0} &amp;= -2 \sum_{i=1}^N (y_i - \theta_0 - \theta_1 x_i) \nonumber \\
&amp;= -2 \sum_{i=1}^N y_i +  2 N\theta_0 + 2 \theta_1 \sum_{i=1}^N x_i = 0 \nonumber
\end{align}
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{dL}{d\theta_1} &amp;= -2 \sum_{i=1}^N (y_i - \theta_0 - \theta_1 x_i) x_i \nonumber \\
&amp;= -2 \sum_{i=1}^N y_i x_i +  2 \theta_0 \sum_{i=1}^N x_i + 2 \theta_1 \sum_{i=1}^N x_i^2 = 0 \nonumber
\end{align}
\end{split}\]</div>
<p>a system of two equations and two unknowns</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} N &amp; \sum_i x_i \\ \sum_i x_i &amp; \sum_i x_i^2\\\end{pmatrix} \begin{pmatrix} \theta_0 \\ \theta_1 \end{pmatrix}  = \begin{pmatrix} \sum_i y_i \\ \sum_i x_i y_i \end{pmatrix} 
\end{split}\]</div>
<p>whose solution is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} \hat \theta_0 \\ \hat \theta_1 \end{pmatrix}  = 
\frac{1}{N\sum_i x_i^2 - (\sum_i x_i)^2}\begin{pmatrix} \sum_i x_i^2 &amp; -\sum_i x_i \\ -\sum_i x_i &amp; N\\\end{pmatrix}  
\begin{pmatrix} \sum_i y_i \\ \sum_i x_i y_i \end{pmatrix} 
\end{split}\]</div>
<p>where we assume that the determinant complies with: <span class="math notranslate nohighlight">\(N\sum_i x_i^2 - (\sum_i x_i)^2 \neq 0\)</span></p>
<p>Note that in this case</p>
<div class="math notranslate nohighlight">
\[
\hat \theta_1 = \frac{\text{Cov}[x, y]}{\text{Var}[x]}, \hat \theta_0 = \bar y - \hat \theta_1 \bar x
\]</div>
</div>
<div class="section" id="fitting-a-line-using-scipy-stats-linregress">
<h3><span class="section-number">5.4.2. </span>Fitting a line using <code class="docutils literal notranslate"><span class="pre">scipy.stats.linregress</span></code><a class="headerlink" href="#fitting-a-line-using-scipy-stats-linregress" title="Permalink to this headline">¶</a></h3>
<p>For unidimensional <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> we can use <code class="docutils literal notranslate"><span class="pre">scipy</span></code> to fit a line</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;theta0: </span><span class="si">{</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">0.5f</span><span class="si">}</span><span class="s2">, theta1: </span><span class="si">{</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">0.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">res</span><span class="o">.</span><span class="n">intercept</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">slope</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;hat theta0: </span><span class="si">{</span><span class="n">theta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">0.5f</span><span class="si">}</span><span class="s2">, hat theta1: </span><span class="si">{</span><span class="n">theta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">0.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;r: </span><span class="si">{</span><span class="n">res</span><span class="o">.</span><span class="n">rvalue</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">rowspan</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;underlying&#39;</span><span class="p">);</span> 
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">);</span> 
<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">-</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>This is equivalent to the formulas we derived:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_line_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">sum_x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">);</span> 
    <span class="n">sum_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sum_xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">);</span> 
    <span class="n">sum_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="p">[[</span><span class="n">sum_x2</span><span class="p">,</span> <span class="o">-</span><span class="n">sum_x</span><span class="p">],[</span><span class="o">-</span><span class="n">sum_x</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)]]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="n">sum_y</span><span class="p">,</span> <span class="n">sum_xy</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">sum_x2</span> <span class="o">-</span> <span class="n">sum_x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">fit_line_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="quality-of-the-linear-fit">
<h3><span class="section-number">5.4.3. </span>Quality of the linear fit<a class="headerlink" href="#quality-of-the-linear-fit" title="Permalink to this headline">¶</a></h3>
<p>Can we measure how strong is <span class="math notranslate nohighlight">\(y\)</span> related to <span class="math notranslate nohighlight">\(\hat y = \hat \theta_0 + \hat \theta_1 x\)</span>?</p>
<p>YES: <strong>Coefficient of determination</strong> or <span class="math notranslate nohighlight">\(r^2\)</span>
$<span class="math notranslate nohighlight">\(
r^2 = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar y_i)^2}
\)</span>$</p>
<ul class="simple">
<li><p>one minus the Sum of residuals divided by the variance of y</p></li>
<li><p><span class="math notranslate nohighlight">\(r\)</span> is equivalent to Pearson’s correlation coefficient</p></li>
<li><p><span class="math notranslate nohighlight">\(r^2 \in [0, 1]\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(r^2 = 1\)</span>, the data points are fitted perfectly by the model. The regressor accounts for all of the variation in y</p></li>
<li><p>If <span class="math notranslate nohighlight">\(r^2 = 0\)</span>, the regression line is horizontal. The regressor accounts for none of the variation in y</p></li>
<li><p>If the relation is strong but non-linear it will not be detected by <span class="math notranslate nohighlight">\(r^2\)</span></p></li>
</ul>
</div>
</div>
<div class="section" id="ordinary-least-squares-ols">
<h2><span class="section-number">5.5. </span>Ordinary Least Squares (OLS)<a class="headerlink" href="#ordinary-least-squares-ols" title="Permalink to this headline">¶</a></h2>
<p>Now we will generalize the previous solution assuming that we have <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i=1,\ldots,N}\)</span> <em>iid</em> observations from unidimensional  Y and <strong>D-dimensional variable</strong> X</p>
<blockquote>
<div><p>How do we find <span class="math notranslate nohighlight">\(\theta\)</span>  such that <span class="math notranslate nohighlight">\(y_i \approx \theta_0 + \sum_{j=1}^D \theta_j x_{ij}, \forall i\)</span>?</p>
</div></blockquote>
<p>We can write the sum of squared errors (residuals) as</p>
<div class="math notranslate nohighlight">
\[
\min_\theta L = \sum_{i=1}^N (y_i - \theta_0 - \sum_{j=1}^D \theta_j x_{ij})^2
\]</div>
<p>For simplicity consider the matrices
$<span class="math notranslate nohighlight">\(
X = \begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1D} \\ 
1 &amp; x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2D} \\
1 &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{N1} &amp; x_{N2} &amp; \ldots &amp; x_{ND} \end{pmatrix},  Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{pmatrix}, \theta =  \begin{pmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_D \end{pmatrix}
\)</span>$</p>
<p>With these we can write the cost function in matrix form as</p>
<div class="math notranslate nohighlight">
\[
\min_\theta  L = \| Y - X \theta \|^2 = (Y - X \theta)^T (Y - X \theta)
\]</div>
<p>From here we can do</p>
<div class="math notranslate nohighlight">
\[
\frac{dL}{d\theta} = -(Y - X \theta)^T X =  -X^T (Y - X \theta) = 0
\]</div>
<p>to obtain the <strong>normal equations</strong></p>
<div class="math notranslate nohighlight">
\[
X^T X \theta  = X^T Y
\]</div>
<p>and if <span class="math notranslate nohighlight">\(X^T X\)</span> is invertible then</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = (X^T X)^{-1} X^T Y
\]</div>
<p>which  is called the <strong>least squares (LS) estimator</strong> of <span class="math notranslate nohighlight">\(\theta\)</span></p>
<div class="section" id="fitting-an-hyperplane-using-np-linalg">
<h3><span class="section-number">5.5.1. </span>Fitting an hyperplane using <code class="docutils literal notranslate"><span class="pre">np.linalg</span></code><a class="headerlink" href="#fitting-an-hyperplane-using-np-linalg" title="Permalink to this headline">¶</a></h3>
<p>We can find <span class="math notranslate nohighlight">\(\theta\)</span> by solving the normal equations with <code class="docutils literal notranslate"><span class="pre">np.linalg.lstsq</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">];</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">10.</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
<span class="n">XY1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="n">X1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">X2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span><span class="o">.</span><span class="n">T</span>

<span class="n">Y_clean</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X1</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">X2</span> 
<span class="n">Y</span> <span class="o">=</span> <span class="n">Y_clean</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">Y_clean</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">param</span><span class="p">,</span> <span class="n">MSE</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">singval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">XY1</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Y_clean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;underlying&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">param</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">param</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X1</span> <span class="o">+</span> <span class="n">param</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">X2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>For the two-dim linear model we found these normal equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} N &amp; \sum_i x_i \\ \sum_i x_i &amp; \sum_i x_i^2\\\end{pmatrix} \begin{pmatrix} \theta_0 \\ \theta_1 \end{pmatrix}  = \begin{pmatrix} \sum_i y_i \\ \sum_i x_i y_i \end{pmatrix} 
\end{split}\]</div>
<p>and they can be written as
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-6a570ad9-6c6f-4a2f-b472-28d81c1dc323">
<span class="eqno">(5.1)<a class="headerlink" href="#equation-6a570ad9-6c6f-4a2f-b472-28d81c1dc323" title="Permalink to this equation">¶</a></span>\[\begin{align}
\begin{pmatrix} 1 &amp; 1 &amp; \ldots &amp; 1 \\ x_1 &amp; x_2 &amp; \ldots &amp; x_N \end{pmatrix} 
\begin{pmatrix} 1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots &amp; \vdots \\ 1 &amp; x_N \end{pmatrix} 
\begin{pmatrix} \theta_0 \\ \theta_1 \end{pmatrix}  &amp;= 
\begin{pmatrix} 1 &amp; 1 &amp; \ldots &amp; 1 \\ x_1 &amp; x_2 &amp; \ldots &amp; x_N \end{pmatrix} 
\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{pmatrix} \nonumber \\
X^T X \theta &amp;= X^T Y \nonumber
\end{align}\]</div>
<p>$$
the same form as the general case</p>
</div>
<div class="section" id="numerical-considerations-of-the-ls-solution">
<h3><span class="section-number">5.5.2. </span>Numerical considerations of the LS solution<a class="headerlink" href="#numerical-considerations-of-the-ls-solution" title="Permalink to this headline">¶</a></h3>
<p>The LS estimator is</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = (X^T X)^{-1} X^T Y
\]</div>
<p>The LS estimator is a batch solution, <em>i.e.</em> it uses all observations at once</p>
<ul class="simple">
<li><p>If N is large it might be impossible to obtain it</p></li>
<li><p>In that case it might be better to use stochastic gradient descent</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(X^{\dagger} = (X^T X)^{-1} X^T \)</span> is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse"><em>Moore-Penrose</em></a> pseudo-inverse:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X^{\dagger}\)</span> is the inverse for matrices that are not squared</p></li>
<li><p>If X is squared and invertible then <span class="math notranslate nohighlight">\(X^{\dagger} = (X^T X)^{-1} X^T  = X^{-1} (X^T)^{-1} X^T = X^{-1}\)</span></p></li>
</ul>
<p>The LS estimator is valid only if we can invert <span class="math notranslate nohighlight">\(X^T X\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X^T X\)</span> is a squared symmetric <span class="math notranslate nohighlight">\(M\times M\)</span> matrix</p></li>
<li><p>If <span class="math notranslate nohighlight">\((X^T X)^{-1}\)</span> exists the LS solution is unique and the problem is <em>well-posed</em></p></li>
<li><p>If <span class="math notranslate nohighlight">\(X^T X\)</span> is positive definite, <em>i.e.</em> <span class="math notranslate nohighlight">\(z^T X^T X z &gt; 0, \forall z\)</span> then its inverse exist</p></li>
<li><p><span class="math notranslate nohighlight">\(X^T X\)</span> being positive definite is equivalent to each of the following</p>
<ul>
<li><p>The columns of <span class="math notranslate nohighlight">\(X\)</span> are Linearly Independent (LI)</p></li>
<li><p>All the eigenvalues of <span class="math notranslate nohighlight">\(X^T X\)</span> are positive</p></li>
<li><p>The Cholesky decomposition of <span class="math notranslate nohighlight">\(X^T X\)</span> exists</p></li>
</ul>
</li>
</ul>
<p><strong>Trick of the trade:</strong> Add a small value to the diagonal of <span class="math notranslate nohighlight">\(X^T X\)</span> to make it invertible. We will see later that this is equivalent to regularization</p>
</div>
<div class="section" id="quality-of-d-dimensional-linear-fit">
<h3><span class="section-number">5.5.3. </span>Quality of D-dimensional linear fit<a class="headerlink" href="#quality-of-d-dimensional-linear-fit" title="Permalink to this headline">¶</a></h3>
<p>In this case the <strong>Coefficient of determination</strong> or <span class="math notranslate nohighlight">\(r^2\)</span> can be written as
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-10131f73-ad3a-463b-8a72-1fd02c9467b2">
<span class="eqno">(5.2)<a class="headerlink" href="#equation-10131f73-ad3a-463b-8a72-1fd02c9467b2" title="Permalink to this equation">¶</a></span>\[\begin{align}
r^2 &amp;= 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar y_i)^2} \nonumber \\
&amp;= 1 - \frac{Y^T(I-X(X^TX)^{-1}X^T)Y}{Y^T (I - \frac{1}{N} \mathbb{1}^T \mathbb{1} ) Y} \nonumber \\
&amp;= 1 - \frac{SS_{res}}{SS_{total}} \nonumber
\end{align}\]</div>
<p>$<span class="math notranslate nohighlight">\(
where \)</span>\mathbb{1} = (1, 1, \ldots, 1)$</p>
<ul class="simple">
<li><p>It has the same interpretation as explained before</p></li>
<li><p>Review the previous class on ANOVA and reflect on the similarities, How do we get F distributions from <span class="math notranslate nohighlight">\(SS_{res}\)</span> and <span class="math notranslate nohighlight">\(SS_{total}\)</span>?</p></li>
</ul>
</div>
</div>
<div class="section" id="statistical-view-of-ols">
<h2><span class="section-number">5.6. </span>Statistical view of OLS<a class="headerlink" href="#statistical-view-of-ols" title="Permalink to this headline">¶</a></h2>
<p>Up to now we have viewed regression from a deterministic (optimization) perspective</p>
<p>To understand the properties and do inference we seek an statistical interpretation</p>
<p>Assuming that we have <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i=1,\ldots,N}\)</span> iid observations from unidimensional  Y and <strong>D-dimensional variable</strong> X</p>
<p>Let’s assume that our measurements of <span class="math notranslate nohighlight">\(Y\)</span> consists of the <strong>true model</strong> plus <strong>white Gaussian noise</strong>, <em>i.e.</em></p>
<div class="math notranslate nohighlight">
\[
y_i = \theta_0 + \sum_{j=1}^D \theta_j x_{ij} + \varepsilon_i, \forall i \quad \text{and} \quad \varepsilon_i \sim \mathcal{N}(0, \sigma^2)
\]</div>
<p>Then we can write the log likelihood of <span class="math notranslate nohighlight">\(\theta\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log L(\theta) &amp;= \log \prod_{i=1}^N \mathcal{N}(y_i | (1, x_{i1}, \ldots, x_{iD}) \theta, \sigma^2) \nonumber \\
&amp;= \sum_{i=1}^N \log \mathcal{N}(y_i | (1, x_{i1}, \ldots, x_{iD}) \theta, \sigma^2) \nonumber \\
&amp;= -\frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - (1, x_{i1}, \ldots, x_{iD}) \theta)^2,\nonumber \\
&amp;= -\frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (Y-X\theta)^T (Y - X\theta) \nonumber 
\end{align}
\end{split}\]</div>
<div class="section" id="maximum-likelihood-of-spherical-gaussian-and-least-squares">
<h3><span class="section-number">5.6.1. </span>Maximum Likelihood of Spherical Gaussian and Least Squares<a class="headerlink" href="#maximum-likelihood-of-spherical-gaussian-and-least-squares" title="Permalink to this headline">¶</a></h3>
<p>Given that we know <span class="math notranslate nohighlight">\(\sigma &gt; 0\)</span> then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\max_\theta \log L(\theta) &amp;= -\frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (Y-X\theta)^T (Y - X\theta) \nonumber \\
&amp;= - \frac{1}{2\sigma^2} (Y-X\theta)^T (Y - X\theta) \nonumber 
\end{align}
\end{split}\]</div>
<p>We can change the sign and transform <em>max</em> to <em>min</em>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\min_\theta \log L(\theta) &amp;=  \frac{1}{2\sigma^2} (Y-X\theta)^T (Y - X\theta) \nonumber \\
\implies \hat \theta &amp;= (X^T X)^{-1} X^T Y \nonumber 
\end{align}
\end{split}\]</div>
<blockquote>
<div><p>In this case the <strong>MLE solution is equivalent to least squares</strong></p>
</div></blockquote>
</div>
<div class="section" id="statistical-properties-of-ols">
<h3><span class="section-number">5.6.2. </span>Statistical properties of OLS<a class="headerlink" href="#statistical-properties-of-ols" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\varepsilon = (\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_N)\)</span>, where <span class="math notranslate nohighlight">\(\varepsilon_i \sim \mathcal{N}(0, \sigma^2) \forall i\)</span> and assume that X is not random</p>
<p>Is the estimator unbiased?</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbb{E}[\hat \theta] &amp;= \mathbb{E}[(X^T X)^{-1} X^T Y] \nonumber \\
&amp;= \mathbb{E}[(X^T X)^{-1} X^T (X \theta + \varepsilon)] \nonumber \\
&amp;= \mathbb{E}[\theta] + (X^T X)^{-1} X^T \mathbb{E}[\varepsilon] \\
&amp; = \mathbb{E}[\theta]
\end{align}
\end{split}\]</div>
<p>YES!</p>
<p>What is the variance of the estimator?</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbb{E}[(\hat \theta - \mathbb{E}[\hat\theta])(\hat \theta - \mathbb{E}[\hat\theta])^T] &amp;= \mathbb{E}[((X^T X)^{-1} X^T \varepsilon) ((X^T X)^{-1} X^T \varepsilon)^T] \nonumber \\
&amp;= (X^T X)^{-1} X^T  \mathbb{E}[\varepsilon \varepsilon^T] X ((X^T X)^{-1})^T  \nonumber \\
&amp;= (X^T X)^{-1} X^T  \mathbb{E}[(\varepsilon-0) (\varepsilon-0)^T] X (X^T X)^{-1}  \nonumber \\
&amp; =\sigma^2 (X^T X)^{-1}
\end{align}
\end{split}\]</div>
<p>because the variance of <span class="math notranslate nohighlight">\(\varepsilon\)</span> is <span class="math notranslate nohighlight">\(I\sigma^2\)</span></p>
<p>Typically we estimate the variance of the noise using the unbiased estimator</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat \sigma^2 &amp;= \frac{1}{N-D-1} \sum_{i=1}^N (y_i - \theta_0 - \sum_{j=1}^D \theta_j x_{ij})^2 \nonumber \\
&amp; = \frac{1}{N-D-1} (Y-X\theta)^T (Y-X\theta)
\end{align}
\end{split}\]</div>
<p>(Hastie, 3.2.2) <strong>The Gauss-Markov Theorem:</strong> The least squares estimate of <span class="math notranslate nohighlight">\(\theta\)</span> have the smallest variance among all unbiased estimators</p>
</div>
<div class="section" id="inference-with-ols">
<h3><span class="section-number">5.6.3. </span>Inference with OLS<a class="headerlink" href="#inference-with-ols" title="Permalink to this headline">¶</a></h3>
<p>We found the expected value and the variance of <span class="math notranslate nohighlight">\(\theta\)</span></p>
<p>From the properties of MLE we know that</p>
<div class="math notranslate nohighlight">
\[
\hat \theta \sim \mathcal{N}(\theta, \sigma^2 (X^T X)^{-1})
\]</div>
<p>and the estimator of the variance will be proportional to</p>
<div class="math notranslate nohighlight">
\[
\hat \sigma^2 \sim  \frac{1}{(N-M)}\sigma^2 \chi_{N-M}^2
\]</div>
<p>With this we have all the ingredients to find confidence intervals and do hypothesis test on <span class="math notranslate nohighlight">\(\hat \theta\)</span></p>
</div>
<div class="section" id="hypothesis-test-on-the-regression-coefficients">
<h3><span class="section-number">5.6.4. </span>Hypothesis test on the regression coefficients<a class="headerlink" href="#hypothesis-test-on-the-regression-coefficients" title="Permalink to this headline">¶</a></h3>
<p>To assess the significance of our model we might try to reject the following <em>hypotheses</em></p>
<ul>
<li><p>One of the parameters (slopes) is zero</p>
<p><span class="math notranslate nohighlight">\(\mathcal{H}_0: \theta_i = 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\mathcal{H}_A: \theta_i \neq 0\)</span></p>
</li>
<li><p>All parameters are zero</p>
<p><span class="math notranslate nohighlight">\(\mathcal{H}_0: \theta_1 = \theta_2 = \ldots = \theta_D = 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\mathcal{H}_A:\)</span> At least one parameter is not zero</p>
</li>
<li><p>A subset of the parameters are zero</p>
<p><span class="math notranslate nohighlight">\(\mathcal{H}_0: \theta_i = \theta_j =0 \)</span></p>
<p><span class="math notranslate nohighlight">\(\mathcal{H}_A:\)</span> <span class="math notranslate nohighlight">\(\theta_i \neq 0 \)</span> or <span class="math notranslate nohighlight">\(\theta_j \neq 0 \)</span></p>
</li>
</ul>
<blockquote>
<div><p>We can do this using t-test, f-test or ANOVA</p>
</div></blockquote>
<p>We can trust the test only if our assumptions are true</p>
<ul class="simple">
<li><p>Relation between X and Y is linear</p></li>
<li><p>Errors/noise are <em>iid</em></p></li>
<li><p>Errors/noise follows a multivariate normal with covariance <span class="math notranslate nohighlight">\(I\sigma^2\)</span></p></li>
</ul>
</div>
<div class="section" id="checking-assumptions">
<h3><span class="section-number">5.6.5. </span>Checking assumptions<a class="headerlink" href="#checking-assumptions" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Check the residuals for normality, Are there outliers that we should remove?</p></li>
<li><p>Check for absence of correlation in the residuals</p></li>
<li><p>Do the errors have different variance?</p></li>
</ol>
<p>For the latter we can use the  <strong>Weighted Least Squares</strong></p>
</div>
</div>
<div class="section" id="weighted-least-squares-wls">
<h2><span class="section-number">5.7. </span>Weighted Least Squares (WLS)<a class="headerlink" href="#weighted-least-squares-wls" title="Permalink to this headline">¶</a></h2>
<p>Before we assumed that the noise was homoscedastic (constant variance). We will generalize to the heteroscedastic case</p>
<p>Assuming that we have <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i=1,\ldots,N}\)</span> <em>iid</em> observations from unidimensional  Y and <strong>D-dimensional variable</strong> X</p>
<p>How do we find <span class="math notranslate nohighlight">\(\theta\)</span>  such that <span class="math notranslate nohighlight">\(y_i \approx \theta_0 + \sum_{j=1}^D \theta_j x_{ij}, \forall i\)</span>?</p>
<p>Let’s assume that our measurements of <span class="math notranslate nohighlight">\(Y\)</span> consists of the <strong>true model</strong> plus <strong>Gaussian noise</strong> with changing variance, <em>i.e.</em></p>
<div class="math notranslate nohighlight">
\[
y_i = \theta_0 + \sum_{j=1}^D \theta_j x_{ij} + \varepsilon_i, \forall i \quad \text{and} \quad \varepsilon_i \sim \mathcal{N}(0, \sigma_i^2)
\]</div>
<p>In this case the maximum likelihood solution is</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = (X^T \Sigma^{-1}X)^{-1} X^T \Sigma^{-1} Y
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma = \begin{pmatrix} 
\sigma_1^2 &amp; 0 &amp;\ldots &amp; 0 \\
0 &amp; \sigma_2^2 &amp;\ldots &amp; 0 \\
\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\
0 &amp; 0 &amp;\ldots &amp; \sigma_N^2 \\
\end{pmatrix}
\end{split}\]</div>
<p>Note that in this case</p>
<div class="math notranslate nohighlight">
\[
\hat \theta \sim \mathcal{N}( \theta,  (X^T X)^{-1} X^T  \Sigma X (X^T X)^{-1} )
\]</div>
</div>
<div class="section" id="linear-models-and-basis-functions">
<h2><span class="section-number">5.8. </span>Linear models and Basis functions<a class="headerlink" href="#linear-models-and-basis-functions" title="Permalink to this headline">¶</a></h2>
<p>Can we estimate a model that is not an hyperplane? YES!</p>
<p>The most general form of a linear model is</p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \sum_{j=0}^M \theta_j \phi_j(x_i),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi_j: \mathbb{R}^D \to \mathbb{R}\)</span> is a set of basis functions</p>
<p>Note that M and D are not related as before</p>
<div class="section" id="example-polynomials">
<h3><span class="section-number">5.8.1. </span>Example: Polynomials<a class="headerlink" href="#example-polynomials" title="Permalink to this headline">¶</a></h3>
<p>Unidimensional variable <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}, i=1,\ldots,N\)</span></p>
<p>Basis function is
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-b406e00e-e1a8-42e0-99c6-642206dcb285">
<span class="eqno">(5.3)<a class="headerlink" href="#equation-b406e00e-e1a8-42e0-99c6-642206dcb285" title="Permalink to this equation">¶</a></span>\[\begin{align}
\phi_0 (x) &amp;= 1 \nonumber \\
\phi_1 (x) &amp;= x \nonumber \\
\phi_2 (x) &amp;= x^2 \nonumber \\
&amp; \vdots \nonumber \\
\phi_M (x) &amp;= x^M \nonumber \\
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[This yields a M-degree polynomial model\]</div>
<p>g_\theta(x_i) = \theta_0 + \theta_1 x_i + \theta_2 x_i^2 + \ldots + \theta_M x_i^M, \forall i
$$</p>
<p>It is linear in the parameters but non-linear in the input</p>
</div>
<div class="section" id="example-transformations">
<h3><span class="section-number">5.8.2. </span>Example: Transformations<a class="headerlink" href="#example-transformations" title="Permalink to this headline">¶</a></h3>
<p>Unidimensional variable <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}, i=1,\ldots,N\)</span></p>
<p>Basis function is</p>
<div class="math notranslate nohighlight">
\[
\phi_j(x_i) = \cos(2\pi j x_i/P)
\]</div>
<p>This yields</p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \theta_0 + \theta_1 \cos(2\pi x_{i}/P) + \theta_2 \cos(4\pi x_{i}/P) + \ldots + \theta_M \cos(2M\pi x_{i}/P), \forall i
\]</div>
<p>A periodic model that is linear on <span class="math notranslate nohighlight">\(\theta\)</span></p>
</div>
<div class="section" id="example-interactions-between-variables">
<h3><span class="section-number">5.8.3. </span>Example: Interactions between variables<a class="headerlink" href="#example-interactions-between-variables" title="Permalink to this headline">¶</a></h3>
<p>Bidimensional variable <span class="math notranslate nohighlight">\(x_i = (x_{i1}, x_{i2}), i=1,\ldots,N\)</span></p>
<p>Basis function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\phi_j(x_i) = 
\begin{cases} 
1 &amp; j=0 \\ x_{i1} &amp; j=1 \\ 
x_{i2} &amp; j=2 \\ x_{i1} x_{i2} &amp; j=3 \\ 
x_{i1}^2 &amp; j=4 \\ x_{i2}^2 &amp; j=5 \\
\end{cases}
\end{split}\]</div>
<p>A basis with interactions between variables up to second-degree</p>
</div>
<div class="section" id="example-the-simplest-linear-model">
<h3><span class="section-number">5.8.4. </span>Example: The simplest linear model<a class="headerlink" href="#example-the-simplest-linear-model" title="Permalink to this headline">¶</a></h3>
<p>D-dimensional variable <span class="math notranslate nohighlight">\(x_i = (x_{i1}, x_{i2}, \ldots, x_{iD}), i=1,\ldots,N\)</span></p>
<p>Basis function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\phi_j(x_i) = \begin{cases} 1 &amp; j=0 \\ x_{ij} &amp; j \in [1,D] \end{cases}
\end{split}\]</div>
<p>This yields</p>
<div class="math notranslate nohighlight">
\[
g_\theta(x_i) = \theta_0 + \theta_1 x_{i1} + \theta_2 x_{i2} + \ldots + \theta_M x_{iD}, \forall i
\]</div>
<p>Which is the simplest linear model we reviewed before</p>
</div>
</div>
<div class="section" id="polynomial-regression">
<h2><span class="section-number">5.9. </span>Polynomial regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">¶</a></h2>
<p>We can create a polynomial basis easily using <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.PolynomialFeatures</span></code></p>
<p>In the following example we fit an <span class="math notranslate nohighlight">\(M\)</span> degree polynomial model.</p>
<ul class="simple">
<li><p>What happens when <span class="math notranslate nohighlight">\(M &gt; N\)</span>?</p></li>
<li><p>What happens to the prediction on the test data point?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span> 
<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">);</span>
<span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">Phi</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">Y_clean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Y_clean</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">M</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">theta_hat</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">Y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">)</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">rowspan</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Y_hat</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>  
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Y_clean</span><span class="p">,</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;underlying&#39;</span><span class="p">)</span>
    <span class="n">Phi_plot</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">M</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_plot</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi_plot</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">),</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">]);</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot2grid</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">Y_hat</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">);</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
    
<span class="n">interact</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">SelectionSlider</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">]));</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="overfitting-and-regularization">
<h2><span class="section-number">5.10. </span>Overfitting and regularization<a class="headerlink" href="#overfitting-and-regularization" title="Permalink to this headline">¶</a></h2>
<p>In the previous example <span class="math notranslate nohighlight">\(M\)</span> represents the complexity of the model</p>
<p>In general, more complex models give more flexibility to fit the data</p>
<p>But too much complexity causes <strong>overfitting</strong></p>
<ul class="simple">
<li><p>the model fits the noise</p></li>
<li><p>we can’t extract the underlying behavior</p></li>
<li><p>the model <strong>does not generalize</strong> to new data</p></li>
</ul>
<p>Ways to avoid overfitting</p>
<ul class="simple">
<li><p>Using low complexity models</p></li>
<li><p>Set complexity using cross-validation</p></li>
<li><p>Regularization</p></li>
</ul>
<div class="section" id="the-bias-variance-trade-off">
<h3><span class="section-number">5.10.1. </span>The Bias-Variance trade-off<a class="headerlink" href="#the-bias-variance-trade-off" title="Permalink to this headline">¶</a></h3>
<p>Let’s assume that the data is <span class="math notranslate nohighlight">\(y = f(x) + \varepsilon\)</span> (true model + Gaussian noise) and that we use a linear model to find <span class="math notranslate nohighlight">\(f(x)\)</span> as <span class="math notranslate nohighlight">\(\hat y = \sum_j \theta_j \phi_j(x)\)</span></p>
<p>We can measure the quality of our model with the Mean Square Error (MSE)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbb{E}[(y - \hat y)^2] &amp;= \mathbb{E}[y^2 -2 y \hat y +\hat y^2] \nonumber \\
&amp;= \mathbb{E}[(f+\varepsilon)^2 -2 (f+\varepsilon) \hat y +\hat y^2] \nonumber \\
&amp;= \mathbb{E}[(f^2 +2 f \varepsilon + \varepsilon^2 -2 (f+\varepsilon) \hat y +\hat y^2] \nonumber \\
&amp;= \mathbb{E}[\varepsilon^2] + f^2  -2 f \mathbb{E}[\hat y]  +\mathbb{E}[\hat y^2]  \pm \mathbb{E}[\hat y]^2  \nonumber \\
&amp;= \mathbb{E}[\varepsilon^2] + (f - \mathbb{E}[\hat y])^2  +\mathbb{E}[(\hat y - \mathbb{E}[\hat y])^2]  \nonumber \\
&amp;= \sigma^2 + (f - \mathbb{E}[\hat y])^2  + \text{Var}[\hat y]  \nonumber 
\end{align}
\end{split}\]</div>
<blockquote>
<div><p>The MSE can be decomposed as irreducible error + squared bias of the estimator + variance of the estimator</p>
</div></blockquote>
<ul class="simple">
<li><p>The MSE can be small if either the bias or the variance are small. The more complex the model the lower the bias and the higher the variance it attains</p></li>
<li><p>The Gauss-Markov theorem says that OLS has the minimum variance of the unbiased estimator.</p></li>
<li><p>Zero bias models are not necessarily good (overfit). In some cases we may want to trade bias for variance. This is achieved by penalizing the complexity of the model: <strong>Regularization</strong></p></li>
</ul>
</div>
</div>
<div class="section" id="bayesian-map-least-squares-and-ridge-regression">
<h2><span class="section-number">5.11. </span>Bayesian (MAP) Least Squares and Ridge regression<a class="headerlink" href="#bayesian-map-least-squares-and-ridge-regression" title="Permalink to this headline">¶</a></h2>
<p>If we assume a Gaussian likelihood and a Gaussian prior we can write the log joint as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log p({x}, \theta) &amp;= \log \prod_{i=1}^N \mathcal{N}(y_i | (1, x_{i1}, \ldots, x_{iD}) \theta, \sigma^2) + \log \prod_{j=1}^M \mathcal{N}(\theta_j | 0, \sigma_0^2) \nonumber \\
&amp;= -\frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (Y-X\theta)^T (Y - X\theta) -\frac{M}{2} \log(2\pi \sigma_0^2) - \frac{1}{2\sigma_0^2} \|\theta\|^2 \nonumber 
\end{align}
\end{split}\]</div>
<p>The MAP estimator of <span class="math notranslate nohighlight">\(\theta\)</span> is given by
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-79be9a1a-f014-46fa-b5c4-ebf7faa54f27">
<span class="eqno">(5.4)<a class="headerlink" href="#equation-79be9a1a-f014-46fa-b5c4-ebf7faa54f27" title="Permalink to this equation">¶</a></span>\[\begin{align}
\hat \theta &amp;= \text{arg}\max_\theta \log p({x}, \theta) \nonumber  \\
&amp;= \text{arg}\max_\theta  - \frac{1}{2\sigma^2} (Y-X\theta)^T (Y - X\theta) - \frac{1}{2\sigma_0^2} \|\theta\|^2 \nonumber \\
&amp;= \text{arg}\min_\theta  (Y-X\theta)^T (Y - X\theta) + \lambda \|\theta\|^2 \nonumber
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where $\lambda = \frac{\sigma^2}{\sigma_0^2}$\\The solution is obtained by taking the derivative on $\theta$\end{aligned}\end{align} \]</div>
<p>\frac{d}{d\theta} (Y-X\theta)^T (Y - X\theta) + \lambda |\theta|^2  = -X^T (Y - X\theta) + \lambda \theta = 0
$$</p>
<p>and finally</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = (X^T X + \lambda I)^{-1} X^T Y
\]</div>
<blockquote>
<div><p>This is known as Ridge regression and Tikhonov regularization</p>
</div></blockquote>
<p>We are forcing the solution to be smooth (have small L2 norm)</p>
<p>This can help to avoid overfitting with complex models. It can also help when <span class="math notranslate nohighlight">\(X^T X\)</span> is not invertible</p>
<ul class="simple">
<li><p>Adding a <span class="math notranslate nohighlight">\(\lambda\)</span> to the diagonal of <span class="math notranslate nohighlight">\(X^T X\)</span> makes the eigenvalues positive (unique solution)</p></li>
</ul>
<p>(There is no free lunch) How to choose <span class="math notranslate nohighlight">\(\lambda\)</span>?</p>
<ul class="simple">
<li><p>Cross-validation: minimize validation error</p></li>
<li><p>L-curve: Plot <span class="math notranslate nohighlight">\( \log (Y-X\theta)^T (Y - X\theta)\)</span> vs <span class="math notranslate nohighlight">\( \log \|\theta\|^2\)</span> and find the elbow</p></li>
</ul>
<p>Note that different priors yield different regularization schemes</p>
<ul class="simple">
<li><p>A Laplacian prior yields a L1 norm which forces the solution to be sparse (LASSO)</p></li>
</ul>
<div class="section" id="example-ridge-regression-with-sklearn">
<h3><span class="section-number">5.11.1. </span>Example: Ridge Regression with sklearn<a class="headerlink" href="#example-ridge-regression-with-sklearn" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">);</span> 
<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">);</span> 
<span class="n">model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">model</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>    

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">lamb</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">regressor</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">M</span><span class="p">),</span> 
                              <span class="n">Ridge</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">lamb</span><span class="p">))</span>
    <span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">cla</span><span class="p">();</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;underlying&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span> <span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_plot</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> 
            <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">);</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    
<span class="n">interact</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">SelectionSlider</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]),</span> 
         <span class="n">lamb</span><span class="o">=</span><span class="n">SelectionSlider</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">100000.</span><span class="p">]));</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="independence-and-correlation">
<h2><span class="section-number">5.12. </span>Independence and correlation<a class="headerlink" href="#independence-and-correlation" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Independence implies uncorrelatedness, but the reverse is not true.</p>
</div></blockquote>
<p>Two variables can have zero correlation but still be dependent</p>
<blockquote>
<div><p>Linear regression (correlation) is sensitive to linear relationships</p>
</div></blockquote>
<p>More generally, to test indepedence we could use:</p>
<div class="math notranslate nohighlight">
\[
p(x,y) = p(x)p(y)
\]</div>
<p>Several methods are based on this</p>
<ul class="simple">
<li><p>Shannon’s <strong>Mutual Information</strong>
$<span class="math notranslate nohighlight">\(
I(X,Y) = \int \int f_{XY}(x,y) \log \frac{f_{XY}(x,y)}{f_{X}(x) f_Y(y)} dx dy
\)</span>$</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/0803.4101.pdf">Correlation distance</a>
$<span class="math notranslate nohighlight">\(
R(X,Y) = \int \int |f_{XY}(x,y)  - f_{X}(x) f_Y(y)| dx dy
\)</span>$</p></li>
</ul>
<p>Although these require that we estimate the joint and the marginals (KDE, Histogram, Parametric)</p>
<p>For categorical variables we can use the <strong>chi square test</strong></p>
</div>
<div class="section" id="extra-topics">
<h2><span class="section-number">5.13. </span>Extra topics<a class="headerlink" href="#extra-topics" title="Permalink to this headline">¶</a></h2>
<p>Suggestions if you want to go deeper</p>
<ul class="simple">
<li><p>(Hastie 3.4 and 3.8) L1 regularization and Least Absolute Shrinkage and Selection Operator (LASSO)</p></li>
<li><p>Robust regression: Least absolute regression and M-estimators for data with outliers (non-Gaussian)</p></li>
<li><p>(Hastie 6 &amp; Bishop 6) Kernel (non-parametric) regression</p></li>
<li><p>Some of these topics can be found at <a class="reference external" href="https://docs.google.com/presentation/d/1UUpK4zSdzRcS79V7_wU9nXe-sR7qYLEWhbmid-Rfp1k/edit#slide=id.g28044c0f85_0_34">Huijse, Regresión</a></p></li>
</ul>
</div>
</div>
<div class="section" id="causality-beyond-correlation">
<h1><span class="section-number">6. </span>Causality: Beyond correlation<a class="headerlink" href="#causality-beyond-correlation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="judea-pearl-the-new-science-of-cause-and-effect">
<h2><span class="section-number">6.1. </span>Judea Pearl - The New Science of Cause and Effect<a class="headerlink" href="#judea-pearl-the-new-science-of-cause-and-effect" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">YouTubeVideo</span><span class="p">(</span><span class="s1">&#39;ZaPV1OSEpHw&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="do-why">
<h2><span class="section-number">6.2. </span>Do Why<a class="headerlink" href="#do-why" title="Permalink to this headline">¶</a></h2>
<p>Library for causal reasoning: <a class="reference external" href="https://github.com/microsoft/dowhy">https://github.com/microsoft/dowhy</a></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "info183"
        },
        kernelOptions: {
            kernelName: "info183",
            path: "./lectures/5_linear_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'info183'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../4_nonparametric_inference/lecture.html" title="previous page"><span class="section-number">4. </span>Inferencia estadística con tests no-paramétricos</a>
    <a class='right-next' id="next-link" href="../9_mixture_models/lecture.html" title="next page"><span class="section-number">7. </span>Gaussian Mixture Models</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Pablo Huijse and Eliana Scheihing<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>