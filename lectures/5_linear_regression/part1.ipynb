{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500), \n",
    "                 hv.opts.Histogram(width=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many problems can be posed as \"finding a relation\" between factors/variables\n",
    "\n",
    "This can be interpreted as predicting and/or explaining a variable given others\n",
    "\n",
    "Some examples:\n",
    "\n",
    "- Predicting sales given money spent in advertising\n",
    "- Predicting chance to rain in Valdivia given temperature, pressure and humidity\n",
    "- Predicting gasoline consumption of a car given acceleration, weight and number of cylinders\n",
    "- Predicting chance to get lung cancer given number of smoked cigarettes per day, age and gender\n",
    "\n",
    "We could ask\n",
    "\n",
    "- Are these variable related?\n",
    "- How strong and/or significant is the relationship?\n",
    "- What is the nature of the relationship?\n",
    "\n",
    "Answering these helps us **understand the underlying processes behind the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining regression\n",
    "\n",
    "**Regression** refers to a family of statistical methods to find **relationships** between **variables**\n",
    "\n",
    "In general the relation is modeled as a function $g(\\cdot)$ that maps two types of variables\n",
    "\n",
    "- The input variable $X$ is called **independent variable** or feature\n",
    "- The output variable $Y$ is called **dependent variable**, response or target\n",
    "\n",
    "The mapping or function $g$ is called **predictor** or **regressor**\n",
    "\n",
    "$$\n",
    "g: X \\to Y\n",
    "$$\n",
    "\n",
    "The objective is to learn $g$ such that we can predict $Y$ given $X$, *i.e.* $\\mathbb{E}[Y|X]$ \n",
    "\n",
    "- **Regression** can be defined from an statistical perspective as a special case of model fitting (parameter estimation)\n",
    "- In many books **Regression** is defined from a pure-optimization perspective (deterministic)\n",
    "- **Regression** is considered part of the *supervised learning* paradigm. The difference between **Regression** and *classification* is the nature of the dependent variable (continuous vs categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametric vs non-parametric  regression\n",
    "\n",
    "Regression methods can be broadly classified as either parametric or non-parametric \n",
    "\n",
    "In parametric regression \n",
    "\n",
    "- We know the model of the regressor\n",
    "- The model has a finite number of parameters\n",
    "- The parameters of the model are all we need to do predictions \n",
    "- Simpler but with bigger assumptions (inductive bias)\n",
    "\n",
    "\n",
    "In nonparametric regression\n",
    "\n",
    "- There is no functional form for the regressor\n",
    "- It can have an infinite number of parameters (and a finite number of hyperparameters)\n",
    "- The regressor is defined from the training data\n",
    "- More flexible but requires more data to fit it\n",
    "- Examples: Splines, Support vector regression, Gaussian processes\n",
    "\n",
    "In this lesson we will focus on parametric regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parametric linear models for regression\n",
    "\n",
    "Let \n",
    "\n",
    "- $X$ be a continuous D-dimensional variable (feature) and $Y$ be a continuous unidimensional variable (target) \n",
    "- $\\{x_i, y_i\\}$ with $i=1,\\ldots,N$ be a set of $N$ *iid* observations of $X$ and $Y$\n",
    "- $g_\\theta$ be a model with a M-dimensional parameter $\\theta$ \n",
    "\n",
    "Then we can define parametric regression as finding a value of $\\theta$ such that \n",
    "\n",
    "$$\n",
    "y_i \\approx g_\\theta(x_i),\\quad i=1,\\ldots, N\n",
    "$$\n",
    "\n",
    "The simplest parametric model is the **linear model**. A linear model gives rise to **linear regression**\n",
    "\n",
    ":::{important}\n",
    "\n",
    "The linear model is linear on $\\theta$ but not necessarily on $X$\n",
    "\n",
    ":::\n",
    "\n",
    "For example a model with unidimensional input\n",
    "\n",
    "$$\n",
    "g_\\theta \\left(x_i \\right) = \\theta_0 + \\theta_1 x_i  + \\theta_2 x_i^2,\n",
    "$$\n",
    "\n",
    "is a linear model and\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 \\log(x_i),\n",
    "$$\n",
    "\n",
    "is also a linear model but\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\log(x_i + \\theta_1),\n",
    "$$\n",
    "\n",
    "is not a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The simplest linear model: The line\n",
    "\n",
    "If we consider a one-dimensional variable $x_i \\in \\mathbb{R}, i=1,\\ldots,N$, then the simplest linear model is\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 x_i\n",
    "$$\n",
    "\n",
    "which has $M=2$ parameters. \n",
    "\n",
    "This corresponds to a line in $\\mathbb{R}^2$ and we recognize\n",
    "\n",
    "- $\\theta_0$ as the intercept\n",
    "- $\\theta_1$ as the slope\n",
    "\n",
    "If we consider a two-dimensional variable $x_i = (x_{i1}, x_{i2}) \\in \\mathbb{R}^2, i=1,\\ldots,N$ then we obtain\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2}\n",
    "$$\n",
    "\n",
    "which has $M=3$ parameters. This corresponds to a plane in $\\mathbb{R}^3$\n",
    "\n",
    "The most general form assumes a D-dimensional variable $x_i = (x_{i1}, x_{i2}, \\ldots, x_{iD}), i=1,\\ldots,N$ \n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}\n",
    "$$\n",
    "\n",
    "which has $M=D+1$ parameters, which corresponds to an hyperplane in $\\mathbb{R}^M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting the simplest linear model: Mathematics\n",
    "\n",
    "Assuming that we have $\\{x_i, y_i\\}_{i=1,\\ldots,N}$ *iid* observations from unidimensional variables X and Y\n",
    "\n",
    "> How do we find $\\theta_0$ and $\\theta_1$ such that $y_i \\approx \\theta_0 + \\theta_1 x_i, \\forall i$?\n",
    "\n",
    "Let's start by writing the squared residual (error) as \n",
    "\n",
    "$$\n",
    "E_i^2 = (y_i - \\theta_0 - \\theta_1 x_i)^2,\n",
    "$$\n",
    "\n",
    "We can fit (train) the model with\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} L = \\sum_{i=1}^N E_i^2 = \\sum_{i=1}^N (y_i - \\theta_0 - \\theta_1 x_i)^2,\n",
    "$$\n",
    "\n",
    "where $L$, the sum of squares errors, is a our loss/cost function\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Later we will see that this cost function arises when a gaussian likelihood for $Y$ is assumed\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative of this expression with respect to the $\\theta_0$ and $\\theta_1$ we obtain\n",
    "\n",
    "$$\n",
    "\\hat \\theta_1 = \\frac{\\text{Cov}[x, y]}{\\text{Var}[x]}, \\hat \\theta_0 = \\bar y - \\hat \\theta_1 \\bar x\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{Cov}[x, y] = \n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\text{Var}[x] =\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{dropdown} Proof\n",
    "\n",
    "With\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{d\\theta_0} &= -2 \\sum_{i=1}^N (y_i - \\theta_0 - \\theta_1 x_i) \\nonumber \\\\\n",
    "&= -2 \\sum_{i=1}^N y_i +  2 N\\theta_0 + 2 \\theta_1 \\sum_{i=1}^N x_i = 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{d\\theta_1} &= -2 \\sum_{i=1}^N (y_i - \\theta_0 - \\theta_1 x_i) x_i \\nonumber \\\\\n",
    "&= -2 \\sum_{i=1}^N y_i x_i +  2 \\theta_0 \\sum_{i=1}^N x_i + 2 \\theta_1 \\sum_{i=1}^N x_i^2 = 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "a system of two equations and two unknowns is obtained\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} N & \\sum_i x_i \\\\ \\sum_i x_i & \\sum_i x_i^2\\\\\\end{pmatrix} \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}  = \\begin{pmatrix} \\sum_i y_i \\\\ \\sum_i x_i y_i \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} \\hat \\theta_0 \\\\ \\hat \\theta_1 \\end{pmatrix}  = \n",
    "\\frac{1}{N\\sum_i x_i^2 - \\left(\\sum_i x_i\\right)^2}\\begin{pmatrix} \\sum_i x_i^2 & -\\sum_i x_i \\\\ -\\sum_i x_i & N\\\\\\end{pmatrix}  \n",
    "\\begin{pmatrix} \\sum_i y_i \\\\ \\sum_i x_i y_i \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "where we assume that the determinant of the matrix is not zero\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the simplest linear model: Python\n",
    "\n",
    "We can fit a line in Python using the `scipy.stats` library\n",
    "\n",
    "```python\n",
    "scipy.stats.linregress(x, # N vector or Mx2 matrix (if y is None)\n",
    "                       y=None, # N vector\n",
    "                      )\n",
    "```\n",
    "\n",
    "This function returns an object, its main attributes are\n",
    "\n",
    "- `slope`: Equivalent to $\\hat \\theta_1$\n",
    "- `intercept`: Equivalent to $\\hat \\theta_0$\n",
    "- `rvalue`: The correlation coefficient (more on this later)\n",
    "- `pvalue`: A p-value for the null hypothesis that $\\theta_1 =0$ (more on this later)\n",
    "\n",
    "Let's create synthetic data to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "theta, sigma = [0.5 , -1], 0.5\n",
    "x = np.random.rand(25)*5\n",
    "\n",
    "def model(x, theta):\n",
    "    return theta[0] + theta[1]*x\n",
    "\n",
    "y = model(x, theta) + sigma*np.random.randn(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the data using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scipy.stats.linregress(x, y)\n",
    "theta_hat = np.array([res.intercept, res.slope])\n",
    "\n",
    "print(f\"hat theta0: {theta_hat[0]:0.5f}, hat theta1: {theta_hat[1]:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with the model and inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the fitted model to interpolate/extrapolate on new values $\\hat x$\n",
    "\n",
    "The following plot shows the fitted model on the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_x = np.linspace(-1, 6, num=50)\n",
    "p_fitted = hv.Curve((hat_x, model(hat_x, theta_hat)), label='Fitted model')\n",
    "p_data = hv.Scatter((x, y), label='Training data').opts(color='k', size=5)\n",
    "\n",
    "hv.Overlay([p_data, p_fitted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model (blue) follows the data closely. \n",
    "\n",
    "To visually assess the quality of the fit we can also plot the residuals, i.e. the distance between each sample of the training set and the fitted line. We can also inspect the histogram of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - model(x, theta_hat)\n",
    "bins, edges = np.histogram(residuals, density=True)\n",
    "\n",
    "p_residuals = hv.Scatter((y, residuals), 'Target variable', 'Residuals').opts(color='k', size=5, width=350)\n",
    "p_zero = hv.HLine(0).opts(color='k', line_dash='dashed', alpha=0.5)\n",
    "p_hist = hv.Histogram((edges, bins), kdims='Residuals', vdims='Density').opts(width=350)\n",
    "\n",
    "hv.Layout([p_residuals * p_zero, p_hist]).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for residuals that\n",
    "\n",
    "- concentrate around zero \n",
    "- are not correlated (white noise like)\n",
    "\n",
    "Correlation in the residuals is a sign that the choice of the model (line) was not adequate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coefficient of determination\n",
    "\n",
    "We can measure how strong is the linear relation between $y$ and $\\hat y = \\hat \\theta_0 + \\hat \\theta_1 x$ using the **coefficient of determination** or $r^2$\n",
    "\n",
    "This is defined as\n",
    "\n",
    "$$\n",
    "r^2 = 1 - \\frac{\\sum_i (y_i - \\hat y_i)^2}{\\sum_i (y_i - \\bar y_i)^2} \\in [0, 1]\n",
    "$$\n",
    "\n",
    "*i.e.* one minus the sum of residuals divided by the variance of $y$. The $r$ statistic is also known as Pearson's correlation coefficient. \n",
    "\n",
    "Interpreting $r^2$:\n",
    "\n",
    "- If $r^2 = 1$, the data points are fitted perfectly by the model. The regressor accounts for all of the variation in y\n",
    "- If $r^2 = 0$, the regression line is horizontal. The regressor accounts for none of the variation in y\n",
    "\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "If the relation is strong but non-linear it will not be detected by $r^2$\n",
    "\n",
    ":::\n",
    "\n",
    "Note that $r$ is available in the object return by `scipy.stats.linregress`\n",
    "\n",
    "For example in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.rvalue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "366px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
