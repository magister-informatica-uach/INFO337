{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500),\n",
    "                 hv.opts.Scatter(width=500, size=4),\n",
    "                 hv.opts.Histogram(width=500),\n",
    "                 hv.opts.Slope(color='k', alpha=0.5, line_dash='dashed'),\n",
    "                 hv.opts.HLine(color='k', alpha=0.5, line_dash='dashed'))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate linear regression\n",
    "\n",
    "In the previous lesson we introduce the topic of linear regression and studied the most simple linear model: the line. \n",
    "\n",
    "In this lesson we will generalize this model to the multivariate case, i.e. when we want to predict an unidimensional (and continuous) variable $Y$ from a multidimensional (and continuous) variable $X$. You can interpret $X$ as a table where each column represents a particular attribute.\n",
    "\n",
    ":::{admonition} Example\n",
    ":class: tip\n",
    "\n",
    "We want to predict a car's $Y=[\\text{fuel consumption}]$ using its $X=[\\text{weight}; \\text{number of cylinders}; \\text{average speed}; \\ldots]$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "In what follows we will learn the mathematical formalism of the Ordinary Least Squares (OLS) method and how to implement it to fit regression models using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical derivation\n",
    "\n",
    "Consider a dataset $\\{x_i, y_i\\}_{i=1,\\ldots,N}$ of *i.i.d.* observations with $y_i \\in \\mathbb{R}$ and $x_i \\in \\mathbb{R}^D$, with $D>1$. We want to find $\\theta$  such that \n",
    "\n",
    "$$\n",
    "y_i \\approx \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij}, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "As before we start by writing the sum of squared errors (residuals) \n",
    "\n",
    "$$\n",
    "\\min_\\theta L = \\sum_{i=1}^N (y_i - \\theta_0 - \\sum_{j=1}^D \\theta_j x_{ij})^2\n",
    "$$\n",
    "\n",
    "but in this case we will express it in matrix form \n",
    "\n",
    "$$\n",
    "\\min_\\theta  L = \\| Y - X \\theta \\|^2 = (Y - X \\theta)^T (Y - X \\theta)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\ldots & x_{1D} \\\\ \n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "1 & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{N1} & x_{N2} & \\ldots & x_{ND} \\end{pmatrix},  Y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}, \\theta =  \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_D \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "From here we can do\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\theta} = -(Y - X \\theta)^T X =  -X^T (Y - X \\theta) = 0\n",
    "$$\n",
    "\n",
    "to obtain the **normal equations**\n",
    "\n",
    "$$\n",
    "X^T X \\theta  = X^T Y\n",
    "$$\n",
    "\n",
    "whose solution is\n",
    "\n",
    "$$\n",
    "\\hat \\theta = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "which is known as the **least squares (LS) estimator** of $\\theta$\n",
    "\n",
    ":::{dropdown} Relation with the Moore-Penrose inverse\n",
    "\n",
    "Matrix $X^{\\dagger} = (X^T X)^{-1} X^T $ is known as the left [*Moore-Penrose*](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) pseudo-inverse. There is also the right pseudo inverse $X^T (X X^T)^{-1}$. Together they act  as a generalization of the inverse for non-squared matrices. Further note that if $X$ is squared and invertible then $X^{\\dagger} = (X^T X)^{-1} X^T  = X^{-1} (X^T)^{-1} X^T = X^{-1}$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "The OLS solution is only valid if $A=X^T X$ is invertible (non-singular). By construction $A \\in \\mathbb{R}^{D\\times D}$ is a squared symmetric matrix. For $A$ to be invertible we require that its determinant is not zero or equivalently\n",
    "\n",
    "- The rank of $A$, i.e. the number of linearly independent rows or columns, is equal to $D$ \n",
    "- The eigenvalues/singular values of $A$ are positive\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{note}\n",
    "\n",
    "The solution we found for the univariate case in the previous lesson is a particular case of the OLS solution\n",
    "\n",
    ":::\n",
    "\n",
    ":::{dropdown} Proof\n",
    "\n",
    "The solution for the univariate case was\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} N & \\sum_i x_i \\\\ \\sum_i x_i & \\sum_i x_i^2\\\\\\end{pmatrix} \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}  = \\begin{pmatrix} \\sum_i y_i \\\\ \\sum_i x_i y_i \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "which can be rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} 1 & 1 & \\ldots & 1 \\\\ x_1 & x_2 & \\ldots & x_N \\end{pmatrix} \n",
    "\\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_N \\end{pmatrix} \n",
    "\\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix}  &= \n",
    "\\begin{pmatrix} 1 & 1 & \\ldots & 1 \\\\ x_1 & x_2 & \\ldots & x_N \\end{pmatrix} \n",
    "\\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix} \\nonumber \\\\\n",
    "X^T X \\theta &= X^T Y \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting an hyperplane using `numpy`\n",
    "\n",
    "The [`linalg`](https://numpy.org/doc/stable/reference/routines.linalg.html) submodule of the `numpy` library provides\n",
    "\n",
    "```python\n",
    "np.linalg.lstsq(X, # a (N, D) shaped ndarray\n",
    "                Y, # a (N, ) shaped ndarray \n",
    "                rcond='warn' # See note below\n",
    "               )\n",
    "```\n",
    "\n",
    "which returns \n",
    "\n",
    "- The OLS solution: $\\hat \\theta = (X^T X)^{-1} X^T Y$\n",
    "- The sum of squared residuals\n",
    "- The rank of matrix $X$\n",
    "- The singular values of matrix $X$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "For a near-singular $A=X^T X$ we might not be able to obtain the solution using numerical methods. Conditioning can help stabilize the solution. Singular values smaller than $\\epsilon$ can be cut-off by setting `rcond=epsilon` when calling `lstsq`\n",
    "\n",
    ":::\n",
    "\n",
    "Let's test `lstsq` on the following database of ice-cream consumption from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ice_cream.csv', header=0, index_col=0)\n",
    "df.columns = ['Consumption', 'Income', 'Price', 'Temperature']\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `corr` attribute of the `pandas` dataframe returns the pairwise correlations between the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- Temperature has a high positive correlation with consumption\n",
    "- Price has a low negative correlation with consumption\n",
    "- Income has an almost null correlation with consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a multivariate linear regressor for ice-cream consumption as a function of the other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"Consumption\"].values\n",
    "X = df[[\"Income\", \"Price\", \"Temperature\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will standardize the independent variables so that their scale is the same\n",
    "- We will incorporate a column with ones to model the intercept ($\\theta_0$) of the hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - np.mean(X, axis=0, keepdims=True))/np.std(X, axis=0, keepdims=True)\n",
    "X = np.concatenate((np.ones(shape=(X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "theta, mse, rank, singvals = np.linalg.lstsq(X, Y, rcond=None)\n",
    "hatY = np.dot(X, theta) # Predicted Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of the fitted model we can visualize the predicted consumption versus actual (real) consumption or the residuals as a function of the latter and/or the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p1 = hv.Scatter((Y, hatY), 'Real', 'Predicted').opts(width=330) * hv.Slope(slope=1, y_intercept=0)\n",
    "p2 = hv.Scatter((Y, Y - hatY), 'Real', 'Residuals').opts(width=330) * hv.HLine(0)\n",
    "hv.Layout([p1, p2]).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p = []\n",
    "for var_name in [\"Income\", \"Price\", \"Temperature\"]:\n",
    "    p.append(hv.Scatter((df[var_name].values, Y - hatY), var_name, 'Residuals').opts(width=330) * hv.HLine(0))\n",
    "hv.Layout(p).cols(3).opts(hv.opts.Scatter(width=280, height=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted consumption follows the real consumption closely. There is also no apparent correlation in the residuals.\n",
    "\n",
    "But some important questions remain\n",
    "\n",
    ":::{important}\n",
    "\n",
    "- How significant is the contribution of each of the independent variables to the prediction?\n",
    "- How to measure in a quantitative way the quality of the fitted model?\n",
    "\n",
    ":::\n",
    "\n",
    "For this we need to view OLS from an statistical perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Statistical perspective of OLS\n",
    "\n",
    "Up to now we have viewed regression from a deterministic (optimization) perspective. To understand its properties and perform inference we seek an statistical interpretation. \n",
    "\n",
    "Let's say that we have $\\{x_i, y_i\\}_{i=1,\\ldots,N}$ *i.i.d.* observations from an unidimensional target variable $Y$ and a **D-dimensional** independent variable $X$. We will assume that our measurements of $Y$ consists of the **true model** plus **white Gaussian noise**, *i.e.*\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i &= f_\\theta(x_i) + \\varepsilon_i \\nonumber \\\\\n",
    "&= \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij} + \\varepsilon_i \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Then the log likelihood of $\\theta$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log L(\\theta) &= \\log \\prod_{i=1}^N \\mathcal{N}(y_i | f_\\theta(x_i), \\sigma^2) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N \\log \\mathcal{N}(y_i | f_\\theta(x_i), \\sigma^2) \\nonumber \\\\\n",
    "&= -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - f_\\theta(x_i))^2\\nonumber \\\\\n",
    "&= -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta), \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the maximum likelihood solution for $\\theta$ can by obtained from\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\log L(\\theta) = - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta).\n",
    "$$\n",
    "\n",
    "Note that this is equivalent to \n",
    "\n",
    "$$\n",
    "\\min_\\theta \\log L(\\theta) =  \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta),\n",
    "$$\n",
    "\n",
    "which yields \n",
    "\n",
    "$$\n",
    "\\hat \\theta = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    ":::{important}\n",
    "\n",
    "The least squares solution is equivalent to the maximum likelihood solution under iid samples and gaussian noise\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistical properties of the OLS solution\n",
    "\n",
    "Let $\\varepsilon = (\\varepsilon_1, \\varepsilon_2, \\ldots, \\varepsilon_N)$, where $\\varepsilon \\sim \\mathcal{N}(0, I \\sigma^2) \\quad \\forall i$ \n",
    "\n",
    "Is the OLS estimator unbiased?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\hat \\theta] &= \\mathbb{E}[(X^T X)^{-1} X^T Y] \\nonumber \\\\\n",
    "&= \\mathbb{E}[(X^T X)^{-1} X^T (X \\theta + \\varepsilon)] \\nonumber \\\\\n",
    "&= \\mathbb{E}[\\theta] + (X^T X)^{-1} X^T \\mathbb{E}[\\varepsilon] \\\\\n",
    "& = \\mathbb{E}[\\theta]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> YES! \n",
    "\n",
    "What is the variance of the estimator? \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[(\\hat \\theta - \\mathbb{E}[\\hat\\theta])(\\hat \\theta - \\mathbb{E}[\\hat\\theta])^T] &= \\mathbb{E}[((X^T X)^{-1} X^T \\varepsilon) ((X^T X)^{-1} X^T \\varepsilon)^T] \\nonumber \\\\\n",
    "&= (X^T X)^{-1} X^T  \\mathbb{E}[\\varepsilon \\varepsilon^T] X ((X^T X)^{-1})^T  \\nonumber \\\\\n",
    "&= (X^T X)^{-1} X^T  \\mathbb{E}[(\\varepsilon-0) (\\varepsilon-0)^T] X (X^T X)^{-1}  \\nonumber \\\\\n",
    "& =\\sigma^2 (X^T X)^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and typically we estimate the variance of the noise using the unbiased estimator\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2 &= \\frac{1}{N-D-1} \\sum_{i=1}^N (y_i - \\theta_0 - \\sum_{j=1}^D \\theta_j x_{ij})^2 \\nonumber \\\\\n",
    "& = \\frac{1}{N-D-1} (Y-X\\theta)^T (Y-X\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**The Gauss-Markov Theorem:** The least squares estimate of $\\theta$ have the smallest variance among all unbiased estimators (Hastie, 3.2.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inference and hypothesis tests for OLS\n",
    "\n",
    "We found the expected value and the variance of $\\theta$. From the properties of MLE we know that\n",
    "\n",
    "$$\n",
    "\\hat \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2 (X^T X)^{-1})\n",
    "$$\n",
    "\n",
    "and the estimator of the variance will be proportional to\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 \\sim  \\frac{1}{(N-M)}\\sigma^2 \\chi_{N-M}^2\n",
    "$$\n",
    "\n",
    "With this we have all the ingredients to find confidence intervals and do hypothesis test on $\\hat \\theta$\n",
    "\n",
    "To assess the significance of our model we might try to reject the following *hypotheses*\n",
    "\n",
    "- One of the parameters (slopes) is zero (t-test)\n",
    "\n",
    "    $\\mathcal{H}_0: \\theta_i = 0$\n",
    "    \n",
    "    $\\mathcal{H}_A: \\theta_i \\neq 0$\n",
    "    \n",
    "    \n",
    "- All parameters are zero (f-test)\n",
    "\n",
    "    $\\mathcal{H}_0: \\theta_1 = \\theta_2 = \\ldots = \\theta_D = 0$\n",
    "\n",
    "    $\\mathcal{H}_A:$ At least one parameter is not zero\n",
    "\n",
    "\n",
    "- A subset of the parameters are zero (ANOVA)\n",
    "\n",
    "    $\\mathcal{H}_0: \\theta_i = \\theta_j =0 $\n",
    "\n",
    "    $\\mathcal{H}_A:$ $\\theta_i \\neq 0 $ or $\\theta_j \\neq 0 $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`OLS`](https://www.statsmodels.org/stable/regression.html) function of the `statsmodels` Python library to perform all these tests \n",
    "\n",
    "First we create the model by giving the target and independent variables. In `statsmodels` jargon these are called endogenous and exogenous, respectively. Then we call the `fit` attribute\n",
    "\n",
    "The coefficients obtained are equivalent to those we found with `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(Y, X, hasconst=True)\n",
    "res = mod.fit()\n",
    "display(theta, \n",
    "        res.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `summary` attribute gives as \n",
    "\n",
    "- the `R-squared` statistic of the model\n",
    "- the `F-statistic` and its p-value\n",
    "- A table with the values of `theta` their standard errors, `t-statistics`, p-values and confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(res.summary(yname=\"Consumption\", \n",
    "                    xname=[\"Intercept\", \"Income\", \"Price\", \"Temperature\"],\n",
    "                    alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from the results table:\n",
    "\n",
    "- The f-test tells that we can reject the hypothesis that all coefficients are null\n",
    "- The t-test tells us that we cannot reject the null hypothesis that the price coefficient is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $r^2$ statistic for the multivariate case is defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r^2 &= 1 - \\frac{\\sum_i (y_i - \\hat y_i)^2}{\\sum_i (y_i - \\bar y_i)^2} \\nonumber \\\\\n",
    "&= 1 - \\frac{Y^T(I-X(X^TX)^{-1}X^T)Y}{Y^T (I - \\frac{1}{N} \\mathbb{1}^T \\mathbb{1} ) Y} \\nonumber \\\\\n",
    "&= 1 - \\frac{SS_{res}}{SS_{total}} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1} = (1, 1, \\ldots, 1)$. And it has the same interpretation that was given in the previous lecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ":::{important}\n",
    "\n",
    "We can trust the test only if our assumptions are true. The assumptions in this case are\n",
    "\n",
    "- Relation between X and Y is linear\n",
    "- Errors/noise follows a multivariate normal with covariance $I\\sigma^2$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "Verify this assumptions by\n",
    "\n",
    "1. Checking the residuals for normality. Are there outliers that we should remove?\n",
    "1. Checking for absence of correlation in the residuals\n",
    "1. Do the errors have different variance?\n",
    "\n",
    "\n",
    "If the variance of the error is not constant (heteroscedastic) we can use the  **Weighted Least Squares** estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extra: Weighted Least Squares (WLS)\n",
    "\n",
    "Before we assumed that the noise was homoscedastic (constant variance). We will generalize to the heteroscedastic case.\n",
    "\n",
    "We can write the multivariate linear regression model with observations subject to Gaussian noise with changing variance as\n",
    "\n",
    "$$\n",
    "y_i = \\theta_0 + \\sum_{j=1}^D \\theta_j x_{ij} + \\varepsilon_i, \\forall i \\quad \\text{and} \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)\n",
    "$$\n",
    "\n",
    "\n",
    "With respect to OLS the only difference is that $\\sigma_i \\neq \\sigma$\n",
    "\n",
    "\n",
    "\n",
    "In this case the maximum likelihood solution is \n",
    "\n",
    "$$\n",
    "\\hat \\theta = (X^T \\Sigma^{-1}X)^{-1} X^T \\Sigma^{-1} Y\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix} \n",
    "\\sigma_1^2 & 0 &\\ldots & 0 \\\\\n",
    "0 & \\sigma_2^2 &\\ldots & 0 \\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots \\\\\n",
    "0 & 0 &\\ldots & \\sigma_N^2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "An the distribution of $\\theta$ is\n",
    "\n",
    "$$\n",
    "\\hat \\theta \\sim \\mathcal{N}( \\theta,  (X^T X)^{-1} X^T  \\Sigma X (X^T X)^{-1} )\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "366px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
