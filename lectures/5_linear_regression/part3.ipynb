{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500),\n",
    "                 hv.opts.Scatter(width=500, size=4),\n",
    "                 hv.opts.Histogram(width=500),\n",
    "                 hv.opts.Slope(color='k', alpha=0.5, line_dash='dashed'),\n",
    "                 hv.opts.HLine(color='k', alpha=0.5, line_dash='dashed'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear models and Basis functions\n",
    "\n",
    "In previous lessons we learned how to fit lines and hyperplanes to data. But the most general form of a linear model is\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\sum_{k=0}^K \\theta_k \\phi_k(x_i),\n",
    "$$\n",
    "\n",
    "where $\\phi_k: \\mathbb{R}^D \\to \\mathbb{R}$ is a set of basis functions. Note that $K$ and $D$ are not necessarily related \n",
    "\n",
    "In this lesson we will \n",
    "\n",
    "- review some examples of basis functions\n",
    "- learn how to perform linear regression with basis functions using Python\n",
    "- learn how to avoid overfitting the data using regularization techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis functions\n",
    "\n",
    "### Polynomials\n",
    "\n",
    "For a unidimensional variable $x_i \\in \\mathbb{R}, i=1,\\ldots,N$, a general polynomial basis is defined \n",
    "\n",
    "$$\n",
    "\\phi_k(x_i) = x_i^k\n",
    "$$\n",
    "\n",
    "which yields a K-degree polynomial model\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\sum_{k=0}^K \\theta_k x_i^k = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\ldots + \\theta_K x_i^K \\quad \\forall i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigonometric\n",
    "\n",
    "If we want to model periodic behavior a trigonometric basis is a suitable choice. For a unidimensional variable $x_i \\in \\mathbb{R}, i=1,\\ldots,N$, a trigonometric basis with period $P=1/f_0$ is\n",
    "\n",
    "$$\n",
    "\\phi_k(x_i) = \\begin{cases} 1 & k=0 \\\\ \\cos(2\\pi k f_0 x_i) & k \\in [1, K] \\\\  \\sin(2\\pi k f_0 x_i) & k \\in [K+1, 2K] \\end{cases}\n",
    "$$\n",
    "\n",
    "which yields a trigonometric model with $2K+1$ parameters\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\sum_{k=1}^K \\theta_k \\cos(2\\pi k f_0 x_i) + \\sum_{k=1}^K \\theta_{k+K} \\sin(2\\pi k f_0 x_i)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions between variables\n",
    "\n",
    "We can create a basis that models linear and non-linear interactions between our independent variables\n",
    "\n",
    "For example if we have a bidimensional variable $x_i = (w_{i}, v_{i}), i=1,\\ldots,N$ a model with interactions up to the second degree would be\n",
    "\n",
    "$$\n",
    "g_\\theta(x_i) = \\theta_0 + \\theta_1 w_i + \\theta_2 v_i + \\theta_3 w_i^2 + \\theta_4 v_i w_i + + \\theta_5 v_i^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Polynomial regression using `scikit-learn`\n",
    "\n",
    "We can create a polynomial basis from our variables using \n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.PolynomialFeatures(degree=2, # Degree of the polinomial\n",
    "                                         interaction_only=False, # Return only products between features\n",
    "                                         include_bias=True, # Include the intercept (constant) term\n",
    "                                         ...\n",
    "                                        )\n",
    "``` \n",
    "\n",
    "The `fit_transform` method of this object receives the data and returns the transformed data. For example if our dataset has two independent variables $x=[w, v]$, then `PolynomialFeatures(degree=2)` would return $[1, w, v, w^2, wv, v^2]$. \n",
    "\n",
    "The `sklearn.linear_model` submodule offers \n",
    "\n",
    "```python\n",
    "sklearn.linear_model.LinearRegression(fit_intercept=True, # Fit the intercept term\n",
    "                                      copy_X=True, \n",
    "                                      normalize=False, # Remove average and divide by standard deviation \n",
    "                                      n_jobs=None, # Number of CPU cores\n",
    "                                      positive=False # Can be used to force positive coefficients\n",
    "                                      ...\n",
    "                                     )\n",
    "\n",
    "```\n",
    "\n",
    "which returns an object with the following attributes and methods\n",
    "\n",
    "- `coef_`: Returns the slopes of the fitted model\n",
    "- `intercept_`: Returns the intercept of the fitted model\n",
    "- `fit(X, Y)`: Fit a model to predict the response `Y` given the features `X`\n",
    "- `predict(X)`: Returns the predicted response for features `X`\n",
    "- `score(X, Y)`: Returns the coefficient of determination ($r^2$)\n",
    "\n",
    ":::{note}\n",
    "\n",
    "`LinearRegression` is a wrapper for `linalg.lstsq`. The advantage of using `LinearRegression` instead of `linalg.lstsq` is that we can use `sklearn.pipeline` to create a polynomial regression model as follows\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def polynomial_regressor(degree):\n",
    "    # Either include_bias is True or fit_intercept is True\n",
    "    return Pipeline([('features', PolynomialFeatures(degree, \n",
    "                                                     include_bias=False)),\n",
    "                     ('regressor', LinearRegression(fit_intercept=True, \n",
    "                                                    normalize=True))])\n",
    "\n",
    "polynomial_regressor(degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data from the previous lesson let's train a polynomial regressor to predict `consumption` as a function of  `temperature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ice_cream.csv', header=0, index_col=0)\n",
    "df.columns = ['Consumption', 'Income', 'Price', 'Temperature']\n",
    "Y = df[\"Consumption\"].values\n",
    "X = df[\"Temperature\"].values\n",
    "\n",
    "model = polynomial_regressor(degree=2).fit(X.reshape(-1, 1), Y)\n",
    "# intercept_ and coef_ attributes return the value of the fitted parameters (theta)\n",
    "display(model['regressor'].intercept_, \n",
    "        model['regressor'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the predictions of our polynomial regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatx = np.linspace(10, 90, num=100)\n",
    "haty = model.predict(hatx.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Overlay([hv.Curve((hatx, haty), 'Temperature', 'Consumption', label='model'), \n",
    "            hv.Scatter((X, Y), label='data').opts(color='k')]).opts(legend_position='top_left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have moved from lines to parabolas! But,\n",
    "\n",
    "> How does the result changes with the `degree` of our polynomial features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatx = np.linspace(np.amin(X), np.amax(X), num=1000)\n",
    "haty = {}\n",
    "for degree in [1, 2, 3, 4, 5, 10, 20]:\n",
    "    model = polynomial_regressor(degree).fit(X.reshape(-1, 1), Y)\n",
    "    haty[degree] = model.predict(hatx.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hMap = hv.HoloMap(kdims='degree')\n",
    "for degree, haty_ in haty.items():\n",
    "    p_model = hv.Curve((hatx, haty_), 'Temperature', 'Consumption', label='model')\n",
    "    p_data = hv.Scatter((X, Y), label='data').opts(color='k')\n",
    "    hMap[degree] = hv.Overlay([p_model, p_data]).opts(legend_position='top_left')\n",
    "\n",
    "hMap            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{danger}\n",
    "\n",
    "As `degree` grows we start overfitting the data more and more\n",
    "\n",
    ":::\n",
    "\n",
    "Let's define what is overfitting and how to combat it using regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overfitting and regularization\n",
    "\n",
    "\n",
    "In the previous example `degree` represents the complexity of the model. In general, more complex models give more flexibility to fit the data\n",
    "\n",
    "But too much complexity causes **overfitting**\n",
    "\n",
    "- the model fits the noise\n",
    "- we can't extract the underlying behavior \n",
    "- the model **does not generalize** to new data\n",
    "\n",
    "Ways to avoid overfitting\n",
    "\n",
    "- Using low complexity models\n",
    "- Set complexity using cross-validation \n",
    "- Regularization\n",
    "\n",
    "In what follows we will focus on the latter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bias-Variance trade-off\n",
    "\n",
    "Let's assume that our data can be modeled as a \"true model\" plus gaussian noise \n",
    "\n",
    "$$\n",
    "y = f(x) + \\varepsilon\n",
    "$$\n",
    "\n",
    "and that we use a linear model to find $f(x) = \\sum_k \\theta_k \\phi_k(x)$\n",
    "\n",
    "We can measure the quality of our model with the Mean Square Error (MSE)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[(y - \\hat y)^2] &= \\mathbb{E}[y^2 -2 y \\hat y +\\hat y^2] \\nonumber \\\\\n",
    "&= \\mathbb{E}[(f+\\varepsilon)^2 -2 (f+\\varepsilon) \\hat y +\\hat y^2] \\nonumber \\\\\n",
    "&= \\mathbb{E}[(f^2 +2 f \\varepsilon + \\varepsilon^2 -2 (f+\\varepsilon) \\hat y +\\hat y^2] \\nonumber \\\\\n",
    "&= \\mathbb{E}[\\varepsilon^2] + f^2  -2 f \\mathbb{E}[\\hat y]  +\\mathbb{E}[\\hat y^2]  \\pm \\mathbb{E}[\\hat y]^2  \\nonumber \\\\\n",
    "&= \\mathbb{E}[\\varepsilon^2] + (f - \\mathbb{E}[\\hat y])^2  +\\mathbb{E}[(\\hat y - \\mathbb{E}[\\hat y])^2]  \\nonumber \\\\\n",
    "&= \\sigma^2 + (f - \\mathbb{E}[\\hat y])^2  + \\text{Var}[\\hat y]  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ":::{important}\n",
    "\n",
    "The MSE can be decomposed as the irreducible error (data noise) + the squared bias of the estimator + the variance of the estimator\n",
    "\n",
    ":::\n",
    "\n",
    "The MSE can be small if either the bias or the variance are small. More complex models tend to have have lower bias and higher variance\n",
    "\n",
    "The Gauss-Markov theorem says that OLS has the minimum variance among the unbiased estimator. But zero-bias models are not necessarily good (overfit). \n",
    "\n",
    "If we are overfitting the data we may want to trade variance for bias. This can be achieved by penalizing the complexity of the model: this is **regularization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian (MAP) Least Squares and Ridge regression\n",
    "\n",
    "If we assume a Gaussian likelihood and a Gaussian prior we can write the log joint as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p({x}, \\theta) &= \\log \\prod_{i=1}^N \\mathcal{N}(y_i | f_\\theta(x_i), \\sigma^2) + \\log \\prod_{j=1}^M \\mathcal{N}(\\theta_j | 0, \\sigma_0^2) \\nonumber \\\\\n",
    "&= -\\frac{N}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) -\\frac{M}{2} \\log(2\\pi \\sigma_0^2) - \\frac{1}{2\\sigma_0^2} \\|\\theta\\|^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The MAP estimator of $\\theta$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p({x}, \\theta) \\nonumber  \\\\\n",
    "&= \\text{arg}\\max_\\theta  - \\frac{1}{2\\sigma^2} (Y-X\\theta)^T (Y - X\\theta) - \\frac{1}{2\\sigma_0^2} \\|\\theta\\|^2 \\nonumber \\\\\n",
    "&= \\text{arg}\\min_\\theta  (Y-X\\theta)^T (Y - X\\theta) + \\lambda \\|\\theta\\|^2 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\lambda = \\frac{\\sigma^2}{\\sigma_0^2}$\n",
    "\n",
    "The solution is obtained by taking the derivative on $\\theta$\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} (Y-X\\theta)^T (Y - X\\theta) + \\lambda \\|\\theta\\|^2  = -X^T (Y - X\\theta) + \\lambda \\theta = 0\n",
    "$$\n",
    "\n",
    "and finally\n",
    "\n",
    "$$\n",
    "\\hat \\theta = (X^T X + \\lambda I)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "which is known as **Ridge regression** and **Tikhonov regularization**\n",
    "\n",
    "\n",
    ":::{important}\n",
    "\n",
    "The gaussian prior is equivalent to a restriction on the $L_2$ norm of $\\theta$. Adding this prior forces the solution to be smooth\n",
    "\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Adding different priors yield different regularization effects. For example a Laplacian prior yields a $L_1$ norm  on $\\theta$ which forces the solution to be sparse \n",
    "\n",
    ":::\n",
    "\n",
    "In general, regularizing a model consists of adding a \"penalty term\" or restriction to the cost/loss function. Typically the additional term will penalize overly complex models. Because of this regularization can help to avoid overfitting with complex models or ill-posed problems, e.g. when we have more parameters than data samples \n",
    "\n",
    "But there is no free lunch. We now have the additional task of choosing $\\lambda$\n",
    "\n",
    "- Cross-validation: Minimize validation error\n",
    "- L-curve: Plot $ \\log (Y-X\\theta)^T (Y - X\\theta)$ vs $ \\log \\|\\theta\\|^2$ and find the elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Ridge Regression in Python\n",
    "\n",
    "The `sklearn` library provides\n",
    "\n",
    "```python\n",
    "sklearn.linear_model.Ridge(alpha=1.0, # The regularization parameter (lambda)\n",
    "                          fit_intercept=True, # Whether to include the intercept (constant)\n",
    "                          normalize=False, # Subtract mean and divide std from the data\n",
    "                          ...\n",
    ")\n",
    "```\n",
    "\n",
    "The attributes and methods of `LinearRegression` are also available in this object\n",
    "\n",
    "We will create a pipeline for the polynomial features plus the ridge regression and explore the influence of the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "hatx = np.linspace(np.amin(X), np.amax(X), num=1000)\n",
    "haty = {}\n",
    "mse = {}\n",
    "for degree in [1, 2, 3, 5, 10, 20]:\n",
    "    for lamb in [0.0, 1e-3, 1e-1,  10]:\n",
    "        non_linear_regressor = Pipeline([('features', PolynomialFeatures(degree)),\n",
    "                                         ('regressor', Ridge(normalize=True, alpha=lamb))])\n",
    "        # Fit\n",
    "        model = non_linear_regressor.fit(X.reshape(-1, 1), Y)\n",
    "        # Score\n",
    "        mse[degree, lamb] = mean_squared_error(Y, model.predict(X.reshape(-1, 1)))\n",
    "        # Predict on new data\n",
    "        haty[degree, lamb] = model.predict(hatx.reshape(-1, 1))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hMap = hv.HoloMap(kdims=['degree', 'lambda'])\n",
    "for (degree, lamb), haty_ in haty.items():\n",
    "    p_model = hv.Curve((hatx, haty_), 'Temperature', 'Consumption', label='model')\n",
    "    p_data = hv.Scatter((X, Y), label='data').opts(color='k')\n",
    "    hMap[degree, lamb] = hv.Overlay([p_model, p_data]).opts(legend_position='top_left')\n",
    "\n",
    "hMap            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- Regularization decreases overfitting in complex models\n",
    "- But an excessive penalization may induce a trivial solution, for example a straigh horizontal line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra topics\n",
    "\n",
    "### A note on independence and correlation\n",
    "\n",
    "Independence implies uncorrelatedness, but the reverse is not true. Two variables can have zero correlation but still be dependent. Also remember that linear regression (correlation) is only sensitive to linear relationships\n",
    "\n",
    "If we are interested in testing independence we could use:\n",
    "\n",
    "$$\n",
    "p(x,y) = p(x)p(y)\n",
    "$$\n",
    "\n",
    "Several methods are based on this, for example Shannon's **Mutual Information**\n",
    "\n",
    "$$\n",
    "I(X,Y) = \\int \\int f_{XY}(x,y) \\log \\frac{f_{XY}(x,y)}{f_{X}(x) f_Y(y)} dx dy\n",
    "$$\n",
    "\n",
    "and the [Correlation distance](https://arxiv.org/pdf/0803.4101.pdf)\n",
    "\n",
    "$$\n",
    "R(X,Y) = \\int \\int |f_{XY}(x,y)  - f_{X}(x) f_Y(y)| dx dy\n",
    "$$\n",
    "\n",
    "Although these methods require that we estimate the joint and the marginals (KDE, Histogram, Parametric). For categorical variables we can use the **chi square test**\n",
    "\n",
    "\n",
    "### Related topics \n",
    "\n",
    "- (Hastie 3.4 and 3.8) L1 regularization and Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "- Robust regression: Least absolute regression and M-estimators for data with outliers (non-Gaussian)\n",
    "- (Hastie 6 & Bishop 6) Kernel (non-parametric) regression  \n",
    "\n",
    "Some of these topics can be found at [Huijse, Regresión](https://docs.google.com/presentation/d/1UUpK4zSdzRcS79V7_wU9nXe-sR7qYLEWhbmid-Rfp1k/edit#slide=id.g28044c0f85_0_34)\n",
    "\n",
    "I also suggests the following lecture by Judea Pearl on **causality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('ZaPV1OSEpHw')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "366px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
