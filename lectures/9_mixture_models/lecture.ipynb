{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500), \n",
    "                 hv.opts.Histogram(width=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Mixture Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Unsupervised methods:** Contrary to supervised (regression, classification) in this scenario there are no labels/targets. In general the objective of unsupervised methods is **to find structure in the data**. We do this to better understand the data: find hidden causes, find shared features, etc\n",
    "\n",
    "This tasks can be solve using **Latent variable models (LVM)**. An LVM models a **cause and effect** structure \n",
    "\n",
    "- Observed variables (effect): Data\n",
    "- Latent variables (cause): Hidden variables that explains the observed variables \n",
    "\n",
    "and the objective is to infer the latent given the observed. In general we have to assume properties for the latent variable, for example\n",
    "\n",
    "- Dimension of the latent is smaller than the observed (Dimensionality reduction)\n",
    "- Latent variable is continuous and Gaussian distributed (PCA)\n",
    "- Latent variables are not correlated (observed data might be)\n",
    "\n",
    "In this lesson we will focus on LVM models with discrete latent variables for clustering\n",
    "\n",
    "> Clustering: Group the data into clusters (sets) such that\n",
    "> - \"Similar\" data samples go to the same cluster. We have to define similarity\n",
    "> - Elements of one cluster are \"different\" from the elements of another cluster\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mixture model\n",
    "\n",
    "A mixture model is an LVM for clustering. We represent each data point \n",
    "\n",
    "$$\n",
    "x_i \\in \\mathbb{R}^D \\text{ with } i=1,\\ldots, N\n",
    "$$\n",
    "\n",
    "as having a **categorical latent variable**\n",
    "\n",
    "$$\n",
    "z_i \\in {1, 2, \\ldots, K}\n",
    "$$\n",
    "\n",
    "note that this is the same as saying that $z_i$ has a categorical prior\n",
    "\n",
    "$$\n",
    "p(z_i) = \\text{Cat}(\\vec \\pi),\n",
    "$$\n",
    "\n",
    "where $\\pi_k \\in [0, 1]$ are the mixing coefficients and $\\sum_{k=1}^K \\pi_k=1$ \n",
    "\n",
    ":::{important}\n",
    "\n",
    "We assume that there are $K$ clusters (sets). The latent variable associates each data point with one of the $K$ clusters\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "We can write the likelihood of the mixture model as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_i | \\theta) &= \\sum_{k=1}^K p(x_i|z_i = k, \\theta) p(z_i=k| \\theta) \\nonumber \\\\\n",
    "&= \\sum_{k=1}^K p_k(x_i| \\theta_k) \\pi_k ,  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To continue we have to specify the family of $p_k(x_i|\\theta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian mixture model (GMM)\n",
    "\n",
    "In the GMM we further assume that each component of the mixture is a Gaussian/Normal distribution. With this we can write the likelihood as \n",
    "\n",
    "$$\n",
    "p(x_i | \\theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k),\n",
    "$$\n",
    "\n",
    "where each cluster has three parameters\n",
    "\n",
    "- The mean of the clusters $\\mu_k$, which represents the location of the center of the clusters\n",
    "- The covariance $\\Sigma_k$, which gives the size and orientation of the clusters\n",
    "- The concentration of the cluster $\\pi_k$\n",
    "\n",
    "The fitting the GMM is: **Finding the best value of these parameters** for all clusters given the data\n",
    "\n",
    "Note that the GMM is a **generative model**, given a value for the parameters we can sample from the GMM\n",
    "\n",
    "Let's explore the influence of these parameters on the \"shape\" of the mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def create_mixture(pis, mus, Sigmas, N=5000, rseed=None):\n",
    "    \"\"\"\n",
    "    pis (list of floats): mixing coefficients : cluster importance\n",
    "    mus (list of arrays): mean: cluster centers\n",
    "    Sigmas (list of arrays): covariance: cluster shape\n",
    "    N (int): number of points to create\n",
    "    \"\"\"\n",
    "    if rseed is not None:\n",
    "        np.random.seed(rseed)\n",
    "    points = list()\n",
    "    labels = list()\n",
    "    for k in range(len(pis)):\n",
    "        Nc = int(pis[k]*N)\n",
    "        if len(mus[k]) > 1:\n",
    "            x = np.random.multivariate_normal(mus[k], Sigmas[k], Nc)\n",
    "        else:\n",
    "            x = mus[k] + np.sqrt(Sigmas[k])*np.random.randn(Nc)\n",
    "        points.append(x)\n",
    "        labels.append([k]*Nc)\n",
    "    return np.concatenate(points), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data, labels = create_mixture(pis=[0.4, 0.5, 0.1], \n",
    "                              mus=[[-1], [2], [2.5]], \n",
    "                              Sigmas=[0.1, 0.5, 0.1])\n",
    "\n",
    "edges, bins = np.histogram(data, bins=50, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Histogram((edges, bins), kdims='data', vdims='Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = create_mixture(pis=[0.9, 0.1], \n",
    "                              mus=[[-1, -1], [1, 1]], \n",
    "                              Sigmas=[0.5*np.eye(2), [[1, -0.9],[-0.9, 1]]])\n",
    "\n",
    "bins, edgesx, edgesy = np.histogram2d(data[:, 0], data[:, 1], bins=30, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Scatter((data[:, 0], data[:, 1])) +  hv.Image((edgesx, edgesy, bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Posterior probability a.k.a. picking a cluster\n",
    "\n",
    "Let's assume that we know the parameters of the mixture and we want to infer the most probable cluster for a given data point\n",
    "\n",
    "> This is given by the posterior probability of $z$ given $x$\n",
    "\n",
    "Using the Bayes rule and our previous assumptions (likelihood) we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r_{ik} = p(z_i = k|x_i, \\theta) &= \\frac{ p(x_i|z_i = k, \\theta) p(z_i=k| \\theta)}{p(x_i)} \\nonumber \\\\\n",
    "&= \\frac{ p(x_i|z_i = k, \\theta) p(z_i=k| \\theta)}{ \\sum_{k=1}^K p(x_i|z_i = k, \\theta) p(z_i=k| \\theta) } \\nonumber \\\\\n",
    "&= \\frac{ \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)  \\pi_k}{ \\sum_{k=1}^K \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\pi_k }, \\nonumber\\end{align}\n",
    "$$\n",
    "\n",
    "this posterior is also called the **responsibility** $r_{ik}$ of cluster $k$ for point $i$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "It is a **soft-cluster assignment**\n",
    "\n",
    ":::\n",
    "\n",
    "In some cases we only need a **hard-cluster assignment**, *i.e.* the single most probable cluster, which can be obtained as\n",
    "\n",
    "$$\n",
    "c_{i} = \\text{arg}\\max_k r_{ik} = \\text{arg}\\max_k \\log \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) + \\log \\pi_k,\n",
    "$$\n",
    "\n",
    "because of the maximum operator we can omit the evidence (denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jointGMM(data, pi, mu, Sigma):\n",
    "    # Assume 2 dimensional data\n",
    "    data_centered = data - mu\n",
    "    norm = pi/(2*np.pi*np.sqrt(np.linalg.det(Sigma)))\n",
    "    xSx = np.multiply(data_centered, np.linalg.solve(Sigma, data_centered.T).T)\n",
    "    return norm*np.exp(-0.5*np.sum(xSx, axis=1))\n",
    "\n",
    "pis = [0.6, 0.4]; mus=[[-1, -1], [1, 1]]; \n",
    "Sigmas=[0.5*np.eye(2), [[1, -0.3],[-0.3, 1]]]\n",
    "data, labels = create_mixture(pis, mus, Sigmas, rseed=0)\n",
    "\n",
    "posterior = np.zeros(shape=(len(data), len(pis)))\n",
    "for k in range(len(pis)):\n",
    "    posterior[:, k] = jointGMM(data, pis[k], mus[k], Sigmas[k])\n",
    "posterior = posterior/np.sum(posterior, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p1 = hv.Scatter((data[:, 0], data[:, 1], posterior[:, 1]), \n",
    "                vdims=['y', 'z'], label='Soft assignment').opts(color='z')\n",
    "p2 = hv.Scatter((data[:, 0], data[:, 1], np.argmax(posterior, axis=1)), \n",
    "                vdims=['y', 'z'], label='Hard assignment').opts(color='z')\n",
    "hv.Layout([p1, p2]).cols(2).opts(hv.opts.Scatter(cmap='RdBu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting the GMM\n",
    "\n",
    "We want to find the optimal point estimates of the parameters of a statistical model. Can we use **MLE**? The log likelihood in this case is \n",
    "\n",
    "$$\n",
    "\\log L(\\pi,\\mu,\\Sigma) = \\sum_{i=1}^N \\log \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "With the exception of $K=1$, we won't be able to find analytic expressions for the parameters\n",
    "\n",
    ":::\n",
    "\n",
    "In general when we don't have complete data, i.e. there are missing or hidden variables, the posterior does not factorize\n",
    "\n",
    "In this case we have the latent/hidden variables $z_i$. We need to marginalize it (sum over k) to compute the likelihood. In what follows we will review approximate MLE estimation: **Expectation Maximization (EM)**\n",
    "\n",
    "Other problems of the MLE/MAP for GMM\n",
    "\n",
    "- Non-convex, it has local optima (Murphy 11.3.2)\n",
    "- Very hard to compute (NP-complete hard!)\n",
    "- **Unidentifiability:** There exist $K!$ configurations that yield the exact same solution (label switch)\n",
    "- **Singularities:** A component may collapse to a single point ($\\Sigma \\to 0$ and $\\log L \\to \\infty$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectation Maximization (EM)\n",
    "\n",
    "The MLE solution can be computed if we **\"fully-observe\"** the data. This means that\n",
    "\n",
    "> We know $z_i$ for each $x_i$, *i.e.* we know which cluster owns $x_i$\n",
    "\n",
    "The **fully-observed** log likelihood (FOLL) is\n",
    "\n",
    "$$\n",
    "\\log L_{\\text{FO}}(\\theta) = \\sum_{i=1}^N \\log p(z_i) p(x_i|z_i, \\theta), \n",
    "$$\n",
    "\n",
    "but in practice we do not observe $z$, so we marginalize it by computing the **expected value** of the FOLL given the posterior\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(\\theta, \\theta^{\\text{old}}) &= \\mathbb{E}_{p(z|x, \\theta^{\\text{old}})} \\left[\\log L_{\\text{FO}}(\\theta) \\right]   \\nonumber \\\\\n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N p(k|x_i, \\theta^{\\text{old}})  \\log p(k) p(x_i|\\theta_k) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is called the **auxiliary function**. After this we update our parameters by **maximizing** \n",
    "\n",
    "$$\n",
    "\\theta^{\\text{new}} = \\text{arg}\\max_\\theta Q(\\theta, \\theta^{\\text{old}}),\n",
    "$$\n",
    "\n",
    "and finally we do $\\theta^{\\text{old}} \\leftarrow \\theta^{\\text{new}}$ and repeat the procedure until convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iterative procedure of\n",
    "\n",
    "1. **E-step:** Estimating the expected value of the likelihood given our current parameters\n",
    "1. **M-step:** Maximizing the estimator and updating our parameters\n",
    "\n",
    "\n",
    "is called **Expectation Maximization** (EM)\n",
    "\n",
    "\n",
    "EM is a general algorithm:\n",
    "\n",
    "- it can be used for other problems with non-analytical solution/non-totally observed data\n",
    "- it has many variants: Incremental, Variational, Monte-Carlo (Murphy 11.4.8 and 11.4.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EM updates for the GMM\n",
    "\n",
    "Let's start by defining the auxiliary function for the GMM\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(\\theta, \\theta^{\\text{old}}) \n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N p(k|x_i, \\theta^{\\text{old}})  \\log p(k) p(x_i|\\theta_k)  \\nonumber \\\\\n",
    "&= \\sum_{k=1}^K \\sum_{i=1}^N r_{ik}^{\\text{old}}  \\left(\\log \\pi_k + \\log \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right),\\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $r_{ik}^{\\text{old}}$ is the responsibility given the parameters of the previous iteration. \n",
    "\n",
    "Now we maximize $Q$ to get an estimator of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The estimator for the means is given by\n",
    "\n",
    "$$\n",
    "\\mu_{k}^{\\text{new}}  = \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i\n",
    "$$\n",
    "\n",
    "Which is obtained by setting the derivative of $Q$ to zero\n",
    "\n",
    "\n",
    ":::{dropdown} Proof\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d Q}{d\\mu_{k'}} &= \\frac{d}{d\\mu_{k'}} \\sum_{k=1}^K \\sum_{i=1}^N r_{ik}^{\\text{old}}  \\left(\\log \\pi_k + \\log \\mathcal{N}(x_i|\\mu_k, \\Sigma_k) \\right) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N r_{ik'}^{\\text{old}}  \\frac{d}{d\\mu_{k'}}  \\log \\mathcal{N}(x_i|\\mu_{k'}, \\Sigma_{k'})\\nonumber \\\\\n",
    "&= \\sum_{i=1}^N r_{ik'}^{\\text{old}}  \\frac{d}{d\\mu_{k'}} \\left( -\\frac{1}{2} (x_i - \\mu_{k'})^T \\Sigma_{k'}^{-1} (x_i - \\mu_{k'}) -\\frac{D}{2} \\log(2\\pi) -\\frac{1}{2} \\log(|\\Sigma_{k'}|) \\right)\\nonumber \\\\\n",
    "&= -\\frac{1}{2} \\sum_{i=1}^N r_{ik'}^{\\text{old}}  \\frac{d}{d\\mu_{k'}} (x_i - \\mu_{k'})^T \\Sigma_{k'}^{-1} (x_i - \\mu_{k'})  \\nonumber \\\\\n",
    "&= \\sum_{i=1}^N r_{ik'}^{\\text{old}} \\Sigma_{k'}^{-1} (x_i - \\mu_{k'})  = 0 \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Following the same procedure for the covariance we arrive to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma_k^{\\text{new}} &= \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} (x_i - \\mu_k) (x_i - \\mu_k)^T \\nonumber \\\\\n",
    "&= -\\mu_k \\mu_k^T  + \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i x_i^T   \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, by incorporating the constraint $\\sum_k \\pi_k = 1$ we arrive to \n",
    "\n",
    "$$\n",
    "\\pi_k^{\\text{new}} = \\frac{\\sum_{i=1}^N r_{ik}^{\\text{old}}}{N}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "The following algorithms summarizes the application of EM to the GMM problem\n",
    "\n",
    "- **(1)** Set initial conditions\n",
    "- **(2)** Compute cluster responsibilities for each data point using\n",
    "\n",
    "$$\n",
    "r_{ik}^{\\text{old}} = \\frac{ \\mathcal{N}(x_i|\\mu_k^{\\text{old}}, \\Sigma_k^{\\text{old}})  \\pi_k^{\\text{old}}}{ \\sum_{k=1}^K \\mathcal{N}(x_i|\\mu_k^{\\text{old}}, \\Sigma_k^{\\text{old}}) \\pi_k^{\\text{old}} }\n",
    "$$\n",
    "\n",
    "- **(3)** Compute the new values for the parameters using\n",
    "\n",
    "$$\n",
    "\\mu_{k}^{\\text{new}}  = \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_k^{\\text{new}} = -\\mu_k \\mu_k^T  + \\frac{1}{\\sum_{i=1}^N r_{ik}^{\\text{old}}} \\sum_{i=1}^N r_{ik}^{\\text{old}} x_i x_i^T , \n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\pi_k^{\\text{new}} = \\frac{\\sum_{i=1}^N r_{ik}^{\\text{old}}}{N}\n",
    "$$\n",
    "\n",
    "- **(4)** Update the parameters $\\theta^{\\text{old}} \\leftarrow \\theta^{\\text{new}}$\n",
    "\n",
    "- **(5)** If convergence criterion is met stop otherwise go back to **2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example using \"Old faithful\" data\n",
    "\n",
    "Iteration by iteration  (Bishop Fig. 9.8)\n",
    "\n",
    "<img src=\"img/gmm_bishop.png\" width=\"700\">\n",
    "\n",
    "Animation (from wikipedia)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Practical details of EM\n",
    "\n",
    "**Initialization and restarts** \n",
    "\n",
    "The EM procedure is guaranteed to converge to a *stationary point* (local minimum/maximum or saddle point)\n",
    "\n",
    "- One can obtain a better solution using the following heuristic:\n",
    "    1. Set a **number of retries**\n",
    "    1. Run GMM with a random initialization\n",
    "    1. If the likelihood of this solution is larger than the previous replace best\n",
    "    1. Repeat until **number of retries** is achieved\n",
    "- Another option is to initialize with the solution from a simpler algorithms (*e.g.* k-means)\n",
    "    - This can avoid catastrophic results\n",
    "    - But it can also limit exploration \n",
    "    \n",
    "**Convergence**\n",
    "\n",
    "EM is an iterative procedure\n",
    "\n",
    "> How many times do we repeat it?\n",
    "\n",
    "- Check log likelihood vs iteration\n",
    "- For example: stop if log likelihood increment is less than 0.1\\% wrt to previous\n",
    "\n",
    "**Number of clusters**\n",
    "\n",
    "The number of clusters $K$ is a parameter set *a priori* by the user\n",
    "\n",
    "> How do we know what value of $K$ is good?\n",
    "\n",
    "Option 1: The Elbow method: Plot log likelihood (in the validation set) as a function of $K$\n",
    "\n",
    "The log likelihood may keep decreasing with $K$ (overfit) but at some point the decrease is negligible: find the **elbow** of the curve\n",
    "\n",
    "<img src=\"img/elbow.png\" width=\"500\">\n",
    "\n",
    "Option 2: Use the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC), or other criterion for model selection that penalizes complexity\n",
    "\n",
    "Option 3: [The Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture with [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `sklearn.mixture` scikit learn module\n",
    "\n",
    "```python\n",
    "GaussianMixture(n_components=1, # Number of clusters\n",
    "                covariance_type='full', # Shape of the covariance\n",
    "                tol=0.001, # termination tolerance of EM\n",
    "                reg_covar=1e-06, # small number added to the diagonal of the covariance\n",
    "                max_iter=100, # Max number of EM iterations\n",
    "                n_init=1, # Number of random initializations\n",
    "                init_params='kmeans', # Initialization for the responsabilities (kmeans or random)\n",
    "                ...\n",
    "               )\n",
    "```\n",
    "\n",
    "The shape can be `full`, `tied`, `diag` or `spherical`. \n",
    "\n",
    "- In a diagonal covariance $\\Sigma = \\vec \\sigma^2 \\text{I}$, $\\vec \\sigma \\in \\mathbb{R}_{+}^D$, clusters cannot have rotations but the variance of each dimension can be different\n",
    "- In a spherical covariance $\\Sigma = \\sigma^2 \\text{I}$, $\\sigma \\in \\mathbb{R}_{+}$, clusters cannot have rotations and all dimensions have the same variance\n",
    "- Tied covariance means that all clusters have the same covariance (dispersion and rotation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this module. We start by creating synthetic data as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = create_mixture(pis=[0.33, 0.33, 0.34], \n",
    "                              mus=[[-3, 0], [0, 0], [3 ,0]], \n",
    "                              Sigmas=[[[1, 0.9],[0.9, 1]], \n",
    "                                      [[1, -0.9],[-0.9, 1]], \n",
    "                                      [[1, 0.9],[0.9, 1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Scatter((data[:, 0], data[:, 1])).opts(width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Bayesian information criterion (BIC) to find the \"best\" number of clusters. \n",
    "\n",
    "$$\n",
    "BIC = K \\log N - 2 \\log L\n",
    "$$\n",
    "\n",
    "The BIC grows with $k$ the number of parameters and number of samples $n$ and decreases with the maximum value of the log likelihood. A model with lowest BIC is preferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "maxK = 20; \n",
    "cov = 'full' \n",
    "metrics = np.zeros(shape=(maxK-1, ))\n",
    "for i, K in enumerate(range(2, maxK+1)):\n",
    "    gmm = GaussianMixture(n_components=K, covariance_type=cov, n_init=3)\n",
    "    gmm.fit(data)\n",
    "    metrics[i] = gmm.bic(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Curve((range(2, maxK+1), metrics), 'K', 'BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum BIC is obtained using three clusters. Let's use $K=3$ and inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=3, covariance_type=cov, n_init=3)\n",
    "gmm.fit(data)\n",
    "hatr = gmm.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Scatter((data[:, 0], data[:, 1], hatr), vdims=['y', 'z'],).opts(width=500, color='z', cmap='Category10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy (uncertainty) of the soft assignments (responsabilities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gmm.predict_proba(data)\n",
    "H = -np.sum(p*np.log(p), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Scatter((data[:, 0], data[:, 1], H), vdims=['y', 'z'],).opts(width=500, color='z', cmap='Blues', colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatdata = gmm.sample(n_samples=10000)\n",
    "hatdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Scatter((hatdata[0][:, 0], hatdata[0][:, 1])).opts(width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Appendix: Relation to K-means\n",
    "\n",
    "The K-means algorithm is a GMM with the following constraints\n",
    "\n",
    "- Spherical clusters: $\\Sigma = \\sigma^2 \\text{I}$, $\\sigma \\in \\mathbb{R}_{+}$\n",
    "- All clusters have equal size: $\\sigma^2 = \\sigma_1^2 = \\sigma_2^2 = \\ldots = \\sigma_K^2$\n",
    "- Uniform prior for the mixture coefficients: $\\pi_k = \\frac{1}{K}$\n",
    "- Hard labels are used instead of responsibilities\n",
    "\n",
    "The multivariate normal for spherical clusters\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x_i|\\mu_k, \\Sigma_k = \\sigma^2 I) = \\frac{1}{\\sqrt{(2\\pi)^D} \\sigma} \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "The update rule for the responsibility \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat r_{ik}^{\\text{old}} &= \\text{arg} \\max_k \\frac{ \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right) }{ \\sum_{k=1}^K \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right)  } \\nonumber \\\\\n",
    "&=  \\text{arg} \\max_k  \\log \\exp \\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2 \\right)  \\nonumber \\\\\n",
    "&=  \\text{arg} \\min_k  \\frac{1}{2\\sigma^2}  \\|x_i - \\mu_k\\|^2   \\nonumber \\\\\n",
    "&=  \\text{arg} \\min_k  \\|x_i - \\mu_k\\|^2   \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the means\n",
    "$$\n",
    "\\mu_{k}^{\\text{new}}  = \\frac{1}{\\sum_{i=1}^N \\hat r_{ik}^{\\text{old}}} \\sum_{i=1}^N \\hat r_{ik}^{\\text{old}} x_i\n",
    "$$\n",
    "\n",
    "\n",
    "Hence the EM for K-means reduces to\n",
    "\n",
    "1. Update \"hard\" responsibilities\n",
    "1. Update the cluster means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## References and additional  material\n",
    "\n",
    "\n",
    "- Bishop, \"Pattern recognition and machine learning\", **Chapter 9**\n",
    "- Murphy, \"Machine Learning: A Probabilistic Perspective\", *MIT Press*, 2012, **Chapter 11**\n",
    "- [Scikit-learn article: Setting the concentration prior](https://scikit-learn.org/stable/auto_examples/mixture/plot_concentration_prior.html)\n",
    "- [Variational GMM in pymc3](https://docs.pymc.io/notebooks/gaussian-mixture-model-advi.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
